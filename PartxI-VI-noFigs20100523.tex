\documentclass{jbook}
\author{
Ryo Yamada
}

\title{ゲノムのための統計学教室}

\usepackage[dvips]{graphicx}
%\usepackage[dvipdfm]{graphicx}
\usepackage{graphicx}
\usepackage{bigdelim,multirow}
\usepackage{amsmath,amsthm,amssymb,cases}
\usepackage{ascmac}
\usepackage{url}
\usepackage{eclbkbox}
\usepackage{wrapfig}
\usepackage{listings, jlisting}
\usepackage[dvips,usenames]{color}
 \usepackage{makeidx}
\renewcommand{\baselinestretch}{1}
\setlength{\textheight}{33\baselineskip}


\makeindex

%\setlength{\topmargin}{0mm}

\usepackage{Sweave}
\begin{document}
\maketitle


\lstset{%
 language={R},
 backgroundcolor={\color[gray]{.85}},%
 basicstyle={\small\tt },%
% basicstyle={\small },%
 identifierstyle={\small},%
 commentstyle={\small\itshape},%
 keywordstyle={\small\bfseries},%
 ndkeywordstyle={\small},%
 stringstyle={\small\ttfamily},
 frame={tb},
 breaklines=true,
 columns=[l]{fullflexible},%
 numbers=left,%
 xrightmargin=0zw,%
 xleftmargin=0zw,%
% xleftmargin=3zw,%
 numberstyle={\scriptsize},%
 stepnumber=1,
 numbersep=1zw,%
 lineskip=-0.5ex%
}
\newpage

\Large 
はじめに\\
\normalsize
\subsubsection{統計遺伝学・遺伝統計学とは　遺伝学と統計学}
本書は統計遺伝学・遺伝統計学の教科書として使うことを目的として書きました。
統計遺伝学・遺伝統計学は、生命現象の根幹の一つである遺伝という現象を軸に
数理的に生命現象を解釈する学問です。
親と子は似ています。
それは、遺伝子が親から子へと伝達されるからです。
よく似てはいますが、異なる部分も持っていて、集団で見ると、
ばらつきがあります。
このように、似ているけれどもばらついていることを遺伝子の伝達とその機能の発揮とに照らして
理解しようとするのが遺伝学です。

子の遺伝子のセットは、母親と父親から遺伝子のセットの半分ずつを受け継いでいます。
逆に言うと、親の遺伝子セットの半分だけが子へと伝わります。
この半分の選び方がランダムに起きるので、遺伝現象は確率的です。
このため、遺伝学は確率的に現象を理解することを基本とします。

このように、「ばらつき」と「確率」は遺伝学の基本ですが、
同じく「ばらつき」と「確率」を基本とする学問に統計学があります。
このように遺伝学と統計学は非常に似た部分があり、実際、
両者は一体となって発展してきたという関係があります。

現在、遺伝学もは、現象としての遺伝から、実体としての遺伝子とその分子機構の解明、さらには、
遺伝子の役割を遺伝子セット全体として理解しようとするゲノム学の展開により、
大規模データ科学としての側面を強くしています。
一方、統計学はありとあらゆる学問に影響を与え、非常に幅の広い学問となっています。

本書では、そのような遺伝学・ゲノム学における数理統計学的アプローチについて、
遺伝学が寄与した部分の大きい事項や、遺伝現象・ゲノム解析に特徴的な事柄として、
離散的なこと・確率と尤度のこと・大規模なこと・不均一なことを取り出して
なるべく広く取り扱いました。

\subsubsection{本書の構成}

本書は遺伝現象と遺伝子の機能に関する研究について幅広く取り扱い、
それらの統計学的側面を理解するときに共通している事柄に焦点を当てて構成しました。

第I部では、遺伝・遺伝子に関連する生物学的事項を扱います。
生物学の基礎知識がある人にとっては内容は平易なので、その内容を例に
本書の第II部以降で活用するフリーの統計ソフト『R』の扱いを覚えて欲しいと思います。
逆に数理統計系の基礎がある人にとっては、遺伝学・生物学の基礎事項の
導入の役割を持つと思います。

第II部は、サンプルに観察されるデータの取り扱いの基礎に関する事項を取り扱います。
遺伝子の離散性からカテゴリカルデータ型に関して重点的に説明しています。
また、データを相対的に評価することについて説明をしています。
また、個々のサンプルを区別して、その個々のサンプルの間の関係を扱うのか、
集団として扱うのか、という視点から、その手法を利用例に即して説明しています。

第III部は、記述統計に関すること、推定や検定の基礎となる概念である、次元・自由度、確率・尤度、分布、指標
について説明しています。
この中で、尤度の考え方を活用して特に発達した解析手法である、
連鎖解析に関して、少し詳しく説明しています。

データを統計学的に取り扱う道具立ての説明である第III部を受けて、
第IV部では、データからどのようにして意味を引き出すかについて
説明しています。具体的には、推定・棄却・検定・関連に関する事項を取り扱っています。

第V部は、データが大規模であるときや、複雑であるときに発生する諸事情に関する事柄を
扱っています。場合の数を数え上げること、場合の数が大きすぎるときに、部分を使って
全体に関する情報を取ること、数多くの検定をするときの解釈方法について順に説明しています。

そして最後にRを使って勉強するときに便利だと思われる事項と、
本書で使用した数式表現をまとめました。

\subsubsection{本書の使い方}
前節で述べた通り、本書は遺伝現象と遺伝子の機能に関する研究の
統計学的側面を理解するときに共通している事柄に焦点を当てて構成しました。
逆に言えば、遺伝子マッピングや遺伝子機能解析などに使われる諸手法の解説や、
各種指数の定義などは
あえて、扱わないこととしました。

それら個々の手法を活用したり、指数を適切に用いなくてはならないことは当然ですが、
それらについての理解を目指したのでは、取り扱わなかった事項はわからなくなりますし、
今後、どんどん、開発・提案されるであろう、
新規の方法や指数などに対応する力はつかないからです。
それよりは、色々な手法や指数に共通する考え方は何なのかという基礎に重みを置くことで、
応用力をつけて欲しいと願っているからです。
語学の机上学習において、基礎を押さえることで、
初めての文章も読みこなせるようになることを目指すようなイメージです。
ですから、本書では、形質マッピングに興味があるならこの章を読むとよい、クラスタリングを知りたいならこの章を、
というようには構成されていません。
１つの研究テーマに関する事項があちこちに出てきて、
それらを相互参照しながら読むのが最善、というような作りになっています。
また、複数回、登場する用語や考え方は、必ずしも初出のところで丁寧に説明することはせず、
丁寧な説明が最も有用と思われるところに書きました。
これは、初出時に定義や説明を書くことで、前半が重くなることを避けるためです。
したがって、多少、曖昧なままでも、意味が通る限りは読み進み、再び読み返したときに納得がいけば、
よしと考えてもらえればと思います。
読み進むにあたり、詳しい説明なしに使用した用語などは、太字で示し、
関連説明を探せるようにしました。
太字で示した語を索引で確認しますと、列挙された複数のページのうち、
一部のページが太字となっているときには、そこが一番詳しい説明ですので、
そのページの記載を参照してください。
ウェブの記事で言えば、相互リンクがあちこちに張ってあるような具合です。
索引を介してジャンプするようなつもりで使ってください。


また、図を多用しました。
さらに、図を描くためのフリー統計ソフトRのソースも掲載しました。
自然言語の文章で内容を押さえ、図でイメージを湧かせた上で、
数式で正確な表現を確認しつつ、Rのソースも理解の助けとして欲しいと思います。
また、Rのソースはシミュレーションであるものが多いですから、是非、使用して、
異なる条件での結果を出力してみることをお勧めします。

本書で使用したRのソースは　\url{http://xxxxxx}  からダウンロードできます。

\subsubsection{参考書}
本書は遺伝学に関係するけれども、遺伝学を網羅しておらず、
形質を決める遺伝子を決める形質マッピングにも強く関係しているけれども、
それを体系的に扱っておらず、
統計学にも関係するけれども、統計学の標準的な内容とはなっていません。
また、Rを使うマニュアルとして書かれたものでもありません。
それぞれの事項を扱った書籍等で別途、学ぶことは非常に有益と思います。
以下に、参考書として挙げておきます。

統計遺伝学

『Handbook of Statistical Genetics (3rd ed.)』David J. Balding et al. John Wiley \& Sons 

本書が扱った内容のほぼすべてを網羅的に扱った大部な書

『Statistics in Humag Genetics』Pak Sham Arnold

コンパクトにまとまった良書

『遺伝統計学入門』鎌谷直之　岩波書店

形質マッピングを中心に扱った書

遺伝学・集団遺伝学

『初歩からの集団遺伝学』安田徳一　裳華房

集団遺伝学の概念を体系的に扱った書

離散数学

『離散数学　「数え上げ理論」』野崎昭弘　講談社

数え上げについての入門書

『マクグロウヒル大学演習　離散数学　コンピュータサイエンスの基礎数学』Seymour Lipschutz 著　成嶋　弘　監訳　オーム社

離散数学を計算機活用を念頭において扱った良書


確率・統計学

『確率概論』河野敬雄　京都大学学術出版会

確率を正面から扱った書

『統計学を拓いた異才たち』ディヴィッド　サルツブルグ　竹内惠行,熊谷悦生　訳　日本経済新聞社

統計学の発展に関する読み物

『入門　ベイズ統計』松原望　東京図書

ベイズ統計に関する入門書

『統計分布ハンドブック』蓑谷千凰彦　朝倉書店

数学

『統計学を学ぶための数学入門30講』永田　靖　朝倉書店

統計学での利用の面から数学の諸項目を解説した良書

『グラフ理論入門』 N ハーツフィールド G リンゲル　鈴木晋一訳　サイエンス社

グラフ理論の平易な入門書

Ｒ

『Ｒによる統計解析』青木繁伸　オーム社

統計解析一般をＲで実施することを念頭においた良書

『Ｒで学ぶクラスタ解析』新納浩幸　オーム社

クラスタ解析を網羅的に概説した入門書


\newpage

\tableofcontents

\part{遺伝子型から表現型まで}
\chapter{遺伝　似ていることと似ていないこと}
\section{形質が遺伝する}
\subsection{遺伝}
遺伝とは何でしょうか。血のつながりがあるときに、ある特徴が共有されることです。
人間の親から人間の子が生まれたとき、人間であるという特徴は親子で共有されています。
このことが余りにも当たり前であるとき、人間は遺伝という現象に言葉を与えることはなかったと思います。
遺伝という現象に単語を与えたということは、それが当たり前とは言えなかったからです。
人間の親から人間の子が生まれるその傍らで、「カエルの子はカエル」であり、「瓜の蔓に茄子はならな」かったために、
種という特徴が遺伝していることに気づいたのでしょう。
しかしながら、種に気づいただけでは、遺伝現象に気づいたとは言い難いです。ここで挙げたカエルと瓜のことわざも、
「鳶が鷹を生む」という逆の意味のことわざも、
「親子は似るのが普通であって、ごく稀に親子に際だった違いが見られる」という人間の親子の観察の結果です。
これらのことわざが示すように、\\
『血のつながりがあるときに、ある特徴について似たり似なかったりするものだけれども、
似ることの方が似ないことよりも多い』\\
ということに気づいたことを持って、遺伝現象に気づいたと言えるでしょう。
さらには、\\
『遺伝する特徴もある一方で、遺伝しない特徴もある』\\
ことに気づいたときに、
遺伝現象には原因があるだろうことに思いが至ったと思われます。これが遺伝学の第一歩です。


\subsection{生物の特徴　形質とフェノタイプ(表現型)}

生物が持つ特徴は\textcolor{red}{形質}\index{けいしつ@形質}\footnote{
ボールド単語は索引掲載語です。索引には、その単語を丁寧に説明したページが示されているので、ウェブのリンク
を辿るように索引を介して、適宜、説明を辿ってください(TeXでは緑色が「丁寧に説明してあるページ」このあたり、うまくフォント
等で調整すること)
}
と呼ばれます。
特徴はなんでも形質です。
どんな形質に興味を持つかが、研究の内容を決めますし、
新たな解析の視点を持つことは、新しく形質を定義することとも言い換えられます。
そういう意味で、形質を考える視点を整理することは有用です。

形質を分類するときの一つの方法は、五感(視覚・聴覚・味覚・嗅覚・触覚)
のどれによる評価であるかが挙げられます。
別の分類の仕方としては、解剖学・構造的、生理・機能的、分子生物学・薬理学的という分け方
があります。
また、個体そのものに備わった特徴であるのか、
個体が外敵や環境との関係の持ち方(行動・免疫反応など）なのか、という分類もできます。
そのほかには、数学的な概念なのか、物理的に測定するものなのか、変化する(化ける)様子
なのか、と言う視点も形質の分類に役立ちます。
これらの形質を観察することでデータが得られます。

この形質の観察データが\textcolor{green}{フェノタイプ}\index{フェノタイプ@フェノタイプ}
(\textcolor{green}{表現型}\index{ひょうげんけい@表現型})です。

\subsection{同一性と多様性}
生物は自分と同じものを次世代に残すことを１つの特徴とします。
\textcolor{green}{無性生殖}\index{むせいせいしょく@無性生殖}\footnote{
無性生殖：１つの個体が単独で新しい個体を生み出すような生殖形式
}では
子は親の遺伝子をそのまま引き継ぎますが、
ヒトをはじめとする
\textcolor{green}{有性生殖}\index{ゆうせいせいしょく@有性生殖}
\footnote{有性生殖：男と女という異なる
性別の個体から構成される種がとる生殖戦略で、子の遺伝子は両親からその半分ずつからなる}
の生物ではまったく同一の
個体を残すわけではなく、両親の遺伝子の半分ずつを引き継ぎます。(図1.1)

\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch1/sexual-asexual/sexual-asexual.eps}
   \includegraphics{./fig/1-1.eps}
\caption{有性生殖と無性生殖。親子の関係は\textcolor{red}{グラフ}\index{グラフ@グラフ}で表されます}

 \end{center}
 \label{fig:one}
\end{figure}
\textcolor{green}{適応}\index{てきおう@適応}
という面から言うと、親がうまく生きているわけだから、
子も親と同じならうまく行くだろうと考えた戦略と言えるかもしれません。
他方、個体ではなく、個体が属する種に着目してみます。
種の個体がどれもみな同じだと、環境が変化したときに、全個体がそろって
生きにくくなることもあるでしょう。
それは得策ではないと考えることができます。
環境の変化があっても、いろいろな個体がいれば、環境の変化が起きても種の全滅する可能性は減りますから、
いろいろな個体で構成しておくことも得策と考えられます。
遺伝現象は、このように、
同一性の確保と多様性の確保の二面性を持っています。

\section{遺伝子}
\subsection{遺伝子とは}
遺伝現象は、親子関係と形質との間の関係です。
なぜ、親子関係があると形質が似るのか、それが何によってもたらされるのかわかりませんでした。
その遺伝現象をもたらす何かの基本単位が\textcolor{green}{遺伝子}\index{いでんし@遺伝子}
です。
わからないなりに、遺伝現象の原因となる実体の存在が信じられ、それはいくつもあると予測されましたので、
遺伝現象の基本単位を遺伝子としました。
現在の生物学で言えば、遺伝現象の原因となる実体が\textcolor{green}{ゲノム}\index{げのむ@ゲノム}
であり、
ゲノムの構成要素が遺伝子と言えるでしょう。
遺伝子は親子の間で共有されるものであるべきです。
習慣や環境も親と子の間で共有されますが、個体の始まりは受精卵であり、
受精卵は母親の子宮の中で母親からの卵子と父親からの精子とが合わさって出来ます。
卵子と精子は、それぞれ母と父からの遺伝情報を染色体として持ちこみかすから、
ここに遺伝子があるはずです。
実際、この染色体が遺伝子のすべてではないかもしれないものの、大部分を担うと考えられています。
染色体は長いＤＮＡ分子とそれをとりまくタンパク質とからできています。
ＤＮＡ分子は次章で説明する通り「情報を担う物質」として優れた特長を持っています。

\subsection{染色体}
\textcolor{green}{染色体}\index{せんしょくたい@染色体}
は非常に長い紐のような構造になっています。
受精に際して、卵子と精子から持ち込まれた染色体のセットは、新しい個体の
遺伝情報となります。
ヒトの身体は多数の細胞出出来ていますが、すべての細胞は原則として、
同じ染色体のセットを持ちます。(図1.2)
染色体は大きく２つに分けられます。
\textcolor{green}{核染色体}\index{かくせんしょくたい@核染色体}と
\textcolor{green}{ミトコンドリア染色体}\index{みとこんどりあせんしょくたい@ミトコンドリア染色体}です。
核染色体は22対の\textcolor{green}{常染色体}\index{じょうせんしょくたい@常染色体}と
1対の\textcolor{green}{性染色体}\index{せいせんしょくたい@性染色体}とからなります。
性染色体はX,Yの２種類で女はXを２本、男はXとYを1本ずつ持ちます。
受精に際して、卵子からは１セットの核染色体(２２本の常染色体と１本の性染色体(Ｘ染色体))と
多数のミトコンドリア染色体が、
精子からは１セットの核染色体(２２本の常染色体と１本の性染色体(ＸもしくはＹ染色体))が
持ち込まれます。父親からはミトコンドリア染色体は受け継ぎません。
対となる常染色体はよく似ていますが、2本の性染色体はかなり違います。
X染色体とY染色体は共通の部分も持ちますが、
それぞれに特異的な部分があり、長さも大きく異なります。
ミトコンドリア染色体は、紐の両端がくっついた輪の形(環状)をしていますが、
この環状染色体はバクテリアの染色体に見られる特徴です。
はるか昔に真核細胞が今のミトコンドリアの祖先であるバクテリアを
細胞内に取り込んだことに由来しています。
ミトコンドリア染色体は核染色体に較べて非常に小さく、長さにして1万分の1程度ですが、
個々の細胞は多くのミトコンドリア染色体を持ちます。
その数は細胞によってばらつきがあり、100から10000個くらいです。

\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch1/nucDNAmtDNA/nucDNAmtDNA/chromosomes.eps}
\includegraphics{./fig/1-2-1.eps}
\includegraphics{./none.eps}
%\includegraphics{./fig/1-2-2.eps}
  %\includegraphics[width=50mm]{./Figures/Ch1/nucDNAmtDNA/freeKyogijyoAndWagomu.eps}
\caption{
上図の上段・下段はそれぞれ母由来・父由来のセットです。
下図は、核染色体とミトコンドリア染色体の大きさのイメージ図です。
ミトコンドリア染色体(約15000塩基対)を輪ゴムとすると、核染色体の短いものだと、１本(約0.5億塩基対)は陸上トラックを1周(400メートル)するくらいの１本の紐に相当します。
長い核染色体だと、トラック５周分くらいになります。
ミトコンドリア染色体は１細胞あたり100から10000個あるので、
箱売りの輪ゴム数箱から数十箱分の輪ゴムがあることになります。
\textcolor{red}{競技場の写真は"http://eyes-art.com/pic/attention.html"より　この写真に相当するイラストが入れられたらいいな・・・} }
 \end{center}
 \label{fig:one}
\end{figure}
本書ではフリーの統計ソフトRのソースを使って内容の理解を助けることにしています。
付録(18.1)を参考に、\textcolor{red}{Rのインストール}\index{Rのいんすとーる@Rのインストール}をして、
ヒト染色体の大きさを表す棒グラフを描いてみます。
\begin{lstlisting}
#染色体の長さ(単位：塩基対数)
#chromLenに24個の数値のベクトルを代入する
chromLen<-c(247249719,242951149,199501827,191273063,180857866,
170899992,158821424,146274826,140273252,135374737,134452384,
132349534,114142980,106368585,100338915,88827254,78774742,
76117153,63811651,62435964,46944323,49691432,154913754,57772954)
#barplot(棒グラフ)を描く。データはchromLen、棒の名前は、1:22(1,2,...,22)と"X","Y"のベクトル。色は黒
barplot(chromLen,names=c(1:22,"X","Y"),col="black")
\end{lstlisting}

\subsection{遺伝子座　アレル　ハプロタイプ　ディプロタイプ　フェノタイプ}
遺伝情報は染色体が担っています。
遺伝子は遺伝情報の単位ですが、この単位はひも状の染色体大きく長い染色体の特定の部分に存在します。
この位置のことを\textcolor{green}{遺伝子座}\index{いでんしざ@遺伝子座}
（\textcolor{green}{ローカス}\index{ろーかす@ローカス}）と言います。
染色体とその主要構成要素である\textcolor{red}{ＤＮＡ}\index{DNA@DNA}分子は線状の構造をしているので、
遺伝子座は、線状の地図の位置として表現されます。
遺伝子は、遺伝情報を担うものであって、染色体・ＤＮＡ上の位置で表すことができるものである、
とも言い換えられるかも知れません。

\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch1/geneAllele/geneAllele.eps}
  \includegraphics{./fig/1-3.eps}
\caption{X,Y,Zが遺伝子座。Xは長さが1塩基、Yは複数塩基の長さのある線分、Zは複数の線分の集合であるような遺伝子座。
△は上下で異なる塩基の位置を示す。}
 \end{center}
 \label{fig:one}
\end{figure}

ここで言う遺伝情報は、あるタンパク質のアミノ酸配列に関する情報の書かれた範囲全体を
指してもよいですし、その発現制御領域のことを指してもよく、
また、それらを合わせたものを指しても、反対にそれらのごく一部だけを
指してもよいです。
後述するように染色体の主要構成成分であるＤＮＡ分子は、
４種類の\textcolor{green}{塩基}\index{えんき@塩基}(A,T,G,C)と呼ばれる部品が線状に並んだ構造をしていますが、
その塩基１つ１つにも遺伝情報はありますし、それが100万個連なった塊にも情報があります。
長さが１塩基の部分を遺伝子座とみなし、染色体によって異なる塩基が対応しているとき、
そこの遺伝子座を\textcolor{green}{１塩基多型}\index{いちえんきたけい@１塩基多型}(\textcolor{green}{SNP}\index{SNP@SNP})とよび、
その座を占める塩基のバリエーションを\textcolor{green}{アレル}\index{アレル@アレル}と言います(図1.3 X)。

ある長さにわたって遺伝子座とみなし、染色体によってその遺伝子座の
塩基の並びが異なるとき、その配列の一つ一つがアレルで、
\textcolor{green}{ハプロタイプ}\index{ハプロタイプ@ハプロタイプ}とも呼びます(図1.3 Y)。

また、タンパク質はアミノ酸がつながった構造をしていて、その情報がＤＮＡ上にコードされていますが、
そのＤＮＡ上の情報は連続してコードされていることもあれば、いくつかの線分にわかれて並んでいることもあります。
そのコードしている線分の一つ一つを\textcolor{green}{エクソン}\index{エクソン@エクソン}と呼び、
エクソンとエクソンの間を\textcolor{green}{イントロン}\index{イントロン@イントロン}と呼びます。
このエクソンとイントロンとで構成されるDNA配列全体を遺伝子座とすることもできます(図1.3 Z)。

このように、遺伝子座に配列のバリエーションがあるとき、
そのバリエーションの一つ一つがアレル(\textcolor{green}{対立遺伝子}\index{たいりついでんし@対立遺伝子})です。


DNA配列のタイプがアレルですが、遺伝子座の取り方によって別名があったり、
名前の付け方が違ったりします。\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
遺伝子座の単位&1塩基&塩基の組み合わせ・長さのある配列&遺伝子\\ \hline
アレルの別称& &ハプロタイプ&遺伝子タイプ\\ \hline
例&"A","G"&"AGCT","CCAT","0011","1101"&"野生型","変異型X","*0401","タイプ2" \\ \hline
\end{tabular}\\
染色体が遺伝子座に持つタイプがアレルです。
個体は、染色体のセットを持ちます。
常染色体の場合には、個体は両親に由来する２つのアレルを持ちます。
個人が遺伝視座に持つタイプ(アレルの組み合わせ)が\textcolor{green}{ジェノタイプ}\index{ジェノタイプ@ジェノタイプ}
(\textcolor{green}{遺伝子型}\index{いでんしがた@遺伝子型})です。
常染色体の場合には２つのアレルの組み合わせのタイプなので、
\textcolor{green}{ディプロタイプ}\index{ディプロタイプ@ディプロタイプ}とも呼びます。\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
単位&1塩基&塩基の組み合わせ・長さのある配列&遺伝子\\ \hline
例&"A,A","A,G"&"AGCT,AGCT","0011,1101"&"野生,野生","*0401,*0901","2,2" \\ \hline
\end{tabular}\\
個体はジェノタイプを持つ一方で、\textcolor{red}{フェノタイプ}\index{フェノタイプ@フェノタイプ}も持ちます。

これらは、単位をDNA分子とするか、そのペアとするか、個体とするかの違いがありますが、
それぞれの単位でのタイプのことです。
その名称がアレル(ハプロタイプを含む)、ジェノタイプ(ディプロタイプ)、フェノタイプです。\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
単位&染色体&染色体のペア&個体\\ \hline
名称&アレル(ハプロタイプ)&ジェノタイプ(ディプロタイプ)&フェノタイプ \\ \hline
\end{tabular}

\subsection{２倍体・ホモ接合・ヘテロ接合・ジェノタイプ・フェノタイプ・遺伝形式}
常染色体は父母由来の１本ずつの対で存在します。
遺伝情報を重複して２組持っていることを意味します。
このように２組持っている生物を\textcolor{green}{２倍体}\index{にばいたい@２倍体}と言います。
ある遺伝子座について、同じアレルを２つ持っているとき、その個体は、
その遺伝子座について\textcolor{green}{ホモ接合体}\index{ほもせつごうたい@ホモ接合体}
であると言い、異なるアレルを持っているときには、
\textcolor{green}{ヘテロ接合体}\index{ヘテロせつごうたい@ヘテロ接合体}であると言います。

ここから先では、２倍体生物のみを考えます。ヒトも２倍体です。
２倍体生物は、常染色体上の遺伝子座にはアレルが２つあります。
この２つのアレルの持ち方を個体のジェノタイプと言います。
これに対して、個体の形質の様子がフェノタイプです。
ジェノタイプがフェノタイプに影響を与える関係の中でもっとも明快な関係に
\textcolor{green}{優性}\index{ゆうせい@優性}遺伝形式と
\textcolor{green}{劣性}\index{れっせい@劣性}遺伝形式という関係があります。
これについて考えます。
今、２つのアレルMとmがあり、ある特徴を持つか持たないかの区別として、
「あり」と「なし」というフェノタイプがあるとします。
個体はMM,Mm,mmのいずれかのアレルのパターン(ジェノタイプ)でアレルを持ちます。
アレルmがあるフェノタイプをもたらす因子であるとします。
\begin{table}
\renewcommand{\footnoterule}{}
\begin{center}
\begin{minipage}{10cm}
\caption{ジェノタイプとフェノタイプの関係　$2\times 3$表}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&MM&Mm&mm&計\\ \hline
あり&$n_{10}$&$n_{11}$&$n_{12}$&$n_{1.} $ \\ \hline
なし&$n_{20}$&$n_{21}$&$n_{22}$&$n_{2.}$ \\ \hline
計&$n_{.0}$&$n_{.1}$&$n_{.2}$&$n_{..} $\\ \hline 
\end{tabular}
\footnotetext{
$n_{ij}$の$i,j$はそれぞれ行と列の番号を表していて、特に列番号はmの本数を表しています。
また、最右の列は行に関する和、最下行は列に関する和を表しています。
\begin{equation*}
\sum_{i=1}^2 n_{ij}=n_{.j}; \sum_{j=0}^2 n_{ij}=n_{j.}; \sum_{i=1}^2 \sum_{j=0}^2 n_{ij}=n_{..}; 
\end{equation*}
記号$\sum$については、付録「本書で使う数式記号」を参照
}
\end{minipage}
\end{center}
\end{table}


フェノタイプが「あり」である率$f_i=\frac{n_{1j}}{n_{.j}},j=0,1,2$はジェノタイプごとに異なります。
これはジェノタイプ別の\textcolor{green}{浸透率}\index{しんとうりつ@浸透率}と呼ばれます。

MMを基準として、各ジェノタイプの浸透率が何倍高いか($\lambda_j=\frac{f_j}{f_0}$)は、
\textcolor{green}{ジェノタイプ相対リスク}\index{ジェノタイプそうたいリスク@ジェノタイプ相対リスク}
(\textcolor{green}{genotype relative risk}\index{genotype relative risk@genotype relative risk}: GRR)
と呼ばれます。
$\lambda_0=1$です。\\
\begin{table}
\begin{center}
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&MM&Mm&mm\\ \hline
GRR& $\lambda_0=1$ & $\lambda_1$ & $\lambda_2$  \\ \hline
\end{tabular}
\end{center}
\end{table}

ヘテロ型(Mm)のGRRがホモ型(mm)のそれと等しいとき($\lambda_1=\lambda_2$)
、アレルmは１本でも２本でも同じ強さで
フェノタイプに影響を与えていることになりますが、このとき、mのこの表現型への影響の仕方を
優性遺伝形式と呼びます。
逆に、ヘテロ型のGRRが基準のホモ型(MM)のそれと等しい($\lambda_1=\lambda_0$)とき、
アレルmは２本揃って初めてフェノタイプに影響を与えていることになります。
この形式を劣性遺伝形式と呼びます。
優性とも劣性ともつかない中間的な形式には大きく分けて２つの定義がありますが、\\
\begin{equation*}
\lambda_1=x \lambda_2 + (1-x) \lambda_0
\end{equation*}
と表せば、
x=0が劣性、x=0.5が\textcolor{green}{相加的}\index{そうかてき@相加的}(\textcolor{green}{additive}\index{additive@additive})
、x=1が優性です。
\begin{equation*}
\lambda_1=\lambda_2^y \times \lambda_0^{1-y}
\end{equation*}
と表せば、y=0が劣性、y=0.5が\textcolor{green}{相乗的}\index{そうじょうてき@相乗的}
(\textcolor{green}{multiplicative}\index{multiplicative@multiplicative})、y=1が優性です。
x=0.5のときは、
\begin{equation*}
\lambda_1=\frac{\lambda_0+\lambda_2}{2}
\end{equation*}
($\lambda_0$と$\lambda_2$の相加\textcolor{green}{平均}\index{へいきん@平均}
\footnote{平均と言えば、\textcolor{green}{相加平均}\index{そうかへいきん@相加平均}
(算術平均 $A=\frac{\sum_{i=1}^n x_i}{n}=\frac{1}{n}(x_1+x_2+...+x_n)$)、
\textcolor{green}{相乗平均}\index{そうじょうへいきん@相乗平均}
(幾何平均 $G=(\prod_{i=1}^n x_i)^\frac{1}{n}=(x_1\times x_2 \times ... \times x_n)^{\frac{1}{n}}$の他に、
\textcolor{green}{調和平均}\index{ちょうわへいきん@調和平均}
($H=\frac{n}{\sum_{i=1}^n \frac{1}{x_i}}$)
がありますが、調和平均に基づいた遺伝形式の議論はあまりされません。
２要素の場合には、$G=\sqrt{AH}$であることや、一般化した形式
$M_p=(\frac{1}{n}\sum_{i=1}^n x_i^p)^\frac{1}{p}$の平均の定義(一般化平均)を使えば
相加平均、相乗平均、調和平均は、$M_{1},M_{\infty},M_{-1}$であるとして
捉えておくことは悪くないです。}
)となるので、
additive(相加的) モデル、\\
y=0.5のときは、
\begin{equation*}
\lambda_1=\sqrt{\lambda_0 \times \lambda_2}
\end{equation*}
($\lambda_0$と$\lambda_2$の相乗平均)となるので、multiplicative(相乗的)モデルと呼びます。

また、アレルmを１本持つ効果が２本持つ効果以下であるときは、$0 \le x,y \le 1$ですが、
アレルmを１本持つ効果が２本持つよりも強いことあることもあり、
それは\textcolor{green}{超優性}\index{ちょうゆうせい@超優性}
(\textcolor{green}{Overdominance}\index{Overdominance@Overdominance})と
呼ばれますが、それは$x,y > 1$の場合に相当します。$x,y < 0$の場合はヘテロ型が着目している表現型を持たないこと
について超優性である場合になり、$x, y \in (-\infty,\infty)$により、GRRに基づき、
すべての遺伝形式が網羅できることになります。

\chapter{DNA・RNA・タンパク質・形質}
\section{DNA２重鎖}
染色体は非常に長い\textcolor{green}{ＤＮＡ}\index{DNA@DNA}分子と多くのタンパク質からなっています。
ＤＮＡ分子は\textcolor{green}{塩基}\index{えんき@塩基}と呼ばれる
４種類の化学構造体(ＡＴＧＣ)を一列に並べた構造をしており、
この４種類の塩基が作る文字列を\textcolor{green}{塩基配列}\index{えんきはいれつ@塩基配列}と言います。
遺伝情報はこの塩基配列と、DNA分子の化学修飾状態によって伝えられる場合が
ほとんどですが、特に塩基配列は明快に情報を保持し伝えることができ、遺伝子の本体
とも言えるので、ここから先は、遺伝情報は塩基配列が伝えるものとして話しを進めます。

DNAの配列はＡＴＧＣの４アルファベットの連なりであり、その配列には向きがあります。
順方向と逆方向の２本の塩基配列が、対を成してよじれることで、二重鎖という構造をとります。
対をなすためには、４文字に対をなす規則が必要です。
ＡとＴがペア、ＧとＣがペアになります。
対になる２つの文字列は異なっていますから、ＤＮＡ二重鎖にある塩基配列には、
２通りの読み方があることになります。(図2.1)

ある配列に対して、それと対を成す配列を\textcolor{green}{相補鎖}\index{そうほさ@相補鎖}配列と言います。
\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch1/DNA/配列ＡＴＧＣ２.eps}
\includegraphics[width=50mm]{./fig/1-4-2.eps}
\caption{ある両親から生まれた２人の子のDNA配列です。
塩基の部品は向きと対になるための凹凸を持たせてあります。
最上段の父由来のＤＮＡ分子は"ATCAGGTT"という
配列と、その逆の"AACCTGAT"という配列の２通りの読み方を持っています。
子１の父由来・母由来のセットは、全く同一なので、子１は、この配列範囲について、
すべてホモ接合体です。
子２の母由来のＤＮＡ分子は、左から右への読み方向での５番目の塩基がＴではなくＧ
に変わっています。
ＤＮＡ２重鎖では、対をなす塩基は決まっているので、
この分子ではもう一つの配列がAからCに変わっています。
子２は、この箇所の塩基の対が父由来と母由来とで異なっていますからヘテロ接合体です。}

 \end{center}
 \label{fig:one}
\end{figure}


ＤＮＡが２重鎖として、常に両方向の文字列を持っていることには少なくとも、
２つの意味がありそうです。
第一に、２本の文字列は相互にもう片方の文字列に関する情報のすべてを持っていることから、
情報のバックアップとしての役割を果たします。
これは、ＤＮＡ分子の複製をしたり、壊れかけたときに修復をしたりするときに役立ちます。
第二に、いつでも読み取れる状態の情報の量を２倍にしておくことができます。
１本鎖として配列を保存していた場合には、その鎖の配列の情報はすぐに
読み取れますが、逆方向の配列の読み方は、相補配列がどうなっているかを
作るひと手間がかかります。２重鎖として持っていればどちらの向きの鎖が持つ情報もすぐに
取り出すことができます。
\subsection{複製・変異・組み換え}
１個の受精卵から、多数の細胞が作られ、個体が形成されるとき、DNA２重鎖は複製されて
すべての細胞に同じ染色体のセットが渡されます。
個体の一生の間にＤＮＡ配列に変化することがあります。
その変化は修復されることが普通ですが、修復されずに残ることもあり、そのような変化を\textcolor{green}
{体細胞変異}\index{たいさいぼうへんい@体細胞変異}と呼びます。
体細胞変異はその細胞から分裂して出来た細胞に受け継がれます。
癌というのは、この体細胞変異が原因で同じＤＮＡ変異を共有する細胞の数が増え、不都合が生じた状態のことです。
この体細胞変異は子に伝えられることはありません。
他方、卵子と精子を作る仮定でもＤＮＡが複製されますが、その途中に\textcolor{green}{変異}\index{へんい@変異}
(\textcolor{green}{生殖細胞変異}
\index{せいしょくさいぼうへんい@生殖細胞変異}と呼ばれて
体細胞変異と区別されます)が起きます。
生殖細胞変異は卵子・精子が作られるときに確率的に一定数は必ず起きると考えてよいです。
この変異は受精卵に伝えられ、子の細胞のすべてに引き継がれます。

また、親から子へのＤＮＡ配列の伝達にあたっては、親のＤＮＡ配列の半分が伝えられます。
第１章で説明したように、親は対になった２本ずつの染色体を持っていますが、そのうちの１本分が卵子・精子に
引き継がれ、その合体が受精卵の新しい遺伝子のセットを作ります。
ここで、２本のうちの「１本」が伝えられるのではなく、「１本分」が伝えられると書きました。
これは、卵子・精子に渡すＤＮＡ配列のセットを作るときに、２本の対になる染色体の一部分ずつを
継ぎ合わせて、１本分の染色体にしてそれを引き渡すことを意味しています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./fig/1-5.eps}
  %\includegraphics{./Figures/Ch1/recombination/rec.eps}
\caption{上段の２本はある個体の染色体の対です。黒が母親由来、白が父親由来とします。
下段は、この個体からできる卵子または精子の染色体です。
黒白のモザイク１本分が受け継がれています。}
 \end{center}
 \label{fig:one}
\end{figure}

図2.2に示したように、親は１対の染色体を持ちます。その２本を白と黒で表しています。
卵子・精子へは１本分の染色体が渡されます。
渡される１本は白と黒とが交代したモザイクになります。
１本分として過不足ないものができます。
白黒の交代する部分では、染色体が\textcolor{green}{交叉}\index{こうさ@交叉}したと言い、
白黒が交代することを\textcolor{green}{組換え}\index{くみかえ@組換え}と言います。
２本の染色体が作ったモザイク状の染色体を\textcolor{green}{組換え体}\index{くみかえたい@組換え体}と言います。
交差する箇所は同じではないので、モザイクの様子が異なります。

\subsection{起源が同じ IBD}

最初に常染色体上のある1つの遺伝子座について考えます。
これは染色体が１対しかないような生物での遺伝について考えることと同じです。
一卵性双生児は、同一の受精卵を起源にしているので、２アレルの起源が確率１で一致しています。
親子の関係の場合、子は１アレルを片親から引き継いでいますが、もう片方のアレルは別の親から引き継いでおり、
その起源は異なっていると考えることにすれば(近親婚でないと仮定すれば)、
１アレルの遺伝子起源が一致する確率が１で、
2アレルの起源が一致する確率は０、いずれのアレルも一致しない確率も０です。
兄弟姉妹(\textcolor{green}{同胞}\index{どうほう@同胞})の場合は、
片親から引き継いだアレルが一致する確率が0.5ですので、
両親から引き継いだアレルがともに一致する確率は、$0.5\times 0.5=0.25$。
両親から引き継いだアレルがそろって起源を異にするする確率も同様に0.25。
2アレルのうち片方が起源を同じにする確率がその残りで0.5です。
このようにアレルの起源が同じであることを、\textcolor{green}{IBD}\index{IBD@IBD}
(\textcolor{green}{Identity by descent}\index{Identity by descent@Identity by descent})と言い、
IBDである数(IBD数)は０か１か２かのいずれかになり、その確率は血縁関係によって決まります。
ですから遺伝的な近さを、IBD数の確率によって表現することがあります。

\begin{table}
\begin{center}
\caption{血縁関係とIBD数の確率の関係}
\begin{tabular}[htb]{|c|c|c|c|c|c|c|} \hline
　&\multicolumn{3}{|c|}{IBD数別確率}&IBD数の期待値&\multicolumn{2}{|c|}{一致率}\\ 
血縁関係&$2$&$1$&$0$& &期待値&分散\\ \hline 
自身・一卵性双生児&$1$&$0$&$0$&$2$&$1$&$0$ \\ 
親子&$0$&$1$&$0$&$1$&$\frac{1}{2}$&$0$ \\ 
同胞&$\frac{1}{4}$&$\frac{1}{2}$&$\frac{1}{4}$&$1$&$\frac{1}{2}$&$\frac{1}{8}$ \\ 
祖父母-孫&$0$&$\frac{1}{4}$&$\frac{3}{4}$&$\frac{1}{4}$&$\frac{1}{8}$&$\frac{3}{64}$ \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsection{１つの数値で表して扱いやすくする　IBDの期待値}
IBD数が0,1,2になる確率は、確かに、遺伝的な近さの情報として有用ですが、
血縁関係ごとに３つの数値を持つベクトルを情報として扱うのは、不便なこともあります。
血縁関係の近さに興味があるときには、１つの数値で表されているのが便利です。
\footnote{
血縁関係を１つの数値で表す方法には、\textcolor{green}{近縁係数}\index{きんえんけいすう@近縁係数}
(kinship coefficient)や相関係数など、
遺伝学・集団遺伝学ではいくつかの異なる定義と名称とがあります。
それぞれの定義や意味がありますが、一番大切なことは、
『複数の値のセットで表されている情報から１つの代表的な値に変えて、理解を助けていること』です。
}
ここでは、３つの値が\textcolor{red}{確率変数}
\index{かくりつへんすう@確率変数}
(３つの場合ですべて尽くされ、それぞれを観測する確率を足し合わせると１になる)である
ことから、確率変数の代表値として有用な\textcolor{green}{期待値}\index{きたいち@期待値}
を代表的な値としてみます。
期待値とは、いろいろな値をとりうるときに、
それぞれの値をとる確率で重み付けをした\textcolor{red}{相加平均}\index{そうかへいきん@相加平均}のことです。
$X=\{x_1,x_2,...\}$という値を$P=\{p_1,p_2,...\} (\sum_{i} p_i =1)$という確率でとるときに、
期待値は

\begin{equation*}
E(X)=\sum_{i} x_i p_i
\end{equation*}
で表されます。
ですから、自身・一卵性双生児の場合に$E(X)=2$となります。
自身の血縁関係の強さは、最強の血縁関係だと考えてこれを１とすることにして、
$\frac{E(X)}{2}$としてみるのも得策でしょう。
これをアレルの一致率と呼ぶことにします。
表の一致率の期待値がこの値です。

親子と同胞とを較べてみることにしましょう。
これらはIBD数の確率が、{0,1,0}と{0.25,0.5,0.25}と異なっています。
しかしながら、アレルの一致率の期待値は
どちらも$\frac{1}{2}$で
同じになっています。
３つの値で表されていた血縁関係の強さを、１つの数値で表したことにより、
ある側面で便利になりましたが、情報が失われたことがわかります。

\subsubsection{平均　分散　モーメント　期待値}
\textcolor{green}{期待値}
\index{きたいち@期待値}は、値$X=\{x_1,x_2\}$をとる確率$P=\{p_1,p_2,...\} (\sum_{i} p_i =1)$が
わかっているときに
\begin{equation*}
E(X)=\sum_{i} x_i p_i
\end{equation*}
で表されます。
このような分布の$X$の期待値は、分布の\textcolor{green}{平均}\index{へいきん@平均}とも言います。
\textcolor{green}{分散}\index{ぶんさん@分散}は値のばらつきを表す指標で、
\begin{equation*}
V(X)=\sum_{i} p_i (x_i-E(X))^2
\end{equation*}
と算出されます。

平均も分散もともに分布の様子を説明する指標です。
これらを統一的に説明するものに\textcolor{green}{モーメント}\index{モーメント@もーめんと}があります。
モーメントはより一般的な分布の指標で、平均と分散はともにモーメントのうちの1つです。
モーメントは次数と中心の取り方で決まります。
中心の取り方としては原点(O)か平均かの2通りを考えるのが普通です。

$c$を中心としたk次モーメントは
\begin{equation*}
\mu_k(c) = \sum_{i} p_i (x_i-c)^k
\end{equation*}
で表され、原点を中心としたそれは
\begin{equation*}
\mu_k(0) = \sum_{i} p_i x_i^k
\end{equation*}
平均を中心としたそれは
\begin{equation*}
\mu_k(E(X))=\sum_{i} p_i(x_i-E(X))^k
\end{equation*}
です。
平均とは、原点を中心にした１次のモーメントになります。
分散は、平均を中心にした2次のモーメントです。\\
モーメントをRで計算することで、\textcolor{green}{Rの関数の作り方}\index{Rのかんすうのつくりかた@Rの関数の作り方}を覚えておくことにします。

値のベクトル(x)とその確率のベクトル(p)、次数(order)、
中心を原点にするか平均にするか(center=FASLEは平均にしない、TRUEはする)
を引数としてモーメントを計算する関数は以下の通りです。
作成した関数の中で、その関数自体を呼び出していることに注意してください。\\
R2-1
\begin{lstlisting}
#関数を作るときには、function()関数を使います
#"momentProb"という名前の関数を作ります
#function()関数は、引数を()の中に、処理を{}の中に書きます。
#xが値ベクトル、pがその確率ベクトル
#引数のうちx,pは"="で値を指定していませんが、
#orderとcenterは値が指定されています
#x,pは引数を与えないといけませんが、orderとcenterは
#値を与えなければ、デフォルト値(1,FALSE)が用いられます
momentProb<-function (x, p,order = 1, center = FALSE) 
{
 if(center) # 平均を中心とするならxの値から平均を引く
  x <- x - momentProb(x,p,order=1,center=FALSE)
 sum(x^order*p)
}
\end{lstlisting}

学校でのテストの点数などから平均や分散を計算するときには、
点数のデータが「標本」であることから、
標本平均、標本分散と呼ばれますが、その場合には、すべての標本の確率を等しいものとして、
計算します。\\
R2-2
\begin{lstlisting}
momentX<-function (x, order = 1, center = FALSE) 
{
 # すべての標本に等確率 rep(1,length(x))/length(x)を与えたmomentProb()に同じ
 momentProb(x,p=rep(1,length(x))/length(x),order=order,center=center) 
 # length(x)はxの要素数、rep(v,L)はvがL個並んだベクトル
}
\end{lstlisting}
ここでは、全員の点数を平等に評価しているとも言えますし、
全員の確率が等しいと考えて、全員の確率の和が１になるように、各人の確率を定め、
各人の値の次数乗にその確率をかけている、とも言えます。

同胞のアレルの一致率の平均と分散を求めてみると次のようになります。
\begin{lstlisting}
x<-c(1,0.5,0) # IBD数別の一致の値
pDoho<-c(1/4,1/2,1/4) # 同胞のIBD数別確率
momentProb(x,pDoho,order=1,center=FALSE) # 期待値
momentProb(x,pDoho,order=2,center=TRUE) # 分散
\end{lstlisting}
この結果が表に記されています。
一卵性双生児、親子、同胞のそれぞれで分散は0,0,$\frac{1}{8},\frac{3}{64}$です。
親子と同胞の一致率の期待値はともに0.5で同じですから、
両者のIBD数分布の違いは、期待値には現われず、分散に現われているといえます。

\subsection{同胞のアレルの一致率}
\subsubsection{染色体は23対}
染色体が１対の場合を考えました。
1つの遺伝子座について考えたとも言えます。
実際には、ヒトのゲノムＤＮＡは23対の染色体・ＤＮＡ分子に分かれています。
今、単純化して考えるために、k対の常染色体のみが存在し、それらは同じ長さを持つものとします。
ｋ対の常染色体はそれぞれ父母起源の１本ずつです。
この起源の一致の程度を考えることにします。
一卵性双生児の場合には、ｋ対、2k本の常染色体のすべてで一致率が１です。
親子の場合は、ｋ対2k本のうち、k本分
\footnote{親から子に伝わる染色体はモザイクですから、１本
が一致するのではなく、「１本分」が一致します}
が必ず一致していて一致率が0.5です。
したがって、一致率は、一卵性双生児では平均1、分散０、親子では、平均0.5、分散０です。

さて、同胞についてです。
ｋ対、2k本分のうち、どれくらいが一致するでしょうか。
IBD数を0,1,2として表してきましたが、
表し方を次のように変えてみます。
同胞が両親から引き継ぐアレルについて、それぞれ、同胞間で起源が一致した場合に１、
一致しなかった場合を0で表すと、
第一番目の染色体の１対、2本が一致する場合は、"11"、父由来は一致して母由来は一致しない場合を”10”、
逆に父由来は一致せず母由来が一致する場合を”01”、両方とも一致しない場合を”00”と表すことにします。
それぞれ場合の確率は等確率で0.25ずつです。
0,1の数字の並べ方(\textcolor{red}{重複順列}\index{ちょうふくじゅんれつ@重複順列})は、$2^2$の４通りで、
各々の確率は$\frac{1}{2^2}$でした。
一致率が1,0.5,0の確率がそれぞれ0.25,0.5,0.25です。

k対、2k本の染色体について考えると$2k$個の0,1の並べ方について
考えることになります。並べ方は$2^{2k}$通りあり、各々の確率は
\begin{equation*}
\frac{1}{2^{2k}}
\end{equation*}
です。
$2k$本の染色体のすべてで起源が同一になる場合というのは、
”11...11”という並びのみですから、その確率は$\frac{1}{2^{2k}}$です。
これが、一致率１の確率です。
起源の同じ$2k$本のうち$i$本の起源が一致する場合というのは、
一致率が$\frac{i}{2k}$の場合ですが、これは、$2k$からiを取る\textcolor{red}{組み合わせ}\index{くみあわせ@組み合わせ}
になるので、その確率は、 
\begin{equation*}
\binom{2k}{i}=\frac{2k!}{i! (2k-i)!} \;\;\;\; \footnote{
２項分布：$1=1^k=(p+(1-p))^k=\sum_{i=0}^k A_{k,i} p^i (1-p)^{k-i}$としたときに、$A_i$を２項
係数と言い、$A_i=\frac{k!}{i! (k-i)!}=\frac{k(k-1)...1}{i(i-1)...1 \times (k-i)(k-i-1)...1}$
}
\end{equation*}
 と$ \frac{i}{2^{2k}} $との積になります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./fig/PartI-003.eps}
\caption{同じ長さの染色体1,5,23本で構成されていると仮定したときの、同胞のアレルの
一致率の分布}

 \end{center}
 \label{fig:one}
\end{figure}
染色体数が1,5,23の場合を図に示しました。
山が高い方から、染色体数が1,5,23です。
染色体数が増えるに連れて、山が細かくなると同時に、一致率が0.5付近である確率が高くなっていきます。

染色体数が多いということは、同胞間の違いの程度のばらつきを少なくする効果があることがわかります。
どの同胞同士を較べても、だいたい半分くらいが一致していて、極端に一致度が高かったり、低かったりする
間柄は稀になっています。

\begin{minipage}{.45\textwidth}
\begin{align*}
&\text{ \#  一致本数}\\
&i=0,1,2,...,2k\\
&\text{ \#  sibIdValue}\\
&\frac{i}{2k}\\
&\text{ \#  sibIdProb}\\
&\binom{2k}{i}=\frac{2k!}{i! (2k-i)!}
\end{align*}
\end{minipage}
\begin{minipage}{.45\textwidth}
\begin{lstlisting}
sibIdValue<-function(k=1){ # 等長染色体k本のときにi=0,1,...2k本が一致した場合の一致率 
 (0:(2*k))/(2*k) 
}
sibIdProb<-function(k=1){ 
 dbinom(0:(2*k),2*k,0.5) #２項分布の確率密度関数 $(p+(1-p))^{N}$のi=0,1,...,N番目の項の値。ただし今はp=1-p=0.5
}
numch<-1:23 # 染色体数の例として1から23
means<-vars<-rep(0,length(numch))
for(k in numch){
 identity<-sibIdValue(k)
 prob<-sibIdProb(k)
 plot(identity,prob,type="b",ylim=c(0,1))
 means[i]<-momentProb(identity,prob,order=1,center=FALSE) # 平均
 vars[i]<-momentProb(identity,prob,order=2,center=TRUE) # 分散
}
plot(means)
plot(vars)
\end{lstlisting}
\end{minipage}\\

\subsubsection{染色体の長さはばらばら}
前項では、染色体の長さが同じものとして計算しましたが、実際には、図1.2のように長さは違います。
長さを実際の染色体のそれにして考えて見ます。
染色体の長さが違うと、長い染色体が一致した場合と、短い染色体が一致した場合とでは、
一致率への貢献の程度が異なるので、$2k$本のうち$i$本が一致した場合でも、
長い染色体で$i$本なのか、短い染色体ばかりで$i$本なのかでは一致率が違ってきます。
これをシミュレーションで確かめてみます。
乱数を発生させてシミュレーションしてみる練習として、以下のことを行ってみることとします。

今、常染色体とＸとを対で計46本持っているとします。
それぞれの一致不一致を２項分布からの乱数(rbinom())でランダムに割り当てて、
Niter=10000回数(Niter同胞ペアのシミュレーションを行います。
比較のために、同じく46本の等しい長さの染色体の場合も同様に実験します。
この結果、どちらの場合も一致率の平均は0.5に非常に近く、同程度であることがわかります。
また、分散は、等長の染色体よりも、長さが本当の染色体のそれの方が大きいようです。
10000回の一致率の分布をみるために、
一致率をソートしてプロットしてみます。\\

\begin{lstlisting}
# f:個々の染色体が占める割合; Niter:シミュレーション回数を与えて、一致率がいくつになるかをシミュレーションする
SibSim<-function(f=f,Niter=10000){ 
# 0(非共有)か1(共有)かの乱数の行列を作る
# (行数：試行回数行、列数：染色体本数)
 rs<-matrix(rbinom(Niter*length(f),1,0.5),nrow=Niter) 
 # 行列の外積。共有染色体の割合を全染色体について合算
 # 乱数が1のときに一致したとして、その染色体の分の割合をかける
 rs%*%f 
}
# 1,2,...,X染色体を2本ずつで46本
chs<-rep(chromLen[1:23],2)
f<-chs/sum(chs) # 個々の染色体の占める割合
simOut<-SibSim(f,Niter=10000)
# 染色体が等長だと・・・
cheven<-rep(1,length(f))
feven<-cheven/sum(cheven)
Niter<-10000 # 試行回数
simOutEven<-SibSim(feven,Niter=Niter)
# 横軸を0-1に納めた上で、Niter回試行の一致率を昇順ソートしてプロット
plot(ppoints(Niter,a=0),sort(simOut),type="l",ylim=c(0,1))
plot(ppoints(Niter,a=0),sort(simOutEven),type="l",ylim=c(0,1))
\end{lstlisting}


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./fig/PartI-009.eps}
   \includegraphics[width=50mm]{./fig/PartI-010.eps}
\caption{左は染色体の長さが実際のもの、右は等長の場合}

 \end{center}
 \label{fig:one}
\end{figure}

平均と分散など、分布の様子を要約した情報では両者に大きな違いはありませんが、
グラフにしてみて初めてわかる情報があります。
染色体の長さに忠実に行ったシミュレーションでは、曲線がかなり滑らかなのに対して、
等長の場合(黒)が階段状になっていることです。
「等長」の場合には、一致率としてとりうる値が限定されるために、
離散的なパターンを取ることです。
ここで確認しておきたいのは、次の３点です。
\begin{itemize}
\item データはプロットしてみないと見逃す情報があること
\item 特殊な条件(今回は、「等長」)には注意が必要であること
\item 離散的な性質は分布に影響を与えること
\end{itemize}

\subsubsection{染色体は23対よりももっと細かくなる　交叉・組み換え ポアッソン過程と指数分布}
2本ある常染色体のどちらが子に伝えられるかは、0.5の確率で決まるものとし、
さらに、染色体2本のうちの片方が伝えられると決まれば、
その染色体全体が伝えらえれるものとしてシミュレーションしてきました。
実際には、ペアとなる染色体は交叉・組み換えという現象を通じて、
父由来・母由来の染色体の一部が置き換わったモザイク状の
染色体が作られて伝達されます。
したがって、46本の染色体について同胞の染色体起源の一致率を考えてきましたが、
この交叉・\textcolor{red}{組換え}\index{くみかえ@組換え}
の結果、23本の2倍以上に細切れになったものとして、
一致率を考える必要があります。

\textcolor{green}{交叉}\index{こうさ@交叉}は、対となる染色体同士が細胞内で並んで起こります。
卵子・精子に染色体のセットを渡すにあたり、各染色体の対の間で１箇所以上、
何箇所かで交叉が起こるとされています。
XとY染色体も、大きさがずいぶん違いますが、一部がよく似た配列(相同)であり、その部分で合い寄り添い、交叉が起きることが知られています。
このように、指で数えられる程度に稀に起きる事象がランダムに起きるとき、
そんな事象の起き方を\textcolor{green}{ポアッソン過程}\index{ポアッソンかてい@ポアッソン過程}と言い、
その起きる回数が従う分布として、\textcolor{green}{ポアッソン分布}
\index{ポアッソンぶんぷ@ポアッソン分布}が知られています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./fig/PartI-013.eps}
\caption{ポアッソン分布による交叉箇所数分布の近似}

 \end{center}
 \label{fig:one}
\end{figure}

ある染色体の長さが$L$塩基とします。
長さ$L$のDNAは、$L-1$箇所の塩基と塩基のつなぎ目があります。
このつなぎ目のどこでも等確率で交叉が起きるものとし、
その確率を$r$とします。
平均交叉回数は
\begin{equation*}
meanN=(L-1)\times r
\end{equation*}
です。
交叉回数の分布をシミュレーションしてみます。
すべての交叉可能箇所で交叉がおきるかどうかをシミュレーション(全箇所シミュレーション)して、交叉箇所数を
数え上げる方法と、
平均回数が$meanN$のポアッソン分布から発生させたを交叉箇所数とする方法との
２方法を実施してみます。
黒が全箇所シミュレーションでの、赤がポアッソン分布からの交叉箇所数の分布です。
よく似ています。
\begin{lstlisting}
# 可能箇所すべてで交叉がおきるかどうかを試す方法
RecombSim<-function(L=10000,r=0.001,Niter=1000){ 
# Lは配列長,rは箇所あたりの交叉確率,Niterはシミュレーション試行回数
 m<-matrix(rbinom((L-1)*Niter,1,r),nrow=Niter) #行数Niter、列数L-1箇所の行列にする
 apply(m,1,sum)
}
# ポアッソン分布を使う方法
RecombPois<-function(L=10000,r=0.001,Niter=1000){
 rpois(Niter,(L-1)*r)
}
L<-10000;r<-0.0001;Niter<-1000
NumSim<-RecombSim(L=L,r=r,Niter=Niter)
NumPois<-RecombPois(L=L,r=r,Niter=Niter)
ylim<-c(0,max(NumSim,NumPois))
plot(ppoints(Niter,a=0),sort(NumSim),ylim=ylim)
par(new=T)
plot(ppoints(Niter,a=0),sort(NumPois),col="red",type="l",ylim=ylim)
\end{lstlisting}

交叉・組換えを起こしてモザイク状の染色体を作って渡すということは、
図2.6に示すように、２本のモザイクが出来るときに起きた交叉場所ごとに交互にIBDかそうでないかが決まります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./fig/1-6.eps}

  %\includegraphics{./Figures/Ch1/mosaicIBD/mosaicIBD.eps}
\caption{
}

 \end{center}
 \label{fig:one}
\end{figure}
上の２本は、染色体のモザイク状態を白黒パターンで示したものです。
モザイク染色体ができるにあたって起きた交叉箇所のは、染色体１が2箇所、
染色体２が３箇所です。
下は、2本のモザイクのIBDである範囲を灰色、そうでない範囲を白としたものです。
モザイク染色体2本のIBDのパターンは、2本の染色体の交叉箇所を併せたすべてで、
交代している様子がわかります。
今、１本の染色体に平均してk回の交叉がおきてモザイク化するとすると、
２本のモザイク染色体のIBDの割合を計算するためには、平均して2k回の交叉箇所を考えればよいです。
交叉・組換えが起きないものとして染色体数を増やしたところ、一致率の分散が小さくなることを観察しました。
交叉・組換えがあると、染色体数を増やしたのと同じ効果が出ますから、
同胞間の一致率の分散は、さらに小さくなることになります。

さて、交叉の箇所数を考えましたが、
今度は、交叉と交叉との間隔について考えてみます。
交叉の箇所数をシミュレートするときに、ポアッソン分布からの
乱数を使いました。
以下のように、交叉間距離(染色体の端は端から交叉までの距離)をシミュレーションで発生
させてみた上で、その分布が\textcolor{red}{指数分布}\index{しすうぶんぷ@指数分布}になっていることを確認してみます。

\begin{lstlisting}
Niter<-1000 # シミュレーション回数
L<-1000000 #染色体の長さ
r<-0.0001 #塩基間あたりの交叉確率
# 交叉箇所数をポアッソン分布からの乱数で指定し、交叉箇所をsample関数で指定する
crosses<-sort(sample(1:(L-1),rpois(1,(L-1)*r),replace=FALSE))
# 交叉間距離のベクトルを作る
A<-c(0,crosses) # 染色体の始点と交叉箇所のベクトル
B<-c(crosses,L) # 交叉箇所と染色体の終点のベクトル
C<-B-A #交叉間距離のベクトル
# 平均がmean(C)の指数分布からの乱数をlength(C)の数だけ発生させてプロット
rexps<-rexp(length(C),1/mean(C))
# 交叉間距離をソートしてプロット
ylim<-c(0,max(C,rexps))
plot(sort(C),ylim=ylim,cex=0.5,pch=15) #交叉間距離の昇順プロット
par(new=T)
plot(sort(rexps),col="red",ylim=ylim,type="l") # 指数分布からの乱数の昇順プロット
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartI-015.eps}
\caption{染色体にランダムに交叉が起きたとき、交叉間距離の分布は、指数分布で近似される}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{変異の消長 遺伝的浮動}
生殖細胞が染色体のセットを引き継ぐとき、必ず、ある数の変異がおき、
塩基配列が変わります。
塩基配列は非常に長いので、卵子と精子の両方でたまたま同じ座位に変異が起きる可能性は、ほぼゼロなので、
この変異を受け継いだ子は、この変異箇所についてヘテロ接合体であるでしょう。
すると、この子が次世代へと引き継ぐ卵子または精子が、この変異を持つ確率は0.5です。
この子が２人の子供を持つとします。
子供2人がともにこの変異を引き継ぐ確率は0.25、2人のうち、1人が引き継ぐ確率は0.5、
2人とも引き継がない確率は0.25です。
このように、発生した変異は、1世代すら引き継がれずに消滅することもあれば、集団の
中で頻度が高くなることもあります。
このようにアレルの頻度は、ランダムに上下します。
このような変動を\textcolor{green}{遺伝的浮動}\index{いでんてきふどう@遺伝的浮動}\index{ふどう@浮動}と言います。

では、実際に発生した変異が消滅するかどうかというのはどのような動きをするのでしょうか？
今、人口がN人(染色体数が2N本)で固定しているとします。
２Ｎ本の染色体があり、それぞれの染色体は、０か１か・・・何本か不明ながら、
何本かを次世代に残し、その総本数が２Ｎだということです。

すべての個体が最大ｋ人くらいの子を次世代に残せる(すべての染色体が最大ｋ本のコピーを残せる)
という制限を設けて、その上で、どの染色体が多く残るか少なく残るかは、
ランダムに決まるというモデルで考えてみることにします。

図2.8を見てください。
左上が現世代の染色体のプールです。
１本の変異染色体(ピンク)と、11本の非変異染色体(青)の計12本の染色体があります。
最大の子供の数が4人として、それぞれ4本、44本の計、48本の染色体プールを作り、
そこから12本の染色体を抜き取るとします。
ある抜き取り操作により変異染色体が2本、非変異染色体が10本になったことを右下の状態で示しています。
プールからの抜き取りは、２ｘ２の分割表で考えることができます。
全部で48本の染色体は４本が変異染色体で44本が非変異染色体です。
それが列の和として表されています。
12本を抜き取るので、抜き取られる12本と抜き取られずに残る36本が行の和として現されています。
今、抜き取った12本の内訳が変異染色体2本、非変異染色体10本ですから、図に示したような
\textcolor{red}{分割表}\index{ぶんかつひょう@分割表}が出来ます。
このような表が観察される確率(\textcolor{red}{生起確率}\index{せいきかくりつ@生起確率})
は$\frac{4! 44! 12! 36!}{48! 2! 10! 2! 34!}$で計算されます。
これが、変異染色体が１本の状態から変異染色体が2本の状態へ遷移する確率です。


\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch1/drift/sampling.eps}
 \includegraphics[width=100mm]{./Fig/1-7.eps}
\caption{
}
 \end{center}
 \label{fig:one}
\end{figure}
\subsubsection{状態推移}
状態遷移という考え方をしてみます。
たった２人(染色体４本)の小集団で考えてみます。
変異染色体の本数としてありえる場合は0,1,2,3,4の５つです。
図2.9を見てください。
0,1,2,3,4はそれぞれ、変異染色体の本数がその数である状態を表します。
左側から右側へ世代が１つ進むとします。
左側・右側それぞれの状態には確率がありますが、
その確率は、世代ごとにすべての状態について
足し合わせると１になります。

左側の0,1,2,3,4の５つの状態から右側の0,1,2,3,4の５つの状態へと→が状態の推移を表します。
0または4の状態は
変異染色体がないか、すべてが変異染色体であるかであり、この場合には、変異染色体の本数が0,4から
変化しようがないので、→は１本しか出ていません。1,2,3の状態からは、0,1,2,3,4のすべての状態に→が出ています。
すべての→が同じ太さで描かれていますが、2の状態からは2の状態に遷移する確率が他の状態に遷移する確率よりも
高い、というように、→には、確率の重みがあります。
左側のそれぞれの状態からは1本以上何本かの→が出ます。
左側の状態の１つから出た→すべてについて、その→が表す遷移の確率を足し合わせると
１になります。
\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch1/drift/drift.eps}
   \includegraphics[width=50mm]{./Fig/1-8.eps}
\caption{左側の５状態から右側の５状態へと推移する。左側のある状態から
右側のある状態へ推移が可能であれば、→があり、不可能なときは→はない。
}
 \end{center}
 \label{fig:one}
\end{figure}
一般に、
２Ｎ本の染色体のうち、変異のある染色体の本数は$i=0,1,2,...,2N$のいずれかです。
$2N+1$個の場合(状態)があり、それ以外はありえません。
変異が起きたときから初めて、一斉に新たな２Ｎ本の染色体のセットに切り替わることとし、
その切り替わりの単位を世代と呼ぶことにします。
第$i$世代で変異染色体本数が$j$本である確率を$p_{i,j}$とし、$2N+1$個の状態を\\
\begin{equation*}
P_i=\{p_{i,0},p_{i,1},...,p_{i,2N}\};\sum_{j=0}^{2N}p_{i,j}=1
\end{equation*}
と表すこととします。
集団において、同じ座位に同じ変異がおきることはありえないとすると、
第0世代では、変異染色体の本数は１本ですから、$p_{0,1}=1$で、それ以外の状態の確率は0です。

集団のサイズを固定して、初代の変異染色体本数を指定してシミュレーションを開始します。
すべての染色体は平等に最大k本の染色体を次世代に残せるものとします。
したがって、ある世代から次の世代への変化においては、
変異染色体と非変異染色体の本数をk倍した後、そのプールから次世代集団サイズだけ
染色体をランダムに抜き取ることとして、$P_i$の変化を追跡します。
初期本数の確率が１の状態から徐々に初期本数を中心に増減した本数の状態の確率が増え、
変異染色体の本数が0の状態の確率が上昇してきます。
これは、一度、変異染色体が集団からなくなると、決して、その変異が
集団に復活して来ないことを意味しています。
逆に、変異染色体のみが集団を占める形で、多型性が失われることもあります。
時間が経つに連れ、多型性のない状態(変異染色体が無い状態もしくは変異染色体)
の確率が高くなります。
また、多型性のある状態は、変異染色体の本数によらずほぼ同じ確率で観察されるようになります。
関数に異なる変数を与えて、その様子を確認してください。

\begin{lstlisting}
probDrift02NInf<-function(N,p,k,infty=FALSE){ #N 染色体集団サイズ,p 変異アレル本数,個々の染色体が次世代に残す染色体最大本数,infty 制限なく残せるならinfty=TRUE
 if(infty){ # 染色体を抜き取るプールのサイズを無限大にする
  ret<-rep(0,(N+1)) # 変異染色体本数0,1,2,...,Nの状態
  pr1<-p/N # 変異染色体頻度
  pr2<-1-pr1 # 非変異染色体頻度
  if(pr1==0){ # 変異染色体がなければ、次世代も変異染色体はない
   ret[N+1]<-1
  }else if(pr2==0){ # 非変異染色体がなければ、次世代は変異染色体ばかり
   ret[1]<-1
  }else{
   for(i in 0:N){ # N本中i本が変異染色体である確率
    ret[i+1]<-exp(lgamma(N+1)-lgamma(i+1)-lgamma(N-i+1)+i*log(pr1)+(N-i)*log(pr2))
   }
  }
  ret
 }else{ # 次世代染色体を抜き出すプールは有限本数とする
  ret<-rep(0,(N+1))
  kN<-k*N
  kp<-k*p
  kNp<-kN-kp
  k1N<-(k-1)*N
  commonLN<-lgamma(kp+1)+lgamma(kNp+1)+lgamma(k1N+1)+lgamma(N+1)-lgamma(kN+1)
  for(i in 0:N){
   if(kp>=i){ # プールにある変異染色体本数より多い本数を抜き出すことはできない
    n11<-i
    n12<-kp-n11
    n21<-N-n11
    n22<-k1N-n12
    if(n11>=0 && n12>=0 && n21>=0 && n22>=00){
     ret[i+1]<-exp(commonLN-lgamma(n11+1)-lgamma(n12+1)-lgamma(n21+1)-lgamma(n22+1))
    }

   }
  }
  ret
 }

}
# ある状態において、次世代の変異染色体本数別の確率を計算する
nextGenExProb2<-function(x,k,infty){
 N<-length(x)-1 # 染色体集団サイズ
 ret<-rep(0,length(x))
 for(i in 1:length(x)){ # 状態0,1,...,Nごとに、次世代の状態への移り変わる確率を計算する
  p<-i-1
  tmpret<-rep(0,length(x))
  tmpret<-probDrift02NInf(N,p,k,infty=infty) 
  ret<-ret+tmpret*x[i] # 今の世代における変異本数がiの確率がx[i]なので、その比率に応じて、次世代の状態0,1,2,...,Nになる確率を加算
 }
 return(ret)
}

DriftSim3<-function(k=2,initial=1,np=20,ng=100,infty=FALSE){ # 最大子孫染色体本数, initial: 初期変異染色体本数, np: 集団サイズ, ng: シミュレーション世代数,infty:無限大プールにするかどうか
 m<-matrix(rep(0,(np*2+1)*ng),nrow=ng) #全世代の全状態の確率を納める
 m[1,initial+1]<-1 # 第1世代は、初期本数の状態の確率が１
 for(i in 2:ng){ # 第２世代以降を順次シミュレート
  m[i,]<-nextGenExProb2(m[i-1,],k=k,infty=infty)

 }
 return(m)
}
# 人数20人、染色体数40本、最大子染色体数2,初期変異染色体数10本、世代数25
out<-DriftSim3(k=2,np=20,initial=10,ng=25,infty=FALSE) # 実行
# 結果を鳥瞰図表示
# theta,phiは鳥瞰の視点を決める変数、shadeは陰影をつけて見やすくするための変数
persp(out,theta=120,phi=30,shade=0.2,xlab="generation",ylab="No. mutants",zlab="probability") 
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartI-017.eps}
\caption{底面のうち、横幅の広い軸(No.mutants)が変異染色体本数。縦に広い軸(generation)が
奥から手前に向かって、世代が進んでいる。高さは、世代ごと・変異染色体本数ごとの確率。
奥のピークが、世代０における、状態(確率が１)。世代が進むにつれ、
変異染色体が０である確率が高くなることが、左端の高さの増加から読み取れる。}

 \end{center}
 \label{fig:one}
\end{figure}
Rを使って、状態の変化を描きましたが、この状態の推移は行列(\textcolor{green}{推移行列}
\index{すいいぎょうれつ@推移行列})を使って表すこともできます。
行が、推移前の変異染色体本数を表し、列が推移後の変異染色体本数を表します。
$s_i$本の推移前状態のときに$s_j$本の推移後状態に変わる確率が$(i,j)$セルで$pr_{i=s_i,j=s_j}$と表されています。
第i行の確率は、推移可能な推移後状態全部について足し併せると１二なります。\\
$\sum_{j=1}^{2N} pr_{i=s_i,j=s_j}=1$\\
\begin{tabular}[htb]{|c|c|c|c|c|c|} \hline
before||after&$s_0$&...&$s_j$&...&$s_{2N}$ \\ \hline
$s_0$&$pr_{i=0,j=0}=1$&...&$pr_{i=0,j=j}=0$&...&$pr_{i=0,j=2N}=0$ \\
...&...&...&...&...&... \\
$s_i$&$pr_{i=i,j=0}$&...&$pr_{i=i,j=j}$&...&$pr_{i=i,j=2N}$ \\
...&...&...&...&...&... \\
$s_{2N}$&$pr_{i=2N,j=0}=0$&...&$pr_{i=2N,j=j}=0$&...&$pr_{i=2N,j=2N}=1$ \\ \hline
\end{tabular}
ただし、
\begin{equation*}
pr_{i=i,j=j}=\frac{(2N)!(2(k-1)N)!(ki)!(k(2N-i)!}{(2kN)!j!(2N-j)!(ki-j)!(2N(k-1)-(ki-j))!}
\end{equation*}
であり、これは、\\

\begin{tabular}[htb]{|c|c|c|c|} \hline
　&変異本数&非変異本数&計\\ \hline
次世代&$j$&$2N-j$&$2N$ \\ \hline
非伝達分&$ki-j$&$2N(k-1)-(ki-j)$&$2N(k-1)$ \\ \hline
k倍プール全体&$ki$&$k(2N-i)$&$2kN$\\ \hline 
\end{tabular}
という分割表の生起確率です。

ここでは、変異アレルの頻度が集団内で変化する様子を扱いました。
世代の変化による頻度の推移の計算にあたっては、次世代の確率が現世代の確率のみを
用いて計算出来ました。
このように次の状態の確率を現世代の確率だけを用いて計算する処理を\textcolor{red}{マルコフ連鎖}
\index{まるこふれんさ@マルコフ連鎖}と言います。
マルコフ連鎖は8章の連鎖解析のところでもう一度扱いますが、その特徴は、
処理が逐次処理であって、組み合わせを使わないために簡単であることです。

\section{DNAからRNA・タンパク質へ}
\subsection{DNAからRNAへ　転写}
DNAに記録された遺伝情報はRNAに写し取られて機能を発揮します。
図のように、DNAの上側の情報部分"CAGGTT"は、"CAGGUU"のRNAとして写しとられます。
これを\textcolor{green}{転写}\index{てんしゃ@転写}と言います。
DNAは"A","C","G","T"を使いますが、RNAは"T"の代わりに"U"を使います。
RNAはさらにタンパク質に翻訳されることも多く、
その場合のRNAはメッセンジャーRNA(mRNA)と呼ばれます。
翻訳されずにRNAとして機能を発揮することもあり、そのときには、機能性RNA遺伝子と呼ばれます。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/transcription/transcription.eps}
   \includegraphics[width=100mm]{./Fig/1-9.eps}
\caption{}
 \end{center}
 \label{fig:one}
\end{figure}
\subsection{RNAからタンパク質へ　翻訳}
mRNAの情報を読み取ってアミノ酸の連なり(タンパク質)を作ることを\textcolor{green}{翻訳}\index{ほんやく@翻訳}と言います。
翻訳に用いられるアミノ酸は２０種類あります。
塩基４種類の順列で２０種類のアミノ酸を識別するための最短の塩基列の長さは$4^2 \le 20 \le 4^3$ですが、
実際、塩基３個の順列のそれぞれがアミノ酸に対応して翻訳されます。
３塩基の並びをコドンと言い、コドンとアミノ酸の対応関係を図2.13に示しました。
タンパク質への翻訳にあたっては、翻訳処理の開始を意味する特別なコドンと、翻訳の終了を
意味する特別なコドンとを決めることで、翻訳処理の開始と終了が制御されます。
開始コドンである"AUG"は特定のアミノ酸である"Met"に対応しているので、タンパク質が
作られるときの最初のアミノ酸はいつも"Met"です。
終了のコドンは複数ありますが、それにはなんのアミノ酸も対応させないことで、翻訳作業が
終了します。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/translation/translation.eps}
   \includegraphics{./Fig/1-10.eps}
\caption{mRNAの塩基は３つが組となって意味をなします。
３つ組(コドン)は、１つのアミノ酸に対応するか、何のアミノ酸にも対応しないかのどちらかです。
対応するアミノ酸がなければ、そこで翻訳が終了します。}
 \end{center}
 \label{fig:one}
\end{figure}


\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/codons/codon.eps}
   \includegraphics{./Fig/1-11.eps}
\caption{mRNAの塩基の３つ組みを分岐木で表しました。
４つの塩基をマークで表しています。分岐木の一番上は、UCUに相当し、
それに対応しているアミノ酸はSerであることがわかります。
$4^3=64$通りの組み合わせがあり、
それが、20種類のアミノ酸と「対応アミノ酸なし」の21通りのどれかに相当します}
 \end{center}
 \label{fig:one}
\end{figure}

\chapter{多様性の諸相}
\section{核酸・タンパク質の多様性}
\subsection{DNA配列の多様性　種の違い　遺伝子多型}
DNA配列の多様性は\textcolor{red}{変異}\index{へんい@変異}によって生まれ、それが集団中で増えるか減るか、
なくなるかは\textcolor{red}{遺伝的浮動}\index{いでんてきふどう@遺伝的浮動}という現象が決まります。
また、複数の変異が同じ染色体上に生まれた後に、\textcolor{red}{組換え}\index{くみかえ@組換え}によって異なる染色体上に分かれたり、
逆に、異なる染色体上に生まれた変異が組換えによって同じ染色体上に乗ることによって
変異の存在状態が変わります。
種の遺伝的な違いはＤＮＡ配列の違いです。
同じ種の個体の集団で、配列が同一でないときに\textcolor{green}{遺伝子多型}\index{いでんしたけい@遺伝子多型}
\index{たけい@多型}(以下、多型)であると言います。

多型はDNA配列の違い方により、\textcolor{green}{置換}\index{ちかん@置換}・\textcolor{green}{挿入欠失}\index{そうにゅう@
挿入}\index{けっしつ@欠失}・\textcolor{green}{リピート}\index{リピート@リピート}(繰り返し)・
\textcolor{green}{逆位}\index{ぎゃくい@逆位}・\textcolor{green}{転座}\index{てんざ@転座}などに分類され、
それぞれの種類に短いものから、長いものまであります。
遺伝子座もそのサイズが１塩基だったり、ある長さをもつ範囲だったり、とびとびの範囲だったりしましたが、
多型でも同様にDNA配列の任意の位置・範囲について定めることができます。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/polymorphism/variationType.eps}
   \includegraphics{./Fig/1-12.eps}
\caption{}
 \end{center}
 \label{fig:one}
\end{figure}
図3.1を見てください。転座以外は１対のDNA二重鎖配列が示してあり、それぞれの鎖の右端の→はその方向です。
置換は１塩基対の例です。上の二重鎖ではTとAの対であるのに対し、下の二重鎖ではCとGの対になっています。
挿入・欠失の例も１塩基です。上の二重鎖に較べると下の二重鎖は欠失で、下の二重鎖に較べると上の二重鎖は挿入です。
リピートの例は、"AT"が４回繰り返されている上の二重鎖に較べて、下の二重鎖は３回の繰り返しです。
逆位では、灰色の塩基に挟まれた黒い塩基対部分が逆位になっています。うえの二重鎖の上側の配列"TTCCAACCC"が、下の
二重鎖の下側の配列になっています。
転座は→で示された配列が上と下とでは異なる位置に入っていることを示しています。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/polymorphism/polymorphism.eps}
   \includegraphics{./Fig/1-13.eps}
\caption{多型のいろいろ }
 \end{center}
 \label{fig:one}
\end{figure}
図3.2には遺伝子多型の大きさが示されています。スケールが最上段に対数スケールで示されています。
多型は顕微鏡で観察可能なもの("microscopic variants")と観察できないもの("sub-microscopic variants")に大別され、
特に大き目ながら視覚観察できないものを
\textcolor{green}{構造多型}\index{こうぞうたけい@構造多型}(Structural variants)と呼びます。
多型のタイプのそれぞれに、
さまざまなサイズの多型があります。\textcolor{green}{SNP}\index{SNP@SNP}は
1塩基の置換型多型、\textcolor{green}{コピーナンバー多型}(\textcolor{green}{CNV})\index{こぴーなんばーたけい@コピーナンバー多型}
\index{CNV@CNV}はリピート型の構造多型です。
X染色体とY染色体の違いを多型とみなせば、染色体サイズの挿入欠失ともいえますし、XとYとをそっくり入れ替えている
とみなせば置換ともいえるでしょう。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/polymorphism/haplotype.eps}
   \includegraphics{./Fig/1-14.eps}
\caption{２つの配列は3箇所の１塩基多型で配列が異なります。
これは２つのハプロタイプです。
多型箇所の塩基は黒く、その他の塩基は灰色で示してあります。
多型箇所だけを抜き出すと"TCG","AGA"という配列になります。
この違う部分だけを用いて\textcolor{red}{ハプロタイプ}\index{ハプロタイプ@ハプロタイプ}としてもよいです。
2アレル型なので、第１，２，３の1塩基多型の"T""C""G"を"0"、
"A""G""A"を"1"で表すことにすると、この２ハプロタイプは"000","111"と表せます。
1塩基多型のアレルの組み合わせが8通りあるので、8種類のハプロタイプがありえることを、
右下の"0""1"の配列が示しています。}
 \end{center}
 \label{fig:one}
\end{figure}

多型を遺伝子座のそれとすれば、多型の異なる配列のひとつひとつは\textcolor{red}{アレル}\index{アレル@アレル}です。
1塩基多型は4種類の塩基を取ることが可能ですが、多くの場合、2種類の塩基のみで存在します。
このような場合、2アレル型の多型であるといいます。
個々の多型は塩基配列上に並んでいますので、ある長さを持つ範囲に複数の小さな多型が存在することがあります。
この複数の小多型を含む範囲を１つの単位として取り扱うときには、そのアレルを\textcolor{green}{ハプロタイプ}
\index{ハプロタイプ@ハプロタイプ}と呼びます。
図3.3はある領域に３つの2アレル型の１塩基多型があり、その組み合わせとしてのハプロタイプは
$2^3=8$種類あることを示しています。2アレル型の場合には"A""T""G""C"の4文字を使わずに"0""1"で表すと
便利なことも多いです。


\subsection{RNAとタンパク質の多様性}
RNAの配列はDNAからそっくり転写されてできますから、塩基配列の多様性はDNA配列のそれと同じです。
あるDNA配列からRNAが転写されてできるときに、鋳型としてのDNAは変わらなくても、
DNA配列のどの部分が転写されるかが変わることによって、多様なRNA配列が得られます。
あるDNA配列上にあるいくつかのエクソンがmRNAに転写され、さらにタンパク質に翻訳されますが、
このエクソン領域の取り方が、場合によって変わってくることがあります。
あるエクソンがそっくり抜け落ちたり、あるエクソンの長さが伸びたりします。
DNAからmRNAを転写する際に、エクソンだけを切り出す過程を\textcolor{green}{スプライシング}
\index{スプライシング@スプライシング}と言うので、
このように異なるmRNA配列のそれぞれを\textcolor{green}{スプライシングバリアント}
\index{スプライシングバリアント@スプライシングバリアント}と呼びます。
この結果、出来上がるタンパク質のアミノ酸配列も当然、違ってきます。
Ｒを使って、スプライシングバリアントの配列を作ってみます。
\begin{lstlisting}
seq1<-sample(c("A","T","G","C"),100,replace=TRUE) # 長さ100のDNA配列
exonpattern1<-c(11:20,41:60,81:90) # mRNA1は３エクソン。そのパターン
exonpattern2<-c(11:20,41:60,66:77,81:90) #mRNA2はmRAN1の第２エクソンと第３エクソンの間にエクソンが挿入されています
seq1[seq1=="T"]<-"U" # mRNAではDNAの"T"が"U"になります。
mRNA1<-seq1[exonpattern1] # mRNA1の配列を抜き出します
mRNA2<-seq1[exonpattern2]
\end{lstlisting}
mRNAではDNAで"T"の塩基が"U"になります。
１つのＤＮＡ配列からエクソンの位置が定まることによって、１つのmRNAが作られました。
実際には、異なるエクソンの位置を用いることによって、１つのＤＮＡ配列から複数のmRNAが転写されます（スプライシング バリアント）。
多くの場合は、余分のエクソンが転写されたり、一部のエクソンが長かったり、短かったりという違いです。

このようにRNAはDNAのどの部分を取り出すか、という「位置的」な変化をつけることで多様性を持っています。
さらに、DNA二重鎖は各細胞に1セットずつあり、それより多くも少なくもなく(量が一定)、
しかも、その細胞が新たに生まれてから死ぬまで変わらない(時間的に一定)のですが、
RNAの方は、どのくらいたくさん転写をするかによって、量の増減があり、
しかも、その量は「時間的」に変わります。
RNAの多様性はDNAの多様性よりも格段に大きいことがわかります。
実際には、DNAの方も量と塩基配列こそ変わらないものの、その分子状態は化学修飾を受けることによって、状態が
「位置」によって多様ですし、その状態が「時間的」にも変化しています。

タンパク質はmRNAが翻訳されることによって出来ます。
DNAからmRNAに引き継がれた配列の多様性が、コドン表でのアミノ酸への対応関係が変えるような
ものであれば、アミノ酸配列を変えます。スプライシングバリアントはタンパク質のアミノ酸配列を長さも
含めて変化させます。
それ以外の多様性は、アミノ酸配列以外での多様性に影響しえます。
タンパク質への翻訳も「いつ」「どこで」「どれくらい」という点で多様度を持ち、それは、生物が
生きていくうえで微妙かつ精密に調節されているので、遺伝子産物としてのタンパク質のアミノ酸配列レベル
での多様性、その量・時間的変化の多様性がより大きくなります。
生物個体の特徴である形質は、このタンパク質の多様性を含めた、
多くの構成分子の多様性の結果として現われてくるものなので、
DNA配列にその根を持ちつつも、大変大きな多様性を持っていることが理解できます。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/tayousei/tayousei.eps}
   \includegraphics{./Fig/1-15.eps}
\caption{DNAからmRNA、アミノ酸配列が転写・翻訳され、機能を持つ分子となり、さまざまな形質に影響を与えます。
DNA配列の多様性(★)はmRNA、アミノ酸配列、分子を介して形質に影響を与えます。
DNA上の１つの遺伝子から複数のmRNAが転写され、その量(○の大きさ)に多様性が生まれます。
さらにアミノ酸配列のレベルで量を含めた多様性がもたらされ、さまざまな形質の出現しやすさへ影響する様子が、
形質の四角の高低で表現してあります。
さらに、DNA→RNA→タンパク質→形質という方向だけではなく、RNAはDNAに調節的に働き、
タンパク質は、RNAや、DNAに働きかけるという逆方向の反応もあり、DNA・RNA・タンパク質・形質の
間の関係は大変複雑になります。}
 \end{center}
 \label{fig:one}
\end{figure}

\section{多様性と分散}
\subsection{分散の分解　分散・共分散}
多様性はばらつき加減ですので、ばらつきの指標の一つである\textcolor{green}{分散}\index{ぶんさん@分散}
として値にすることができます。

分散は足し合わせることができ、逆に、分解できることを見てみます。
あるものが複数のものから構成されているとき、
その分散は、構成要素の分散と、構成要素間の関係とに分解できます。

まず、この話しを進める前に、
構成要素の分散と構成要素間の関係という２種類の要素を考えていますが、
これを統一して取り扱うことにします。
個々の構成要素はその要素と要素自身との関係であるということにすれば、
構成要素$s$の自身との関係に関して、$\sigma_{s,s}$を、
$s$の自身ではない$t$との関係に、$\sigma_{s,t}$とすれば、$s=t,s\not = t$のいずれでも
同じように取り扱うことができます。
さて、ある変数$Xs$について、N個のサンプルに$\{x_{s,i}\}$があったときに、$X1$の分散は
\begin{equation*}
V(Xs)=\frac{1}{N}\sum_{i=1}^N (x_{s,i}-\mu_s)^2
\end{equation*}
で
計算できるのでした。
ただし、$\mu_s=\frac{1}{N}\sum_{i=1}^N x_{s,i}$は$Xs$の\textcolor{red}{平均}\index{へいきん@平均}です。
分散の計算式の$(x_{s,i}-\mu_s)^2=(x_{s,i}-\mu_s)\times (x_{s,i}-\mu_s)$に着目します。
ここに$s$が２回登場しているので、
\begin{equation*}
\sigma_{s,s}=V(Xs)=\frac{1}{N}\sum_{i=1}^N (x_{s,i}-\mu_s)(x_{s,i}-\mu_s)
\end{equation*}
としてやれば、$\sigma_{s,t}=\frac{1}{N}\sum_{i=1}^N (x_{s,i}-\mu_s)(x_{t,i}-\mu_t)$
と書き換えてやることで、分散の定義の拡張として、2要素間の関係$\sigma_{s,t}$が
定義できました。
これが\textcolor{green}{共分散}\index{きょうぶんさん@共分散}です。

今、２つの変数$X1,X2$があって、N個のサンプル$i=1,2,...,N$についてその値
$x_{1,i},x_{2,i}$があり、その和$P=\{p_i=x_{1,i}+x_{2,i}\}$を考えます。
物理の試験の点$X1$と生物の試験の点$X2$の和で、理科の総合点とするようなものです。
\begin{equation*}
\sigma_{P,P}=\sigma_{X1,X1}+\sigma_{X2,X2}+\sigma_{X1,X2}+\sigma_{X2,X1}=\sum_{s=1}^2 \sum_{t=1}^2 \sigma_{X_s,X_t}
\end{equation*}
となります。
また、共分散は、2因子が独立のときにゼロで、関係があるときに大きくなります。
計算式での証明は、統計学の本にはよく出ていますので、ここでは省略して、それよりは、「本当にそうなることの確認」
をすることにしてみます。
このことをＲで見てみます。
$X1,x2$が乱数であるとします。
$P$をその和とします。

図3.5のように$X1,X2$が相互に独立な場合と、
相互に関連のあるとを試してみます。
その上で、分散、共分散の関係を確認します。\\
\begin{minipage}{0.4\textwidth}
\begin{lstlisting}
nsample<-10000
x1<-rnorm(nsample)
x2<-rnorm(nsample)
p12<-x1+x2
#x1, x2の分散・共分散行列を計算します
cov12<-cov(cbind(x1,x2))
# 分散共分散行列を表示します
cov12 # 共分散は小さいです(右上、左下のセル)
#p12の分散は
vpp_2<-cov(p12,p12)
# p12の分散が、分散・共分散の和である。
vpp_2-sum(cov12) 
plot(x1,x2) # x1, x2のばらつきを示します
# x1と関係のあるx2を作ります
x3<-x1+0.1*rnorm(nsample) 
p13<-x1+x3
#x1, x2の分散・共分散行列を計算します
cov13<-cov(cbind(x1,x3))
# 分散共分散行列を表示します
cov13 # 共分散は大きいです(右上、左下のセル)
#p12の分散は
vpp_3<-cov(p13,p13)
# p13の分散が、分散・共分散の和である。
vpp_3-sum(cov13) 
plot(x1,x3) #プロットは強い相関を示します
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{Schunk}
\begin{Sinput}
> cov12
\end{Sinput}
\begin{Soutput}
             x1           x2
x1  1.009561663 -0.006727835
x2 -0.006727835  1.018274923
\end{Soutput}
\begin{Sinput}
> vpp_2 - sum(cov12)
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\begin{Sinput}
> cov13
\end{Sinput}
\begin{Soutput}
         x1       x3
x1 1.009562 1.010659
x3 1.010659 1.021820
\end{Soutput}
\begin{Sinput}
> vpp_3 - sum(cov13)
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\end{Schunk}
\end{minipage}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartI-019.eps}
   \includegraphics[width=50mm]{./Fig/PartI-020.eps}
\caption{相関のない例と強い例}

 \end{center}
 \label{fig:one}
\end{figure}

2つの例では$X1,X2$の相関の強さが、かたや無しで、かたや非常に強いという違いがありますが、
どちらも
$P$の分散 $V(P)=V(X1)+V(X2)+2*Cov(X1,X2)$になっていることが確かめられます。
$X1,X2$の相関の強さの違いは、$V(P)$に占める$Cov(X1,X2)$の割合が、ほとんどないか、非常に大きいかに現われています。
ここでは正規分布を使いましたが、これは、分布によりませんし、サンプルサイズが小さくても成り立ちます。
また、ここまで、２つの因子とその因子間の関係で考えてきましたが、
因子の数は２つに限らずいくつでもよいので
\begin{equation*}
\sigma_{p,p}=\sum_{s=1}^k \sum_{t=1}^k \sigma_{s,t}
\end{equation*}
となります。

\subsection{遺伝率と分散}
これが、遺伝因子に関してどのように使われているかを見てみます。
ある形質($P$)のばらつき加減が、遺伝因子とそれとは無関係の(独立な)その他の因子で説明できるとしたとします。
「遺伝因子($G$)」と「その他の因子($E$)」の２つが登場し、
$\sigma_{P,P}=\sigma_{G,G}+\sigma_{E,E}$
と分解されます。
$G$と$E$とは独立なので、$\sigma_{G,E}=0$であって、上式では省略しています。
$\frac{\sigma_{G,G}}{\sigma_{P,P}}$は、形質を決める遺伝因子の強さとして、
\textcolor{green}{遺伝率}\index{いでんりつ@遺伝率}(Heritability)と呼ばれます。


\subsection{ハーディワインバーグ平衡(HWE)と分散}
２アレル型の多型を考えます。
アレルがAとa、その頻度がpと(1-p)とします。
２倍体のジェノタイプは"AA","Aa","Aa"の３種類です。
Aの本数を用いて３ジェノタイプをG0,G1,G2と書き表すことにします。
この３ジェノタイプの頻度を考えます。
a,Aの選ばれ方が独立であるという仮定のもとでの期待値は
\begin{align*}
G2=p^2\\
G1=2p(1-p)\\
G0=(1-p)^2
\end{align*}
となります。
この値からのずれがあるとして、それを変数で表してみます。
\begin{align*}
G0=p^2+\Delta\\
G1=2p(1-p)-2\times \Delta\\
G2=(1-p)^2+\Delta
\end{align*}
と１変数を使って表すことができます。
$\Delta=0$のとき、a,Aが独立な仮定の下での期待頻度となり、
逆に$\Delta=p(1-p)$のときに、ヘテロ型がいなくなります。
$\Delta=0$であるとき(Aとaの組み合わせ方が独立であるとき)に、
遺伝学ではその集団が"\textcolor{green}{ハーディワインバーグ平衡}\index{ハーディワインバーグへいこう@
ハーディワインバーグ平衡} Hardy-Weinberg平衡(\textcolor{green}{HWE}\index{HWE@HWE})にある"と言います。
そして、そこからずれていることを、HW不平衡と言います。
また、そのずれを
\begin{align*}
G0=p^2+p(1-p)F\\
G1=2p(1-p)(1-F)\\
G2=(1-p)^2+p(1-p)F
\end{align*}
と表して、このFを\textcolor{green}{近交係数}\index{きんこうけいすう@近交係数}と呼びます。

アレルAとaとを因子のありなしとして、Aに値1を、aを0とカウントすることとします。
このようなアレルがもたらす値の分散の大小について考えてみます。
今、人数$N$の集団を考えます。
この集団を染色体単位で考えると、
$2N$本の集団とします。

その平均と分散$\mu(h)$,$v(h)$は、\\
\begin{equation*}
\mu(h)=(p \times 1 +(1-p) \times 0) =p
\end{equation*}
\begin{equation*}
v(h)=p \times (1-p)^2+ (1-p) \times (0-p)^2=p(1-p)
\end{equation*}
となります。
さて、2倍体ではどうでしょうか。
２倍体なので、ジェノタイプは"aa","aA","AA"の３種類あります。
３ジェノタイプの値はそれぞれ0,1,2となります。
さて、３つのジェノタイプの頻度が定まれば、この集団における、この影響力の分散が計算できます。
２倍体での平均($\mu(d)$)と分散($v(d)$)とします。
\begin{equation*}
\mu(d)=G0\times 0 +G1\times 1 + G2 \times 2=2p=2\mu(h)
\end{equation*}
\begin{align*}
v(d)&=G0 \times (0-\mu(d))^2+G1\times (1-\mu(d))^2+G2\times (2-\mu(d))^2\\
&=2p(1-p)(1+F)\\
&=2v(h)+2\times F \times v(h)
\end{align*}
2倍体の分散は、1倍体の分散の和($2v(h)$と$2\times F\times v(h)$の和となっています。
このFに依存する項が共分散成分になっており、
ここから共分散は$Cov(d)=F \times v(h)$であることが見て取れます。
実際、$F=\frac{Cov(d)}{v(h)}=\frac{Cov(d)}{p(1-p)}$ですが、これは、
\textcolor{red}{相関係数}\index{そうかんけいすう@相関係数}です。
Fと相関係数は、共分散に比例する値で、
ヘテロ型がいないような状態のときに1をとるような\textcolor{red}{指数}\index{しすう@指数}であることがわかります。

\subsection{アレル関連・連鎖不平衡と分散}
今、２つの２アレル型多型が同じ染色体の比較的近いところにあるとします。
多型をMa,Mbとすします。
それらのアレルをA/a,B/bとし、そのアレル頻度をp,(1-p),q,(1-q)とします。
Ma,Mbともに、A,Bが値1を持ち、a,bは値0であるとします。
前節で見たように、
Ma,Mbの値の平均と分散はそれぞれ、
\begin{equation*}
\mu(Ma)=p,\mu(Mb)=q
\end{equation*}
\begin{equation*}
v(Ma)=p(1-p),v(Mb)=q(1-q)
\end{equation*}
です。
２多型の組み合わせとして４ハプロタイプAB,Ab,aB,abが出来ます。
そのハプロタイプ頻度をh1,h2,h3,h4とします。
先ほど、ＨＷＥのときに、独立の仮定での頻度を算出し、そこからの逸脱で、ジェノタイプの頻度を表すことにしました。
今回もそれと同じ手順を踏むことにします。
２多型が独立であると仮定すると
\begin{align*}
h1=pq\\
h2=p(1-q)\\
h3=(1-p)q\\
h4=(1-p)(1-q)
\end{align*}

異なる座位のアレルの組み合わせの頻度が、独立を仮定した頻度となっているとき、
その座位はお互いにアレルが独立である・\textcolor{green}{連鎖平衡}\index{れんさへいこう@連鎖平衡}にあると言います。
逆に、独立ではないときにアレルに関連がある・\textcolor{green}{連鎖不平衡}\index{れんさふへいこう@連鎖不平衡}にあると言います。
連鎖不平衡のときには、ハプロタイプ頻度がこの値からずれますので、
ずれの項を持ち込んで、表せば、次のようになります。
今、$p=h1+h2,q=h1+h3$なので、
\begin{align*}
h1=pq+\delta\\
h2=p(1-q)-\delta\\
h3=(1-p)q-\delta\\
h4=(1-p)(1-q)+\delta
\end{align*}
と表せます。

ここで、
\begin{equation*}
\delta=r \sqrt{p(1-p)q(1-q)}
\end{equation*}
と置くと、$h1=p=q,h2=h3=0,h4=(1-p)=(1-q)$
のときに$r=1$となります。
これは、アレルAとBとが強く関係し、aとbとが強く関係し、
４つの可能なハプロタイプのうち、２つだけが存在するという特別な状態に対応しています。
先ほどのHWEの例で、$F=1$がホモ型だけの状態を表していたのに対応します。
この$r$は２つの２アレル型多型のアレル同士が関係する程度の指数(アレル関連指数、連鎖不平衡指数)と
呼ばれるものの１つです。
逆に言うと、多型のアレル同士が同じ染色体の上に乗るかどうかは独立なこともあるけれども、
特定のアレルの組み合わせが高頻度で同じ染色体上にあるという場合が存在し、
それを遺伝学では、アレル関連と読んだり、連鎖不平衡と
呼んだりする、というように言い換えることも出来ます。

さて、
\begin{align*}
h1=pq+\delta\\
h2=p(1-q)-\delta\\
h3=(1-p)q-\delta\\
h4=(1-p)(1-q)+\delta
\end{align*}
で、この４ハプロタイプが独立して２倍体のジェノタイプを構成しているとします。
$A,B$ともに、得点１を持つものとし、AB,Ab,aB,abの得点はそれぞれ0,1,1,2であるとします。
このときの、染色体の集団の得点の平均と分散は
\begin{equation*}
\mu(Ma+Mb)=p+q
\end{equation*}
\begin{equation*}
v(Ma+Mb)=p(1-p)+q(1-q)+2\delta=v(Ma)+v(Mb)+2\delta
\end{equation*}
これから、２座位の得点の和の分散は、個々の座位の分散の和と、
それ以外に分解されました。
$\delta$は２多型のアレル間の共分散に関する項なわけです。
実は
\begin{equation*}
r=\frac{\delta}{\sqrt{p(1-p)q(1-q)}}
\end{equation*}
は相関係数であって、
それらは共分散と比例し、存在し得る４ハプロタイプのうち、
２ハプロタイプしか存在しない状態で１となるように調整された指数である
ことがわかります。
\section{データの取り扱い方と分散・共分散}
\subsection{HWEとアレル関連・連鎖平衡を２列のデータで考える}
HWEとアレル関連・連鎖平衡について、それぞれの指標F,rと共分散の関係をみてきました。
両者を2カラムのデータとして考えてみます。
図3.6を見てください。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/var/var.eps}
   \includegraphics{./Fig/1-16.eps}
\caption{}
 \end{center}
 \label{fig:one}
\end{figure}
まず、このデータをHWEのデータとして考えます。
左の２列は、個人がもつ２つのアレルを表しています。右のカラムは2倍体としての点数となっています。
2倍体の点数の分散は、２つのアレルの分散と、２つのアレルの間の共分散の和であって、それはFと比例します。
11,(10,01),00は３ジェノタイプに分かれますので、それに相当する頻度が列の左側に示してあります。

次にこのデータを
２多型のハプロタイプのものとして考えます。
左の２列は、それぞれMa,Mbのアレルを表しています。
４つのハプロタイプが、２列にまたがった
数値ペア11,10,01,00で表されます。
右の列はハプロタイプごとの得点に相当します。
このとき、ハプロタイプの点数の分散が、
個々の多型の分散の和と、多型のアレル間の共分散の和であることが示されています。
この場合の共分散は、アレル間の関連指標rに比例しています。
４ハプロタイプの頻度が右側に示してあります。

HWEとして扱う場合と連鎖平衡として扱う場合との違いは、
(1,0),(0,1)の行数がHWEのデータの場合には常に等しいのに対して、
アレル関連・連鎖平衡のデータの場合には等しくなくてもよいという点です。


\subsection{遺伝形式(優性・劣性)は第3の列}

HWE、アレル関連・連鎖平衡の場合には、
独立しているかもしれないもの(２倍体の個々のアレル、２つの多型)を図3.6のように２列で
扱い、それぞれの分散と２列間の共分散とに分解しました。
では、優性・劣性・相加的と言った遺伝形式は、それと同じように扱うことができるでしょうか。
アレル２本のジェノタイプの影響度が２であるとしたときに、相加的形式では、アレル１本のジェノタイプの
影響度は１です。
これは、ＨＷＥ・連鎖平衡を考えたときと同じ枠組みです。
したがって、２列で扱うことは適当です。
では、優性形式の場合はどう考えればよいのでしょう。
ヘテロ型の場合だけ、影響度を１付け加えることが必要です。
純粋に優性の場合はこの効果が１、純粋に劣性の場合には−１、それ以外の場合も、この値を
適当に定めることで、表現することが可能です。
相加的形式は、この値が０である特別な場合とも言えます。
そのために図3.7のように、この効果専用のカラム$Vd$を付け加えることにします。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch1/var/varDominance.eps}
   \includegraphics{./Fig/1-17.eps}
\caption{}
 \end{center}
 \label{fig:one}
\end{figure}
2列ではうまく表せませんでしたが、３列目を加えることで表すことができます。
遺伝形式を考えるときには、ヘテロ型に特有の効果の要素と、それと２つのアレルとの関係の要素の３要素を
付け加えて考えなくてはいけないことがわかりました。
３要因が関与しますから、それぞれの分散と３要因が作る３通りの共分散とが関係してきます。
新たに加えた第３列の分散$V_d$と第３列と第１、第２列との間の共分散$Cov1d,Cov2d$は次のように表されます
(細かい式はの導出は省略しました)。
\begin{equation*}
Vd=G1(1-G1)d^2
\end{equation*}
\begin{equation*}
Cov1d=Cov2d=\frac{1}{2}G1(G2-G0)d
\end{equation*}
G0,G1,G2は、点数が0,1,2の人数です。
このように、ジェノタイプの頻度の関数で表され、$d=0$のとき(相加的形式)に両方とも0となり、
$G1=0$(ヘテロ型)が存在しないときにも、この効果はなく、
また、$G0=G2$(２つのホモ型の頻度が同じとき=アレル頻度が0.5のとき)には共分散の項が0になることなどがわかります。
$d=\frac{G0-G2}{G0+G2}$のときには、$Vd+Cov1d+Cov2d=0$となって、遺伝形式の寄与が全体として
相殺されることなどがわかります。
逆に言えば、アレル頻度が0.5でないときには、遺伝形式が相加的以外の場合には、形式の寄与の列が
アレルの列と独立であることはありえないことを意味しています。
遺伝形式の効果は、アレルの持ち方に依存して決まる要素であることを思い出せば、当然のことといえます。

\section{たくさんの要因 多因子遺伝}
形質の多くは、複数の、そして多数の遺伝子と環境因子の影響を受けて決まっていると考えられています。
そのような形質を\textcolor{green}{多因子遺伝形質}\index{たいんしいでんけいしつ@多因子遺伝形質}とか
\textcolor{green}{複合遺伝形質}\index{ふくごういでんけいしつ@複合遺伝形質}と言います。

例えば、ある形質に100個の2アレル型多型が関係しているとします。
これらの多型は相互に連鎖平衡にあるとし、
またＨＷＥを仮定した集団を考えます。
このような集団で、100個の多型がさまざまな強さでさまざまな遺伝形式で
寄与しているとします。

このような場合には、たくさんの要素が影響するために、
遺伝因子全体の形質への影響は、個人ごとにばらついて、正規分布様の
分布になることが示せます。
そしてその遺伝因子全体の寄与の分散は、寄与因子の分散・共分散の和であることも確かめられます。
多型間に連鎖不平衡だったり、ＨＷＥからずれている要素を加味するためには、
それらのために変数を追加してやるだけですから、ここでは、単純な設定としました。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartI-023.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

\begin{lstlisting}
covXY<-function(x,y){ #２列の値ベクトルから共分散を算出
 mx<-momentX(x,order=1,center=FALSE)
 my<-momentX(y,order=1,center=FALSE)
 sum((x-mx)*(y-my))/length(x)
}
covMat<-function(m){ # すべての列ベクトルのペアについて共分散を計算
 ret<-matrix(rep(0,length(m[1,])^2),nrow=length(m[1,]))
 for(i in 1:length(m[1,])){
  for(j in 1:length(m[1,])){
   ret[i,j]=covXY(m[,i],m[,j])
  }
 }
 ret
}
N<-1000 # サンプル数
M<-100 # 2アレル型多型数
af<-runif(M) # アレル頻度
r<-rnorm(M) # アレル１本分の得点
d<-rnorm(M) # マーカーごとの相加モデルからのずれ
Xmat<-matrix(rep(0,N*M),nrow=N)
for(i in 1:M){ # マーカーごとに人数分の得点を計算
 x<-rep(0,N)
 x<-x+rbinom(N,1,af[i])
 x<-x+rbinom(N,1,af[i])
 x[x==1]<-1+d[i]
 x<-x*r[i]
 Xmat[,i]<-x
}

X<-apply(Xmat,1,sum) # 個人の全多型分の得点

momentX(X,order=2,center=TRUE) # 全多型分の得点の分散と
sum(covMat(Xmat)) #分散共分散行列の値の和は一致する
hist(X) # 個人の全多型分得点の分布
\end{lstlisting}



\part{データ・サンプル・サンプルの集まり}
\chapter{観察して評価すること}
\section{データの種類と構成}
\subsection{遺伝子から見たデータの種類　遺伝子型と表現型　最終形質と中間形質}

いでん統計学では生命現象を遺伝子という観点で整理・解釈しますので、データの種類と構成を考える
ときにも、その視点から種類分けをします。
２つに分かれます。
\begin{itemize}
\item{\textcolor{red}{ジェノタイプ}\index{ジェノタイプ@ジェノタイプ}(遺伝子型)}
\item{\textcolor{red}{フェノタイプ}\index{フェノタイプ@フェノタイプ}(表現型)}
\end{itemize}
ジェノタイプというのは、親から子に伝えられる情報である「遺伝情報」に関するデータである、
ということです。
これは個体が生まれるとき(受精したとき)から死ぬまで、変わらずに保持する「何か」です。
生まれるときからあるので、形質に影響を与えます。
変わらずに存在するので、形質から影響を受けることはありません。
ゲノムの型と言い換えてもよいかも知れません。
ＤＮＡ分子とそれを含む染色体は化学修飾を受けたりして、変化することもありますので、
ＤＮＡ分子・染色体の状態のうち、変化しないものの型をジェノタイプと考えましょう。
塩基配列がそれに相当します。
そして、ジェノタイプでないものをすべてひっくるめて、ふぁのタイプとします。
表現型は相互に影響を及ぼしたり、及ぼされたりします。
RNAの発現の状態(トランスクリプトーム)はDNAの配列に影響を受けますが、
発現調節をするタンパク質の影響を受けますし、
そのタンパク質の発現は、様々なタンパク質とRNAの影響を受けます。
\footnote{
塩基配列以外で親から子に伝達する情報もあります。
その要素の影響を\textcolor{green}{エピジェネティクス}\index{エピジェネティクス@エピジェネティクス}
と言います。
そのことを考えると、ＤＮＡの塩基配列とＤＮＡ・染色体の塩基配列以外の状態とを分けてしまうのは、
乱暴に過ぎますが、本書では、「割り切り方」の一方法として、このようにしておきます
}

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch2/genotypePhenotype/genotypePhenotype.eps}
   \includegraphics{./Fig/2-1.eps}
\caption{データはジェノタイプとフェノタイプに分けられます。ジェノタイプは不変。フェノタイプは可変。
フェノタイプは形質のものですが、それらは、ジェノタイプからの遠近関係により、
最終形質とその中間形質と
いう関係で見ることもできます。}
 \end{center}
 \label{fig:one}
\end{figure}
同じフェノタイプと言っても、それらは、ＤＮＡから転写されてmRNAができ、さらに
翻訳されてタンパク質ができて、さまざまな現象が起きて、
ときには、それを測定するのが検査マーカーなどがあって、
その検査マーカーは病気などの判断の有力な根拠になっているというように、
遺伝子からの段々離れてきています。
今、病気に特に興味がある場合、それは最終的な興味がある形質であるとして、\textcolor{green}{最終形質}
\index{さいしゅうけいしつ@最終形質}と
呼べます。
すると、それと関係する形質ではあるけれども、「遺伝子から最終形質までの道のり」の
遺伝子と最終形質との間にあるものとして、mRNAとタンパク質とは
\textcolor{green}{中間形質}\index{ちゅうかんけいしつ@中間形質}とも
呼びます。
遺伝子発現を研究標的にするときには、mRNA自体が最終形質になります。


\subsection{解析対象としてのデータの種類　データ型}
\subsubsection{データ型}
データを遺伝子の視点で分類しましたが、解釈・解析する視点から分類することにします。
その分類が\textcolor{green}{データ型}\index{データがた@データ型}です。
分類の視点は次の２つです。
\begin{itemize}
\item{とりうる値が限られている(\textcolor{green}{離散}\index{りさん@離散}的)かそうでない(
\textcolor{green}{連続}\index{れんぞく@連続}的)か}
\item{\textcolor{green}{順序}\index{じゅんじょ@順序}があるかないか}
\end{itemize}
離散的なデータ型は２個以上のカテゴリから成ります。
\begin{table}
\renewcommand{\footnoterule}{}
\begin{center}
\begin{minipage}{10cm}
\caption{データ型の分類}
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&\multicolumn{2}{|c|}{離散}&連続 \\ \hline
順序の有無&２カテゴリ&３以上カテゴリ&量的 \\ \hline
順序あり&△&○&○ \\ \hline
一部順序あり&×&○&×\\ \hline
順序なし&△&○&×\\ \hline
\end{tabular}
\footnotetext{この表では、○印はそのようなデータ型があることを示し、×印はないことを、△印は
事情によって、あるともないとも言えることを示しています。}
\end{minipage}
\end{center}
\end{table}


\subsubsection{２カテゴリ型は順序があるともないとも言える}
\begin{itemize}
\item{Ａさんは病気Ｘと診断されています(されていません)}
\item{Ａさんの遺伝子ＧのmRNAは検出されています(されていません)}
\end{itemize}

このデータは、数えあげることができる場合(カテゴリ)のどれかに該当するので離散的です。
場合の数が２つしかないので、カテゴリ数は２です。
必ずどちらかのカテゴリに属します。どちらにも属さなかったり、両方に属することはありません。
２カテゴリ型は、相互に排他的で、二つのカテゴリをあわせると全体になります。
\textcolor{red}{全体集合}\index{ぜんたいしゅうごう@全体集合}\index{しゅうごう@集合}
に対して、\textcolor{red}{部分集合}\index{ぶぶんしゅうごう@部分集合}とその
\textcolor{red}{補集合}\index{ほしゅうごう@補集合}になっている、とも言います(図4.2)。

\begin{figure}[htbp]
 \begin{center}

%\includegraphics{./Figures/Ch2/set/AcapB.eps}
   \includegraphics{./Fig/2-2.eps}

\caption{全体を２つのカテゴリに分けると、集合とその補集合に分かれます。
全体を３カテゴリに分けると、相互に重なりのない(相互に排他的な)３つの集合に分かれます。}
 \end{center}
 \label{fig:one}
\end{figure}

表で、２カテゴリ型は、順序がある、ないのどちらにも△印がついています。
これは、「診断・検出されている」ことを「されていない」ことに対して、「何かがある」と考えると、
「ある」ことに１点、「ない」ことに０点をつければ、順序があることになりますし、
「ある」ことと「ない」ことは並列な２つのこと、と考えれば順序がないことになります。
以降の話しでは、順序がある他のデータ型と一緒に取り扱われることもあり、順序がない
データ型と一緒に取り扱われることもあります。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch2/set/TwoCat3Cat4CatContin.eps}
   \includegraphics{./Fig/2-3.eps}

\caption{全体が直線で表せるときは、順序ありのデータ型とみなせます。
この直線を区切る線を引くことによっていくつかの線分に分けると、
順序のあるカテゴリに分けられます(上段、中段左)。
区切りの線を引かずに数直線として扱えば連続型です(下左)。
3以上のカテゴリは、全体が直線では表せず、平面・立体・高次元空間になりえます。
この場合も全体を複数の相互に排他的なカテゴリに仕切る点は同じです。
右中・下の２つの図は２次元平面を３カテゴリに分ける様子と、３次元空間を
４カテゴリに分ける様子です}
 \end{center}
 \label{fig:one}
\end{figure}

\subsubsection{３以上カテゴリ型}
次の例を見てみます。
\begin{itemize}
\item{Ａさんの遺伝子Ｇの遺伝子多型Pの父由来の塩基は"A"です}
\end{itemize}
塩基は"A,T,G,C"の４つの可能性があります。
そしてそれ以外の可能性はなく、必ずどれか１つをとります。
ですから、４つのカテゴリを持つデータ型です。
ここで、"A,T,G,C"には特に順番の定めようもないので、順序なしです。
図4.3(右下)のように3次元空間を４つの部分に分けることに相当します。
\begin{itemize}
\item{Ａさんの遺伝子多型Pのジェノタイプは"AT"です}
\end{itemize}
この例では、
３つのジェノタイプ"AA","AT","TT"があり得ます。
離散的な３つのカテゴリで、順序がないとみることが出来ます。
この場合は図4.3(右中)のように平面を３つの部分に分けることに相当します。
別の見方では、この遺伝子多型の遺伝子型としてはアレル"T"の本数に着目すると、0,1,2本
というように、数値を割り当てることができます。
この場合は、順序のあるカテゴリです。この場合は、直線を３つに仕切ることに相当します。(図4.3右上)
どちらで扱うかは、どのような点に着目したいかによって変わります。
一方、次の例は
\begin{itemize}
\item{Ａさんの検査Ｔ(定性)の結果は"++(強い)"です}
\end{itemize}
カテゴリ数は３個以上ですが、順序があります。
"++(強く陽性(20-30))"は、"-(陰性(0-10))","+(陽性(10-20))","++(強く陽性(20-30))",
"+++(非常に強く陽性(30以上))"という４個の
カテゴリのうち、２番目に強いカテゴリです。
順序があるのは、直線を仕切ったものであるとして定義されているからです。
量的データに、仕切り線の位置を決める値(閾値(10,20,30,))が存在する場合もこのような形になります。

\subsubsection{連続型}
\begin{itemize}
\item{Ａさんの検査Ｔ(タンパク質Ｔ：遺伝子Ｇの翻訳産物)(定量)の結果は256ユニットです}
\end{itemize}
量で量ると数直線という１本の直線で表すことができ、
データはこの直線上の点として表すことができます。
では、ＡさんとＢさんの２人のデータが、ともに256ユニットであったとき、
２人のタンパク質量は同じなのでしょうか。
たとえ256ユニットと同じ値だとしても、十分に精密に較べれば、
まったく同じということはないでしょう。
そういう意味で、量的データの値は、数直線上の点で表される値を適当な精度で丸めた値である
と考えておくのがよいでしょう。
精度という細かさで多くの順序ありカテゴリに分けられたカテゴリ型のデータであるが、
そのカテゴリ数が十分多いので、カテゴリであることを忘れてもよいというように
考えてもよいかも知れません。

\subsection{一部に順序があること}
では、順序が一部にある場合というのはどういう場合でしょうか。
２つの2アレル型の多型のジェノタイプのデータを見てみます。\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
 &CC&CG&GG\\ \hline
AA&AA,CC&AA,CG&AA,GG\\ \hline
AT&AT,CC&AT,CG&AT,GG\\ \hline
TT&TT,CC&TT,CG&TT,GG\\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|} \hline
 &0&1&2\\ \hline
0&(0,0)&(0,1)&(0,2)\\ \hline
1&(1,0)&(1,1)&(1,2)\\ \hline
2&(2,0)&(2,1)&(2,2)\\ \hline
\end{tabular}\\

この表では、縦に多型P(A/T)の横に多型Ｑ(C/G)のディプロタイプが表されています。
これは、３個のカテゴリを持つ２つの因子の組み合わせのカテゴリ型データで、
そのカテゴリは$3^2=9$個ある場合です。
では、Ｐのアレル"T"の本数とQのアレル"G"の本数に着目してＰもＱも順序ありと考えます。
9通りのカテゴリはそれぞれＰとＱとのアレルの本数のパターンで表すこともできます。
特に、特殊事情を考慮しなければ、Pの"T"の本数に順序があって、Qの"G"の本数に順序を考えてよく、
Ｐの"T"もQの"G"も病気Ｘのリスクを上げているとき、
\begin{align*}
(0,0) < (0,1) < (0,2)\\
(0,0) < (1,0) < (2,0)\\
(0,0) < (1,1) < (2,2)
\end{align*}
の順序は問題がないでしょう。
はっきりしないのは、$(0,1)$と$(1,0)$との順序、$(0,2)$と$(1,1)$と$(2,0)$の順序、$(1,2)$と$(2,1)$の順序です。
これが、一部に順序があるけれども、一部に順序がない場合です。
このような場合を\textcolor{green}{半順序}\index{はんじゅんじょ@半順序}\index{じゅんじょ@順序}と言います。
全体に順序をつけるためには、何かしらのルールを入れる必要があります。
すべてに順序がある場合は\textcolor{green}{全順序}\index{ぜんじゅんじょ@全順序}と言います。

\subsection{カテゴリの組み合わせ}

多型P,Qの例では、２つのカテゴリ型を組み合わせていました。
カテゴリ数が$k_i$であるようなカテゴリ変数をN個組み合わせれば、
\begin{equation*}
\prod_{i=1}^N k_i =k_1\times k_2 \times ... \times k_N
\end{equation*}
通りのカテゴリが生じます。

\subsection{唯一選択、重複選択}
複数の項目からなる診断基準の例を見てみます。
\begin{itemize}
\item{Ａさんは５項目の診断基準のうち、３項目を満足しています}
\end{itemize}

これは５項目のうちの該当する項目をいくつでも選ぶというタイプのデータ型です。
それらはいくつかのパターンに分けられます。
５カテゴリ("a","b","c","d","e")から１つ選ぶときに得られたデータの記録方法としては
次の表のような方法が考えられます。

\begin{tabular}[htb]{|c|c|} \hline
サンプル&カテゴリ\\ \hline
サンプル１&b\\ \hline
サンプル２&a\\ \hline
サンプル３&a\\ \hline
サンプル４&c\\ \hline
\end{tabular}

「n個の中から選んでください。選ぶ数はいくつでも構いません」というような選択肢の
場合もよくみかけます。
選ぶ数に上限を設けて、「n個の中から選んでください。ただし、最大３個までとします」ということも
比較的多いかもしれません。
こんな場合は、次のように記録することになるでしょう。

\begin{tabular}[htb]{|c|c|c|c|} \hline
サンプル&選択カテゴリ１&選択カテゴリ２&選択カテゴリ３\\ \hline
サンプル１&b& - & - \\ \hline
サンプル２&a &d&c\\ \hline
サンプル３&a&c&d\\ \hline
サンプル４&c&e&-\\ \hline
\end{tabular}\\
サンプルによって、選択カテゴリ数が異なると表の形で記録するのが難しくなります。
サンプル２とサンプル３はともにa,c,dを選んでいますが、表の上では異なっています。
次のようにしてみます。

\begin{tabular}[htb]{|c|c|c|c|c|c|} \hline
サンプル&a&b&c&d&e\\ \hline
サンプル１&0&1&0&0&0\\ \hline
サンプル２&1&0&1&1&0\\ \hline
サンプル３&1&0&1&1&0\\ \hline
サンプル４&0&0&1&0&1\\ \hline
\end{tabular}

列の数が増えましたが、表の中はシンプルになりましたし、
選択された項目の順番についても気を遣わなくても
同じ選択パターンは同じレコードになっています。

この方式だと、「最大３個を選び、それに順序をつけなさい」という選択だとしたときには
「選ばないカテゴリを0点、1番目に選んだ項目に３点、2番目に選んだ項目に２点、3番目に選んだ項目に1点」と重みを
つけることにすれば、

\begin{tabular}[htb]{|c|c|c|c|c|c|} \hline
サンプル&a&b&c&d&e\\ \hline
サンプル１&0&3&0&0&0\\ \hline
サンプル２&2&0&3&1&0\\ \hline
サンプル３&3&0&1&2&0\\ \hline
サンプル４&0&0&3&0&2\\ \hline
\end{tabular}

のように、表の中の値に重みを反映するだけでうまく記録ができます。

言い方を変えると、順序のない３以上カテゴリ型の変数は、
カテゴリごとに２カテゴリ型変数を割り当てて、それを組み合わせたものとして捉えることが
できることが、データの記録方式からわかります。

ただし、個々のカテゴリに割り当てた変数同士が独立かというと、そうではありません。
５カテゴリから１個だけを選択する条件の場合には、５カテゴリに入れる値は０か１で、その和は１でなくてはならない、
という制約が５つの変数の間にありますし、
最大３個を選ぶという条件の場合には、5カテゴリに入れる値は０か１で、その和は３以下でなくてはならない、
という制約があります。
$N$個のカテゴリから、$k$個を選ぶとき、その選び方は$\binom{N}{k}=\frac{N!}{k!(N-k)!}$通りあります
(\textcolor{red}{組み合わせ}\index{くみあわせ@組み合わせ}。
選んだ項目が何番目かも問題にするとしたら、$\frac{N!}{(N-k)!}$通りあります(\textcolor{red}{順列}\index{じゅんれつ@順列})。

A,T,G,Cを並べて長さ$L$の塩基配列を作るとき、
その配列の場合の数は、\textcolor{red}{重複順列}\index{ちょうふくじゅんれつ@重複順列}
で$4^L$通りでした。
これに対して、３以上カテゴリから選択するときの場合の数はこれより小さくなっています。
この違いは、塩基配列のときに組み合わせた各塩基同士は独立であったのに対して、
複数項目からの選択の場合には、選択項目間は独立でないことを反映しているとも言えます。

\subsection{２倍体という特殊性　Hardy-Weinberg平衡(HWE)の正確検定}
カテゴリに注意して表を作ることが便利であることの例としてHWEの正確確率検定
を挙げることにします。

ジェノタイプは父母由来のアレルの組み合わせです。
今、ある２アレル型多型があり、その３ディプロタイプ"AA","AT","TT"の人数がそれぞれ、
$n_0,n_1,n_2;n_0+n_1+n_2=N$人だとします。アレルAとTの染色体本数が$n_A,n_T$とします。
$N$行２列の表があり、各行の和がすべて２で、第１、２列の和が$n_A,n_T$で、総和が$2N$で
ある表ができました。\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
サンプル&Aの本数&Tの本数&計\\ \hline
AA1&2&0&2\\ 
AA2&2&0&2\\ 
...&...&...&...\\ 
AA$n_0$&2&0&2\\ \hline
AT1&1&1&2\\ 
AT2&1&1&2\\ 
...&...&...&...\\ 
AT$n_1$&1&1&2\\ \hline
TT1&0&2&2\\ 
TT2&0&2&2\\ 
...&...&...&...\\ 
TT$n_2$&0&2&2\\ \hline
計&$n_A$&$n_T$&2N\\ \hline
\end{tabular}\\
これは
「２カテゴリから、重複を許して必ず２個選べ」というタイプのデータ型です。

第1列の値をが0,1,2のときには、第2列の値は必ず2,1,0という関係になっていて、
片方の列はなくてもわかります。
2列分の情報は冗長です。
では、この２列は全く不要なのかというと、そうではありません。
２アレル型多型のHardy-Weinberg平衡を考えるときには、この２列を持つ表が活用されます。
この表を用いて、HWEの\textcolor{red}{正確確率検定}\index{せいかくかくりつけんてい@正確確率検定}
をしてみます。

一般的な分割表の正確確率検定については、13章を参照してください。

このような\textcolor{red}{分割表}\index{ぶんかつひょう@分割表}の
\textcolor{red}{生起確率}\index{せいきかくりつ@生起確率}は、
$2$のセルと$0$のセルが$n_0+n_2$個、
$1$のセルが$2\times n_1$個、あるので
\begin{equation*}
\frac{\prod_{i=1}^N (2!) n_A! n_T!}{(2N)! (2!)^{n_0+n_2} (0!)^{n_0+n_2}(1!)^{2\times n_1}}=
\frac{2^{n_1} n_A! n_T!}{(2N)!} 
\end{equation*}
となります。
$0!=1,1!=1,2!=2$、$N=n_0+n_1+n_2$を使って式変形しました。

ここまでは、N行を区別していました。
これは、N人のそれぞれを区別していたことになるのですが、
今は、N人のうち、３ジェノタイプの内訳が$n_0,n_1,n_2$である場合を区別しないので、
$n_0,n_1,n_2$人で観測する場合は、$\frac{N!}{n_0!  n_1!  n_2!}$通りあり、これらの区別がつきません。
したがって、そのような確率は結局、
\begin{equation*}
\frac{N!}{n_0!n_1!n_2!} \times  \frac{2^{n_1} n_A! n_T!}{(2N)!} =
\frac{2^{n_1} n_A! n_T! N!}{(2N)!n_0!n_1!n_2!}
\end{equation*}

観測した表の生起確率
がわかりましたので、正確検定をするためには、
与えられた条件($n_0,n_1,n_2$が与えられた条件。必然的に$n_A,n_T$も決まります)の下で、
起こりえるすべての表を数え上げ、その生起確率を求めて、
観測表のそれ以下の場合に足し合わせることになります。

今、$n_A+n_T=2N$なので、$n_A$と$n_T$は、ともに偶数か、ともに奇数のどちらかです。
$n_A,n_T$が奇数のときには、ヘテロ型の人数$n_1$は奇数しかとらず、
$n_A,n_T$が偶数のときには、$n_1$は偶数しかとりません。
そのことを用いてＨＷＥの正確確率検定をＲで行ってみるソースが以下です。
取り得る場合の確率のすべてを足し併せると確かに１になっていることも確認できます。
\begin{lstlisting}
hweExact<-function(g=c(813,182,5)){ #3ジェノタイプの人数
 n<-sum(g) # 総人数
 nA<-2*g[1]+g[2] # Aアレル本数
 na<-2*g[3]+g[2] # aアレル本数
 evod<-g[2]%%2 # ヘテロ人数の偶数奇数判断
 maxAa<-min(nA,na)-evod
 Aa<-seq(from=evod,to=maxAa,by=2) # 観測しうるヘテロ人数のベクトル
 AA<-(nA-Aa)/2 # 観測しうるAA人数
 aa<-(na-Aa)/2 # 観測しうるaa人数
 obs<-(g[2]-evod)/2+1 # 観察データのヘテロ人数はAa[obs] 
 prob<-rep(0,length(Aa)) # 観測しうる表の生起確率
 prob<-exp(n*lgamma(2+1)+lgamma(nA+1)+lgamma(na+1)-lgamma(2*n+1)-(AA*lgamma(2+1)+Aa*lgamma(1+1)+aa*lgamma(2+1))+lgamma(n+1)-(lgamma(AA+1)+lgamma(Aa+1)+lgamma(aa+1)))
 p.value<-sum(prob[prob<=prob[obs]]) # 観測表の生起確率以下の生起確率を持つ表の生起確率の和
 # Aa 観測しうるヘテロ人数リスト
 # prob ヘテロ人数別の生起確率
 # obsprob 観察テーブルの生起確率
 # p.value 正確検定p値
 list(Aa=Aa,prob=prob,obsprob=prob[obs],p.value=p.value)
}
xx<-hweExact(c(813,182,5))
xx$p.value # 検定p値
sum(xx$prob) # 全表の生起確率の和は１になります
\end{lstlisting}


\subsection{親項目と子項目}
複数の項目を組み合わせたときに、それらが独立である場合もあれば、相互に関係がある
場合もあることはすでに述べてきました。
\begin{itemize}
\item{Ａさんは病気Ｘのタイプbと診断されています}
\end{itemize}
このカテゴリ型項目はどうでしょうか。
この項目のデータは病気Ｘという項目が「あり」の場合にのみ存在します。
病気Ｘという項目を親項目とすれば、その子項目と言えます。
アンケート調査などにもこのような項目間の帰属関係があります。
「前の設問で(1)と答えた場合に、以下の設問に答えよ」という形式の場合です。
データの構造が複雑になってきたときには、このような項目間の相互帰属関係を明確にして
取り扱うことも適当です。
このような項目の帰属関係はフェノタイプにのみあるわけではありません。
ジェノタイプの場合には、次のような例があります。
Y染色体上にあって、Y染色体固有の遺伝子に関するジェノタイプは、Yを持っていて初めて意味を成しますから、
Y染色体の保有に関する項目と、Y染色体固有のジェノタイプに関する項目は親子関係です。
また、挿入型の遺伝子多型があって、その挿入配列がさらに多型性を持つ場合にも、遺伝子型の間に
親子関係があります。
親子関係はグラフ理論で言うところの\textcolor{red}{木}\index{き@木}で表すことができますので、
次章で扱う\textcolor{red}{グラフ}\index{グラフ@グラフ}に関する説明が参考になります。


\subsection{カテゴリの配置　カテゴリ間の非独立性　正単体}
５カテゴリから３カテゴリを選択するときに、
５列の表でデータを表すときには、表に0か1の値を入れ、
行についての和が３になるようにするという制約がありました。
ある列に入れる値によって、別の列に入れる値が影響を受けています。
これが、カテゴリ間の非独立性です。
このことを異なる視点から眺めてみます。

相互に関係のないnカテゴリは、\textcolor{green}{正単体}\index{せいたんたい@正単体}の頂点に配置できることが知られています。
正単体とは、正三角形を多次元一般化したものと考えればよく、次の様に説明できます。。
\begin{itemize}
\item k次元空間にあるk-正単体はk+1個の頂点を持ち、すべての頂点は中心から等距離にあって、
各頂点への方向はどの２頂点を取り上げても
等しい角度$\theta$をなす($cos\theta=-\frac{1}{k}$)。\footnote{
k+1次元空間のk+1個のベクトルを$e_i =(0,0,....,0,1,0,...,0); i=1,2,...,k+1$という単位ベクトル
を考えます。
このk+1個のベクトルはk+1次元空間にあって、すべてのベクトルは直交しています。
今、このすべての中心を$o=\frac{1}{k+1}\sum_{i}^{k+1} e_i=\frac{1}{k+1}(1,1,1,...,1)$とします。
ここで、$f_i=e_i-o=(-\frac{1}{k+1},-\frac{1}{k+1},....,-\frac{1}{k+1},1-\frac{1}{k+1},-\frac{1}{k+1},...,-\frac{1}{k+1})$
とします。
$\sum_{i=1}^{k}f_i=-f_{k+1}$なので、これらは、k次元亜空間に納まっており、すべての$f_i$は同じ長さ
$|f_i|=\sqrt{\frac{1}{(k+1)^2}(k*1+k^2)}=\sqrt{\frac{k}{k+1}}$を持ち、
任意の$i,j, i \not = j$について、$f_i f_j=(k-1)*(\frac{1}{k+1})^2+2\times (-\frac{1}{k+1})(1-\frac{1}{k+1})=-\frac{1}{k+1}$であるから
その成す角$\theta_{i,j}$は$cos(\theta_{i,j})=\frac{f_i f_j}{|f_i||f_j|}=-\frac{1}{k+1} \frac{k+1}{k}=-\frac{1}{k}$
です。
}
\end{itemize}
低次元の場合には
\begin{itemize}
\item 1次元空間(直線)　1-正単体　線分
\item 2次元空間(平面)　2-正単体　正三角形
\item 3次元空間(空間)　3-正単体　正四面体
\end{itemize}

k+1個のカテゴリデータ型ではすべてのカテゴリが相互に対等な関係にあって、
k次元空間中のk-正単体の頂点とみなすことができます。

k=1のとき(カテゴリ数が2のとき)は、2-正単体の頂点に配置できるわけですが、
2-正単体とは、ただの線分で、その両端
に２つのカテゴリが配置されています。
直線配置できる点が連続データ型と共通であって、直線配置できることが、
「順序あり」扱いできることですので、２カテゴリカルデータ型は必ず順序ありなのです。
また、２カテゴリは相互に逆向きのベクトルなので、片方のデータを知れば、もう片方の
データは自ずから確定するのも、こういう事情です。

さて、カテゴリ数$k$を大きくしていくと、$cos(\theta_{i,j})=-\frac{1}{k}$は0に収束していきます。
$\theta_{i,j}$は直角に近づいていくということです。
\textcolor{red}{直交}\index{ちょっこう@直交}関係は
\textcolor{red}{独立}\index{どくりつ@独立}な関係なので、カテゴリ数が大きくなると、
カテゴリ同士は相互に独立な
関係に近づいて行くことがわかります。
次章で扱うグラフで正単体を描けば、頂点数k+1の完全グラフとなります。
すべての頂点の間に辺を引いたグラフで、\textcolor{green}{完全グラフ}\index{かんぜんグラフ@完全グラフ}という名前で呼ばれます。
また、正単体という高次元表現にも、グラフという平面表現にも共通するのは、すべての頂点を
相互に平等に扱う方法であるという点です。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch2/perfectGraph20100126/figures/Ks.eps}
   \includegraphics{./Fig/2-4.eps}

\caption{カテゴリを表す正単体は完全グラフ。頂点数：1,2,3,4,5,6,7,8,9,10,12,15,20,25,30
。
Knのnが頂点数。}
 \end{center}
 \label{fig:one}
\end{figure}


\section{データを比較する}

\subsection{２つのデータの関係　対称的な関係と非対称的な関係}
データの分類についてまとめました。
今度はデータを引き比べるとします。
引き比べることを、\textcolor{green}{関係}\index{かんけい@関係}
\footnote{
ここで言う「関係」は\textcolor{green}{二項関係}\index{にこうかんけい@二項関係}
と数学的に定義されています。
}
をみる作業と呼ぶことにします。
まず、関係には方向性があって、
ＡのデータのＢのデータに対する関係を見るのか、
ＢのデータのＡのデータに対する関係を見るのか
の２通りのやり方があります。
\begin{itemize}
\item{病気Ｘと診断されているかどうかという項目について、Ａさんは"1(はい、そうである)"、
Ｂさんも"1"です}
\end{itemize}
AとBとの関係は、$A=B$か$A\not = B$のいずれかです。
どちらの場合もAのデータのBのデータに対する関係とBのデータのAのデータに対する関係とが
同じです。
これを、「\textcolor{green}{対称}\index{たいしょう@対称}的」な関係と言います。
次は「非対称的」な関係の例です。
\begin{itemize}
\item{ＡさんはXを1つ持ち、Bさんは2つ持ちます}
\end{itemize}
このデータは順序があります。
そして、ＡさんのデータはＢさんのそれに対して、「小さ」く、
ＢさんのデータはＡさんのそれに対して、「大きい」です。
２方向の関係が異なりますから、これは、「非対称的」な関係です。

\subsection{非対称な関係を対称にする　距離}
順序のあるデータ型は非対称な関係をもたらしました。
次の例を見てみます。
\begin{itemize}
\item{Ａさんの検査Ｔ(定性)の結果は"++"で、Ｂさんのそれは"+++"です}
\end{itemize}
順序のあるカテゴリ型データで、非対称な関係です。
これを
\begin{itemize}
\item{Ａさんの検査Ｔ(定性)の結果はＢさんのそれと"1"段階違います}
\item{Ｂさんの検査Ｔ(定性)の結果はＡさんのそれと"1"段階違います}
\end{itemize}
と評価するとします。
こうしますと、「対称的」な関係になります。
順序があるデータについて、「差」を取ることによって、「対称的」にしています。
その結果、負の値をとらなくなりました。
絶対値を採用したとも言えます。
これは、\textcolor{green}{距離}\index{きょり@距離}と呼ばれるものです。

距離を扱いやすいように、定義します。

\subsubsection{距離の定義}
\begin{itemize}
\item{２つの何か（Ａ，Ｂ）の間に定められ}
\item{非負(0以上)の値をとり}
\item{その値は、ＡのＢに対する値もＢのＡに対する値も同じであり}
\item{ＡとＢとが同じときには0をとり}
\item{３つの何か（Ａ，Ｂ，Ｃ）があるときには、Ａ−Ｂ，Ｂ−Ｃ，Ｃ−Ａ
の間の距離は、
Ａ，Ｂ，Ｃの３つを平面に配置して三角形(つぶれてしまって直線になってもよい)
が描けるような値をとるもの（三角不等式を満足する）}
\end{itemize}

\subsubsection{三角不等式}
ここで、距離を定義するのに三角不等式という用語が出てきました。
これを説明します。

図4.5(左)を見てください。
平面上に３点A,B,Cがあります。
AからCへ行くのに、まっすぐ行くほうがBを経由していくよりも距離が短いです。
この関係がどの点からどの点へいく場合にもあてはまります。
△ABCの辺AB,BC,CAの長さがそれぞれc,a,bとすると、
\begin{equation*}
b+c>a,c+a>b,a+b>c
\end{equation*}
が成り立ちます。
このとき、
\begin{align*}
x=\frac{a+b+c}{2}-a = \frac{1}{2} (b+c-a) \ge 0\\
y=\frac{a+b+c}{2}-b = \frac{1}{2} (c+a-b) \ge 0\\
z=\frac{a+b+c}{2}-c = \frac{1}{2} (a+b-c) \ge 0\\
x+y=c, y+z=a, z+x=b
\end{align*}
が成り立ちますから、A',B',C'の３点を点Pからそれぞれx,y,zの距離に配置すれば(図4.5右)、
A'からB'へPを経由して行く道のりの長さは、ABの道のりと同じです。
B'C',C'A'の場合も同様です。
従って、
\textcolor{green}{三角不等式}\index{さんかくふとうしき@三角不等式}
が成り立つような、３点の距離関係は、３点間の道のりを変えることなく、
枝分かれで描けることがわかります。
本章の後半では、複数の点の間の距離の情報から、この枝分かれ道(\textcolor{red}{木}\index{き@木})を描く手法のことを述べますが、
距離の定義はそのような木を作れることを保証しています。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch2/triangleIneq/triangleIneq2.eps}
   \includegraphics[width=50mm]{./Fig/2-5.eps}

\caption{}
 \end{center}
 \label{fig:one}
\end{figure}


\subsection{ユークリッド距離とそれ以外の距離}
データが数直線上にあるとき、関係を距離として表すことができました。
私たちが日常生活で使っている、いわゆる距離は、平面上(２次元)にも空間中(３次元)にも
定義されています。
それは\textcolor{green}{ユークリッド距離}\index{ユークリッドきょり@ユークリッド距離}
距離と言われるもので、k次元空間においては
点$A=(a_1,a_2,....,a_k)$と点$B=(b_1,b_2,...,b_k)$の間のユークリッド距離は
\begin{equation*}
d_E(A,B)= \sum_{i=1}^k \sqrt{(a_i-b_i)^2} =\sum_{i=1}^k \sqrt{\delta_i^2} 
\end{equation*}
ただし、$\delta_{i}=|a_i-b_i|$、と定義されます。
しかしながら、距離の定義はすでに述べたとおりで、この式を必ずしも要求していません。
ですから、遺伝子型や表現型のデータを使って距離を定めるときも、
ユークリッド距離に限る必要はありません。
たとえば、
\begin{equation*}
d_M(A,B)=\sum_{i=1}^k |a_i-b_i|=\sum_{i=1} \delta_i 
\end{equation*}
で定義される\textcolor{green}{マンハッタン距離}\index{マンハッタンきょり@マンハッタン距離}と
いうものがあります。
マンハッタン距離は、図のように格子の辺をたどっていくときの距離です。
■図2-6.eps
\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch2/distance/distance.eps}
   \includegraphics[width=50mm]{./Fig/2-6.eps}
\caption{２つの黒丸の間に直線で結んだ距離がユークリッド距離($d_E$)で、格子(マンハッタン街区のような)を
最短でたどるのがマンハッタン距離($d_M$)です。
ユークリッド距離の道は１通りですが、マンハッタン距離の道の取り方は１通りではありません。
今、k次元で$d_M = \sum_{i=1}^k \delta_i$であるとすると、
通り道の取り方の場合の数は、
$\binom{d_M}{\delta_1 \delta_2 ... \delta_k}=\frac{d_M!}{\prod_{i=1}^k \delta_i!}$です。
15章参照}
 \end{center}
 \label{fig:one}
\end{figure}

ユークリッド距離とマンハッタン距離以外にも距離はいくつも定義できます。
脚注を参考にRを活用していろいろな距離の名称と定義を確認してみてください。
\footnote{
Rで距離を扱う関数であるdist()関数では、"euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski"の6種類
から選択することができます。
dist()関数の説明文書(\textcolor{red}{Rのヘルプ}\index{Rのヘルプ@Rのヘルプ}記事)
を表示したり、dist関数のソースコード(\textcolor{red}{Rのソース}\index{Rのソース@Rのソース})
を表示することで、これらの内容がわかります。
}

\subsection{配列の違いとマンハッタン距離}
たとえば、DNA塩基配列同士の距離は、塩基の異なる箇所数で表すことができます。
\begin{itemize}
\item{ヒト　:cccggaCAcCgActtcccGgggctcatt}
\item{マウス:cccggaTGcAgGcttcccAgggctcatt}
\end{itemize}
この部分の配列の長さは、29塩基で、5塩基が違います。
これは、29次元空間で、マンハッタン距離が5と考えることができます。
変異が５箇所に起きたことをマンハッタン距離５とみなしているわけです。
配列の比較には、塩基の置換の他に、挿入・欠失も考慮が必要です。
\begin{itemize}
\item{ヒト　:...cccggaCAcCgActtcccGgggctcattACcctCAc...}
\item{マウス:...cccggaTGcAgGcttcccAgggctcatt=Tcct=Tc...}
\end{itemize}
この場合には、置換と挿入・欠失という異質な出来事を総合的に考えて距離を
決める必要があります。
それぞれの出来事に、生物学的な背景を考慮して重み付けをします。
また、塩基配列の比較では、２配列がランダムに作られたという仮説に立つと、
ありえないほど似ており、そのありえなさを数値にして、配列の
似ている程度を数値化しますが、
その珍しさを\textcolor{red}{極値分布}\index{きょくちぶんぷ@極値分布}に照らしてP値として数値化することがあります。
よく使われる公共サイトのblastなどがその例です。

\subsection{距離の代わりに角度で表す　相関係数}
距離は対称な関係を0以上の値で表現しました。
負の数も含めて関係を量的に評価することはできないでしょうか。
カテゴリ型を空間に配置したときに、
すべてのカテゴリが相互に$\theta=cos^{-\frac{1}{k}}$なる関係にあって、
この角度が均等であることが、カテゴリの平等な関係を意味していると述べました。
このように角度も二つのものの間の関係を表すことができます。
$\theta$をその関係を表す量とすることができますし、$cos(\theta)$をその量としてもよいでしょう。
今、２つのデータがベクトル形式であれば、ベクトル同士の内積と長さで表されます。\\
$cos(\theta)=\frac{\sum_{i=1}^k x_i y_i}{|x||y|}$\\
これは\textcolor{red}{相関係数}\index{そうかんけいすう@相関係数}と呼ばれる値です。


\section{複数のサンプル、たくさんの比較}
\subsection{1対N-1とN対N}
２個のデータを比較してきました。
ここからは、N個のものを較べます。

ある１個に特に興味があって、それ以外の(N-1)個との関係が調べたいときもあります。
N個の全部に興味があるときには、
すべての関係を調べることになります。
関係が対称でないときには、NxN(\textcolor{red}{重複順列}\index{ちょうふくじゅんれつ@重複順列})の比較をしますし、
関係が対称であるときには、自身との関係の情報も取りたければ、$\frac{N(N+1)}{2}$ペア
(\textcolor{red}{重複組み合わせ}\index{ちょうふくくみわせ@重複組み合わせ})の関係を調べますし、
自身との関係は"0"であって、調べるには及ばない、ということであれば、$\frac{N(N-1)}{2}$ペア
(\textcolor{red}{組み合わせ}\index{くみあわせ@組み合わせ})の関係を調べます。
NxNの関係を図のように行列で表せば、
NxNは行列全体、対称的な関係のときは、
\textcolor{green}{下三角行列}\index{したさんかくぎょうれつ@下三角行列}部分のみ、１対その他ならば、１行のみを使います。

\begin{figure}[htbp]
 \begin{center}
%\includegraphics{./Figures/Ch2/nxn/nxn.eps}
   \includegraphics{./Fig/2-7.eps}
\caption{比較の仕方。すべての組み合わせは正方行列全体($N\times N$)。
対称な関係をみるときは、三角行列部分のみが対象}
 \end{center}
 \label{fig:one}
\end{figure}


\subsection{一部に関係がないとき　半順序}
NxNのすべてに関係が定義できれば行列状に関係をみればよいですが、
関係が定義できない場合が含まれることもあります。

例を挙げます。
４個の選択肢からの選択を考えます。
１個も選ばない場合を{0,0,0,0}、１番と３番を選ぶ場合を{1,0,1,0}と表すことにします。
こうすると、選び方は全部で$2^4=16$通りあります。
1,2,3番を選んだ場合と1,3番を選んだ場合は、前者の選択は後者の選択を含みます
(包含関係にあります${1,2,3,0} \subset {1,0,1,0}$) 。
しかしながら、${1,1,1,0}$と${1,0,0,1}$との関係は、包含関係が成り立ちません。
このように包含関係が定まる場合と定まらない場合があります。
$2^4$通りを4次元空間の点とみなして、
$k=4$次元の長さ１の\textcolor{green}{立方格子}\index{りっぽうこうし@立方格子}
\footnote{
1次元の立方格子点(0),点(1)を結んだ線分、
2次元立方格子は点(0,0),(0,1),(1,0),(1,1)を結んだ正方形、
3次元立方格子は点(0,0,0),(0,0,1),(0,1,0),(1,0,0),(0,1,1),(1,0,1),(1,1,0),(1,1,1)を結んだ立方体。
k次元立方格子は$2^k$個の点からなり、その点の座標の成分は0か1でできている。
}
の点として表すとします。
要素数が１個だけ異なっていて、包含関係にある点同士が辺で結ばれます。
$2^k$個の点を持つ多次元の格子は平面に描くのが難しいので、
図4.8のように平面に押しつぶして描くことができます。
図の上部にある点から下向きの辺だけをたどって下部にある点たどりつける場合には、
上の点は下の点に含まれる関係になっています。

このように一部に順序があり、一部に順序がないときに、
このような図で表されることがわかります。
\textcolor{green}{ハッセ図}\index{ハッセず@ハッセ図}と言います。
「一部に順序がある」という呼び方をしていましたが、それを\textcolor{green}{半順序}
\index{はんじゅんじょ@半順序}と言います。
「全部に順序がある」のが\textcolor{green}{全順序}\index{ぜんじゅんじょ@全順序}です。

"0,1"を２つのアレルとすれば、
４箇所の２アレル型多型が作る１６種類のハプロタイプを変異回数で結んだ場合も
多次元立方格子で表されます。

\begin{figure}[htbp]
 \begin{center}
  %\includegraphics{./Figures/Ch2/hasse2/hasse2.eps}
   \includegraphics{./Fig/2-8.eps}

\caption{
}
 \end{center}
 \label{fig:one}
\end{figure}

\subsection{距離行列と木}
今、N個の要素同士のすべてに対称的な関係があって距離として表されるとします。
N個のサンプルのN個のサンプルに対する関係を調べるとします。
距離で表す関係は対称なので、(下)三角行列と呼ばれる行列に納めることができます。
さらに、距離であるときには、自身と自身との関係は$0$であって不要なので、
以下に示すように、対角成分のない三角行列成分として表すことができます。
これが\textcolor{green}{距離行列}\index{きょりぎょうれつ@距離行列}です。
\begin{screen}
\begin{Schunk}
\begin{Soutput}
         1        2        3        4
2 4.725108                           
3 1.372943 4.600435                  
4 4.135837 2.220284 4.311534         
5 2.020896 3.608861 1.844573 3.591590
\end{Soutput}
\end{Schunk}
\end{screen}
距離の定義としてユークリッド距離とマンハッタン距離を用いて、木を作ってみます。
\textcolor{red}{Rのパッケージ}\index{Rのパッケージ@Rのパッケージ}
apeを取り込み、apeが持つ、\textcolor{red}{近隣結合法}\index{きんりんけつごうほう@近隣結合法}(Neighbor-Joining)による
木作成関数nj()を使います。
距離の定義が違うので、距離行列も異なりますから、
そこから作られる木も少し違ってきます。
木の作成はサンプルの\textcolor{red}{階層的クラスタリング
}\index{かいそうてきくらすたりんぐ@階層的クラスタリング}
\index{クラスタリング@クラスタリング}の一種です。
階層的クラスタリングは引き続き５章で扱います。

\begin{lstlisting}
#サンプル数Ns=5が５次元のデータを持っているようなデータセットを
作成
Ns<-5;k<-5;x <- matrix(rnorm(Ns*k), nrow=Ns)
dist(x,method="euclidean") # ユークリッド距離の距離行列
library(ape) # 木を作る関数nj()を持つパッケージapeの読み込み
treu<-nj(dist(x,method="euclidean")) # apeの近隣結合法関数njにより木を作る
trman<-nj(dist(x,method="manhattan")) # マンハッタン距離で木を作る
par(mfcol=c(1,2)) # 画面を１行２列に分割
plot(treu);plot(trman) # ２つの距離法で木の表示
par(mfcol=c(1,1))
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-003.eps}
\caption{ユークリッド距離(左)とマンハッタン距離(右)による距離行列に基づく近隣結合法による木}

 \end{center}
 \label{fig:one}
\end{figure}

\chapter{サンプルを個別に捉える}
サンプルに関してデータをとって、データから意味を取り出すのが遺伝統計学の
役割ですが、サンプルの取り扱いには大きく２つのやり方があります。

１つは、個々のサンプルを個別に扱う方法で、もうひとつは、
サンプルを集団として扱う方法です。
第５章では、個別に扱う方法についてを、
第６章では、集団として扱う方法についてを扱います。
\section{グラフとは}
\subsection{グラフの定義}

この章では、個々のサンプルを区別して扱います。
区別して扱うために、個々のサンプルは、それぞれを点として扱います。
サンプルを点で表して、それらの関係を表す方法のひとつが\textcolor{green}{グラフ}
\index{グラフ@グラフ}です。
すでに、いくつかのグラフの例が登場していました。

用語を整理する意味も込めて、グラフ理論の基礎事項をまとめます。
\begin{itemize}
\item 「頂点」と「辺」とで構成されたものをグラフと言います。
\item 頂点は、点であり、辺は、頂点と頂点を結んだものです。
\item 辺は２つの異なる頂点を結ぶ場合と、１つの頂点から出て同じ頂点に戻る場合とがあります。
\item 異なる２頂点を結ぶ辺のみを許せば、辺は頂点ペアの間の関係を表すことになります。
\item 辺に向きがあることとする場合と、向きがないこととする場合があります。
\item 向きのあるグラフを有向グラフ、ないグラフを無向グラフと言います。
\item 辺は頂点に「接続」し、ある頂点に「接続」している辺の数をその頂点の「次数」と言います。
\item 辺で結ばれた頂点は互いに「隣」であると言います。
\item ある頂点からある頂点まで辺をたどるとき、両端の頂点と途中の頂点と辺とを併せて
「道」と言います。
\item 「道」がある点から元の点に戻ってくるとき、それは、「サイクル」と言います。
\item グラフ上での２頂点間の距離は２頂点を結ぶ最短の道の長さです。
\item 特徴的なグラフには名前がついています。
\item すべての頂点の間に辺があるグラフは、「完全グラフ」と呼ばれます。
\item すべての頂点同士に道があり、サイクルがないグラフは「木」と呼ばれます。
\item 関係をグラフにすること利点のひとつは、グラフを扱うアルゴリズムや証明済みの
事項を利用できることです。
\end{itemize}
\section{サンプルを並べる 数直線というグラフ}


順序のある連続データ型の値を持つサンプルセットがあるとします。
サンプルを値に応じて数直線上に並べます(図5.1の中央の水平線とその上の点)。
この並び方はどのように読み取ればよいでしょうか。

数直線上の点から任意の２点を取り出して、その長さを
見えるように数直線の上部に並べたのが、
図5.1の中央の数直線の上部に描かれた線分です。
このようにしますと、
点を乗せた数直線には、
$N \times N$の関係が距離として埋めこまれていることがわかります。
このことから、
量的データを用いて定めたサンプル間距離を表したグラフは直線状のグラフとして
表せることがわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-004.eps}
\caption{中央の数直線上に５点あります。
それらが作る$5\times 4/2=10$個のペアの距離を表す線分が数直線上に示されています。
この10ペアの情報を埋め込んだのが、この数直線状のグラフです}

 \end{center}
 \label{fig:one}
\end{figure}



\section{木というグラフ}
\subsection{木とは}
前の例は、グラフと言ってもずいぶんと単純な形をしていました。
今度は、
グラフの中で\textcolor{green}{木}\index{き@木}と呼ばれるタイプのものを扱います。

前項の頂点を持った直線線分も木の特別な場合です。
図5.2を見てください。
グラフで言うところの木は、植物の木と同様に、ある点から成長して出来る形をとります。
その成長にあたっては、先端を伸ばすか、枝分かれするかの２通りの方法があります。
成長するときに出来る辺の先は必ず新しい頂点であって、
既存の頂点には接続しないので、木にはサイクルはできません。
木の上の２点間に辺を渡したら、必ずサイクルが出来ますので、木ではなくなります。
そのサイクルを構成する１辺を取り去ると、再び木に戻ります。
木の頂点の数と辺の数は\\
（T の辺の個数）=（T の節点の個数）- 1\\
の関係にあります。

木には「根(root)」がある場合(「根つき木」)とない場合があります。
根つき木は、木であって、その１つの頂点を特別扱いして、それを「根」と呼びます。
根つき木は辺に向きがあり(有向)、辺の向きは「根」から離れる方向です。
「根つき木」の先端には、次数１(接続している辺の数が１)の頂点(端末点)があり、それは、
「葉」と呼ばれます。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-10.eps}

%\includegraphics{./Figures/Ch2/tree/tree.eps}
\caption{頂点１のグラフも木です。白丸が「根」、黒丸が「葉」、灰丸がそれ以外の点です。
上段の右２つのグラフは無向グラフとしては同じですが、「根」を変えて
有向グラフとすると、グラフとして異なります。
下段の最左は、１本の辺を２頂点間に差し渡して、木でなくしたグラフです。
生じたサイクルの辺を１本取り去ると、必ず木に戻ります。}
 \end{center}
 \label{fig:one}
\end{figure}
\subsection{木の形状　トポロジー}
木でデータを理解するためには、木の違いがわかる必要があります。
木の違いは、枝分かれの具合と、分岐間の長さ・末梢の枝の長さで決まります。
長さには意味を持たせずに枝分かれの具合だけに着目したとき、その形状を木の
\textcolor{green}{トポロジー}\index{トポロジー@トポロジー}と言います(図5.3)。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-11.eps}
  %\includegraphics{./Figures/Ch2/treeTopology/treeTopology.eps}
\caption{木のトポロジー。トーナメント戦のやりかたが変わらなければトポロジーは同じです。
変わればトポロジーは異なります。
}
 \end{center}
 \label{fig:one}
\end{figure}

\section{木構造でのデータの理解 階層的クラスタリング}
\subsection{進化系統樹}
種の進化の過程を樹で表したものを
\textcolor{green}{系統樹}\index{けいとうじゅ@系統樹}と言います。
系統樹は「根つき」の木グラフで現在の種は「葉」にあたります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/2-12.eps}

%\includegraphics{./Figures/Ch2/TreeLeaves/TreeLeaves.eps}
\caption{進化系統樹は、生命の共通祖先から、現在の種まで木状の
関係があると考えて作られるものです。
現在の種は系統樹の葉に相当します。
過去には、共通の祖先がいると考えられています。
}
 \end{center}
 \label{fig:one}
\end{figure}
複数サンプル(今の場合は種)の外見的特長・塩基配列等、何かしらで遠近関係があるときに、その
関係を木として捉えようとした例です。
木の構造を、階層的クラスタ構造とも言います。
枝分かれがサンプルの分類(クラスタ化)をしており、
その枝分かれが、根元から末梢に向かって段階的(階層的)に繰り返されている
からです。
\textcolor{green}{分岐図}\index{ぶんきず@分岐図}(cladogram)とも呼ばれます。
サンプルを分類することで理解しようとする手法全体を\textcolor{green}{クラスタ解析}
\index{クラスタかいせき@クラスタ解析}・\textcolor{green}{クラスタリング}
\index{クラスタリング@クラスタリング}と
言いますが、階層的クラスタリングは木構造を作ってデータを理解する方法の１つです。

\subsection{階層的クラスタリング}
クラスタリング手法には、階層的クラスタリング以外に非階層的クラスタリングもあります。
\textcolor{red}{非階層的クラスタリング}
\index{ひかいそうてきくらすたりんぐ@非階層的クラスタリング}
はサンプルを集団として扱うので、６章で改めて扱います。

階層的クラスタリングは、サンプルペア間の遠近関係・類似関係から、
木のトポロジーと辺の長さを答えとして出します。
はじめは、すべての要素を独立したクラスタとし、
クラスタ同士をだんだんに合併することを繰り返すことで、最終的に１つに纏め上げます。

この処理を実行するにあたって、原則として
次の３つの決まりを定める必要があります。
\begin{itemize}
\item{遠近関係・類似関係の測り方}
\item{合併してできたクラスタと、その他のクラスタとの距離の決め方}
\item{合併の順序の規則}
\end{itemize}
です。
サンプル同士の遠近関係・類似関係の測り方については、対称的な量的な関係であれば、
距離でもよいですし、データベクトルのなす角に基づく値をとってもよいです。
前章での議論に基づいて選べばよいです。

複数サンプルが作るクラスタと、他のクラスタとの間の遠近関係・類似関係を
定めるためには、クラスタ内のどの点をどのように用いるのかを必要があります。
その方法を説明したのが図5.5です。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/2-13.eps}

%\includegraphics{./Figures/Ch2/clusterNJ/clusterNJ.eps}
\caption{
}
 \end{center}
 \label{fig:one}
\end{figure}

図5.5に示すとおり、
生じたクラスタとその他のサンプルや他のクラスタとの間の遠近関係・類似関係
を測るにあたり、クラスタの代表点を新たに定める場合と、定めない場合があります。
定める場合には、その定め方の定義が必要です。
図右上は\textcolor{green}{近隣結合法}(NJ法)\index{きんりんけつごうほう@近隣結合法}
\index{Neighbor joiningほう@Neighbor joining法}を示しています。
あくまでも、分岐図であることを意識した方法ですので、まとめる(結合する)２点の
根元側に共通な点を定め、
そこから２点が分岐したものとして新しい点を定めます。

右中段は、根元に近いところに新点を置くのではなく、新たに生じたクラスタの「真ん中」に
クラスタを代表する点を置きます(\textcolor{green}{重心法}
\index{じゅうしんほう@重心法}・\textcolor{green}{メディアン法}\index{メディアンほう@メディアン法})。
この「真ん中」の定義もいくつかの定め方があります。

他方、新たな代表点を定めない場合には、クラスタが範囲を持った領域であると
みなします。
このやり方では、クラスタを表す領域とクラスタ外部の点ないし領域との
関係を測る方法を定める
必要があります。
図左中段は、クラスタ化した後も、
クラスタの構成要素のすべてが測定に意味を持つ方法です
(\textcolor{green}{群平均法}
\index{ぐんへいきんほう@群平均法}(
\textcolor{green}{UPGMA}\index{UPGMA@UPGMA}（Unweighted Pair-
Group Method using Arithmetic averages法))、
\textcolor{green}{ウォード法}\index{ウォードほう@ウォード法})。
図下左は、クラスタとクラスタ外部の関係は最小関係を、
図下右は最大関係をとるように定めます(
\textcolor{green}{単連結法}\index{たんれんけつほう@単連結法}、\textcolor{green}{完全連結法}
\index{かんぜんれんけつほう@完全連結法})。

合併の順序のルールは、最も近いペア同士を合併していくのが素直な考え方なので、
そのようにします。

このように、距離・関係の定義が色々ありますし、
クラスタ間の距離・関係を決めるときに色々な方法がありますから、
出来あがるクラスタもいろいろです。

４章で、距離行列の扱いのときには、進化・集団遺伝学系のクラスタリング法であるNJ法を用いました。
ここでは、データマイニング系のRの関数hclust()を使ってみます。
\footnote{
hclust()の\textcolor{red}{Rのヘルプ}\index{Rのヘルプ@Rのヘルプ}を見ることにより、複数の手法
"ward"', '"single"', '"complete"', '"average"', '"mcquitty"', '"median"'  '"centroid"'
が利用可能であることがわかります。
\textcolor{red}{Rのソース}\index{Rのソース@Rのソース}を確認すれば、その定義が確認できます
}
\begin{lstlisting} 
distMatrix<-dist(x,method="manhattan") # 距離にはマンハッタン距離を使用
trclust<-hclust(distMatrix,method="ward") # クラスタ間距離の定義にはウォード法を使用
plot(trclust)
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-005.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}



\section{行列データを眺める}
\subsection{要素を並び替えて眺める ヒートマップ}
$N\times N$の関係を階層的クラスタリングにて木に纏め上げましたが、$N$個のサンプルの間の関係が
複数($M$個)の項目によって決まるときに項目の方の関係にも興味があったらどうしたらよいでしょうか？
両方で階層的クラスタリングをして、$N\times M$個のデータを表示させてみればよいでしょう。

図5.7(左)は、\textcolor{green}{ヒートマップ}
\index{ヒートマップ@ヒートマップ}という図示方法です。
データはデータは$6 \times 20$の長方形型の行列です
(２アレル型多型が６箇所あり、
20本の染色体のハプロタイプを想定しています)。
以下のようにしてデータを作り、ヒートマップを作るheatmap()関数に処理させて
います。

\begin{lstlisting}
m<-matrix(rbinom(120,1,0.5),20,6)
heatmap(m)
\end{lstlisting}

水平軸では6個の多型がクラスタリングされ、
垂直軸では20本の染色体がクラスタリングされています。
$6\times 20$個のデータはその値に応じて濃淡がついています。
\footnote{関数heatmap()は、内部で、距離行列を
作る部分にdist()関数を、クラスタリングにhclust()関数を用いています。\textcolor{red}{Rのソース}
\index{Rのソース@Rのソース}を確認するとわかります}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-006-2.eps}
   \includegraphics[width=50mm]{./Fig/PartII-007-2.eps}
\caption{6多型・20染色体の多型データのヒートマップ(左)と6多型の連鎖不平衡マップ(右)}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{要素を並び替えずに眺める 連鎖不平衡係数プロット}
ヒートマップでは$N \times M$行列の情報を、N側とM側の両方でクラスタリングし、
$N \times M$をカラースケールで視覚的に表現しました。
クラスタリングの結果、要素の並び順が変更されました。
要素の並び順に意味があるときは、並び順を変えずに表示します。
多型マーカーが染色体上に並んでいるときに、
マーカー間の$N\times N$の関係を表示してみます。
マーカー間の関係として、\textcolor{red}{連鎖不平衡係数}\index{れんさふへいこうけいすう@連鎖不平衡係数}
\index{れんさふへいこう@連鎖不平衡}を使って、
図示することにします。
連鎖不平衡の係数のうちrと呼ばれるものは、\textcolor{red}{相関係数}
\index{そうかんけいすう@相関係数}でしたので、
以下のようにして図5.7(右)にすることができます。
実際には、相関係数が1のときと-1のときは、
片方の多型のアレルの"0","1"のラベル付けを取り替えれば、同じことですので、
$r^2$を連鎖不平衡係数としてよく使いますので、以下の図もそのようになっています。
これは、連鎖不平衡の視覚的表現で最もよく使われるプロットです。
\begin{lstlisting}
cormatrix<-cor(m);rsqmatrix<-cormatrix^2
image(1:nrow(rsqmatrix),1:ncol(rsqmatrix),rsqmatrix,col=gray((100:0)/100))
\end{lstlisting}

\subsection{片方の軸に着目 両方の軸に着目}
もともとのデータが$N\times M$の形をしていたときに、
クラスタリングをすれば、要素の関係の強弱により、
要素の順序が変わります。
行についてクラスタリングを行ったり、
列についてクラスタリングを行ったり、
行・列の両方についてクラスタリングを行ったり(ヒートマップ)する
ことができました。

要素順を変更せずに、$N\times N$の関係表示をすることもできました(連鎖不平衡係数プロット)。
$N\times N$とは逆に$M\times M$で相関を取って図示することもできます。
これらの関係は図5.8のようになっています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/2-14.eps}

%\includegraphics{./Figures/Ch2/NxM/NxM.eps}
\caption{２次元データそのものは左上。
行のN個の要素について処理すれば、右上になります。
要素の順序を変えればクラスタリングですし、
変えなければ、相関パターンの図示になります。
列のM個の要素についても同様です(左下)。
両方でクラスタリングをするとヒートマップ表示になる}
 \end{center}
 \label{fig:one}
\end{figure}
\section{個体の家系図・アレルの系図　同一種の中のグラフ}
親子・血縁関係を図で表した\textcolor{green}{家系図}\index{かけいず@家系図}
というものがあります。
遺伝学では血縁関係は非常に重要ですから、家系図も大変重要です。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-15.eps}
%\includegraphics{./Figures/Ch2/pedigree/pedigree.eps}
\caption{常染色体性劣性遺伝病の家系図の例です。
左のI II III IV Vは世代番号です。
四角は男性、丸は女性を表します。黒塗りは発病者、白抜きは非発病者です。
白抜きのうち、中央に点がある場合は、遺伝因子を有する人(保因者)です。
男女が水平線で結ばれているときには、生物学的な父母であることを示しています。
男女の水平線から垂直に下りた線の先に子が続きます。
子が複数あるときには、その線に水平線が接続し、水平線から垂直線が枝分かれして
その先に子が接続します。
両親を同じくする子はこのように水平線からの枝分かれで示されます。
第III世代の右から２番目の女性は２人の男性と水平線で接続し、それぞれの関係から子供を持っています。
この場合、第IV世代の右端と中央の２人は半同胞です。
第IV世代の夫婦は二重線です。
これは、近親婚関係を表しています。
斜めの線は死亡を表しています。
個人を頂点に、親子関係を辺に変えたのが中央の図。見やすくするために死亡の斜線は省略してあります。
右の図は、サイクルの１つを強調した図です。
}
 \end{center}
 \label{fig:one}
\end{figure}

この図は、点(丸と四角)が線で結ばれていますが、すべての線が点と点とを結んでいるわけではないので
\textcolor{red}{グラフ}\index{グラフ@グラフ}ではありません。
また点にフェノタイプ(発病・非発病、死亡・生存)の情報が付随し、
夫婦間の線に近親婚か否かの情報があるなど、
複雑な情報を搭載している点も、データ解析的には取り扱いを難しくします。
家系図をグラフとして扱うためにはどのようにすればよいでしょう。
図の中央が血縁関係をグラフ化した図です。
遺伝因子の伝達関係を表すためのグラフとしたので、夫婦の関係を表す水平線は消え、
子供を表すの頂点を介して間接的に夫婦の関係が示されるようになりました。
近親婚を表す２重線も消えました。
近親婚であることは、近親婚で生まれた子から出発すると、グラフをぐるりと
回って、本人に戻ってこられることからわかります。
なお、辺に向きをつけていませんが、遺伝子の伝達は親から子へ伝わりますから、伝達関係を
考慮すれば\textcolor{red}{有向グラフ}\index{ゆうこうグラフ@有向グラフ}です。
木ではありません。

\subsection{個人の関係のグラフと染色体の関係のグラフ}
\subsubsection{個人の関係のグラフ　家系図}
図5.10(左)は常染色体劣性遺伝病の家系図です。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-16.eps}

%\includegraphics{./Figures/Ch2/DiploidPedigree/DiploidPedigree.eps}
\caption{常染色体性劣性遺伝形式の形質の家系図とそのグラフ化表現}
 \end{center}
 \label{fig:one}
\end{figure}
図5.11は個人が持つ、染色体を三角で表して、その伝達関係をグラフにしてあります。
子に伝わる染色体は、必ず親の染色体セットの両方から一部分ずつを受け継ぎます。
したがって、すべての染色体は2本の辺を親の染色体から受け入れます。
染色体数は個体数の2倍あるので、その分だけ、グラフが混雑していますが、
ある染色体に着目して、祖先の方向に向かってたどると、
世代ごとに祖先染色体が2倍になる２分岐木があることがわかります。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-17.eps}

%\includegraphics{./Figures/Ch2/ChromosomePedigree/ChromosomePedigree.eps}
\caption{染色体の伝達グラフ}
 \end{center}
 \label{fig:one}
\end{figure}
\subsubsection{個人の関係のグラフは複数の染色体の関係のグラフを含む}
ある親の２倍体染色体から、精子もしくは卵子の１倍体染色体が取り出される過程は、
観測できないので、その取り出しパターンは、推測するしかありません。

ある家系において、ジェノタイプを調べたとします。
そのジェノタイプを満足する、染色体の伝達グラフのパターンは１つとは限りません。

2アレル型多型の例を示します。
図5.12を見てください。
２アレル型の多型のジェノタイプを観察したときに、
上段と下段と、どちらも白黒の三角パターンは同じですが、
辺の結び方が違います。
遺伝因子解析では、ジェノタイプデータに基づいて、
アレルがどのように伝達してきたかを
推定しますが、その必要性をこの図は示しています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-18.eps}
%\includegraphics{./Figures/Ch2/HaploidPedigree/HaploidPedigree.eps}
\caption{２アレル型多型の伝達グラフ。第II世代の左から３番目の個体が受け取る白△の染色体が、
下段では、同胞と共有されているのに対して、
上段では、共有されていません。
また、第III世代の左から2番目の個体の黒▲の染色体は、上段では父親から受け取っていますが、
下段では母親から受け取っています。
}
 \end{center}
 \label{fig:one}
\end{figure}
\subsection{染色体の伝達グラフと組換え}

染色体の伝達グラフを、染色体の場所ごとに分解してみます。
図5.13の上段は、２組の染色体対(黒と白)から、交叉が１箇所でおきて、
\textcolor{red}{組換え}\index{くみかえ@組換え}染色体ができる様子を示しています。
染色体を、３つの部分A,B,Cに分けて考えます。
それぞれの部分での染色体の伝達の様子を中段に示します。
A,B,Cに分けてグラフを描くと、子染色体と親染色体との関係は
１対１で、枝分かれがありません。
逆に、交叉・組換えが起きた点をまたいで伝達グラフを描くと、
下段にあるように、枝分かれが生じます。
交叉・組換えが起きていない範囲だけを取り出すと、
何世代さかのぼっても、枝分かれのない一本道の伝達関係になります。
図の下段ではAとB、BとC、A,B,Cのすべてを重ね合わせました。
このように重ね合わせると、染色体のグラフが分岐木になります。
なお、長さ１塩基の線分の内部で交叉・組換えは起きませんので、塩基に関する
伝達グラフは必ず枝分かれのない一本道になります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-19.eps}

%\includegraphics{./Figures/Ch2/RecombinationPedigree/RecombinationPedigree.eps}
\caption{上段の図は６本の白黒棒が描かれています。それぞれが、染色体で、上側の４本は親染色体で、２本ずつの
ペアになって、下側の子染色体をぞれぞれ作ります。
左側のペアは、ＡとＢの間で交叉・組み換えを起こしているために、子染色体はＡ部分が黒、Ｂ，Ｃ部分が白です。
右側のペアはＢとＣの間で交叉・組み換えを起こしています。
図の中段は、染色体のＡ、Ｂ、Ｃのそれぞれの部分での、染色体の伝達をグラフにしたものです。
アレルの伝達と同様で、親染色体と子染色体は１対１対応になっています。
図の下段は、中段のＡとＢ、ＢとＣ、ＡとＢとＣとを、重ね合わせた図です。
}
 \end{center}
 \label{fig:one}
\end{figure}

\subsection{祖先に遡る　コアレセント}
染色体の伝達グラフをより多くの世代にわたって考えてみることにします。

染色体の集団があって、交叉・組み換えをしつつ、次世代の染色体を作る過程での様子を
シミュレーションしてみます。
図5.14は、８本の染色体で１０世代の経過を見ています。
縦軸が世代経過を表しています。各世代にある８個の点が染色体を表しています。
左上の図は、ある特定の塩基についての伝達グラフです。
１塩基の伝達グラフなので一本道です。
最も若い世代(最下端の世代)の８本の染色体の親をたどると、一番上の世代では、
２本の染色体に行き着いていることがわかります。
現在の染色体の祖先染色体を遡るとだんだんに同じ染色体に行き着くわけですが、
これを\textcolor{green}{コアレセント}\index{コアレセント@コアレセント}(合体・統合)
と呼びます。

右上のグラフは、左上のグラフの塩基の近傍の塩基での伝達の様子です。
木の形は良く似ていますが、左の方が右よりも濃い木が大きいことがわかります。
この２つのグラフを重ね合わせたのが、左下のグラフです。
第６世代の左から２番目の染色体だけが、上から２本の辺を受けています。
祖先に向かって、一本道ではなくなり、分岐が生じています。
これは、２つの塩基の間で交叉・組換えが起きたことを示しています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-20.eps}

%\includegraphics{./Figures/Ch2/Coalescent/Coalescent.eps}
\caption{
}
 \end{center}
 \label{fig:one}
\end{figure}




右下の図では、薄い色の木の途中に変異を表す★印が描きこまれ、その変異を引き継ぐ部分が太い
木として描いてあります。
この木はこの★印が「根」になります。

このように、特定の塩基に着目すると、複数の木が独立に存在して、根から枝を介する部分は過去
にあり、現在は、その末端である葉として現われていることになります。
この葉を２つ持っているのが個人です。
個人はフェノタイプを持っています。
すべての塩基について、伝達の木を描くことができて、
隣合う塩基の木々のパターンはほぼ同じで、交叉・組換えが起きたときにのみ、パターンが変わります。
塩基ごとの伝達グラフはゲノム全体にわたって存在しており、
そのすべての木を重ね合わせたものが、塩基配列情報の現在と過去とその伝達に
関する情報のすべてです。


実際、ある変異がフェノタイプと関係していることを調べる作業は、
この過去に遡るたくさんの木々のパターンのうちのどれが、
葉に現われているフェノタイプのパターンと良く合致するかを判断することと同じで。
この点は、家系サンプルを用いた\textcolor{red}{連鎖解析}\index{れんさかいせき@連鎖解析}でも、
集団からのサンプルを用いた\textcolor{red}{関連解析}\index{かんれんかいせき@関連解析}でも同じです。

\section{ネットワーク}

個々のサンプルを個別に取り扱う方法としてのグラフに関する最後のトピックとして
\textcolor{green}{ネットワーク}\index{ネットワーク@ネットワーク}を取り上げます。
木はサイクルを持たないグラフですが、サイクルを持ったグラフを活用することもできます。
サイクルを持った有向グラフでそこに何かしらの流れを想定するとき、
それはネットワークと呼ばれます。

分子としてのＤＮＡ、ＲＮＡ、タンパク質、その他の因子は、
同時に存在して相互に影響を及ぼしあうことができます。
これらの関係を表現しようとすると、\textcolor{green}{サイクル}
\index{サイクル@サイクル}が生じます。
ＡがＢを促し、ＢがＡを促すという関係は、\textcolor{green}{ポジティブフィードバック}
\index{ポジティブフィードバック@ポジティブフィードバック}\index{フィードバック@フィードバック}
と呼びますし、
ＡがＢを促し、ＢがＡを抑制するという関係は、\textcolor{green}{ネガティブフィードバック}
\index{ネガティブフィードバック@ネガティブフィードバック}と呼ばれ、
どちらも、単純に表せば、図5.15上のようになります。
Ａ，Ｂ、Ｃの３要素にしてみます。
ＣがＡに影響を与える様子を「作用している」印象を強めて図にしたのが
図5.15の中段です。



\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/2-21.eps}


%\includegraphics{./Figures/Ch2/feedback/feedback.eps}
\caption{フィードバックとグラフ。
左上がポジティブフィードバック、右上がネガティブフィードバック。
中段は３点間の相互関係を点の配置を換えて描いたもの。
左下は自身へのフィードバック。
右下は２つの要素の作用に相互の影響がある場合を示しています}
 \end{center}
 \label{fig:one}
\end{figure}
少しずつ、配置を変えていくと、３頂点の間に方向の異なる２つの辺を引いた状態
という意味では、変わりがないことが
わかります。

要素間のネットワークの解析では、図5-15 右下のような関係も登場します。
２つの要素を組み合わせたときに初めて、第３の要素への効果が生じる、という関係です。
このような場合には、「２要素の組み合わせ」に対応した点をグラフに付け加える
ことになります。
遺伝形式を取り扱うときに列を追加する必要があったことと同じことです(図3.7)。

要素の組み合わせを考えるときには、\textcolor{red}{組み合わせ}\index{くみあわせ@組み合わせ}
の数が要素数に応じて猛烈な勢いで増えていくために、グラフが
巨大となり、全部を数え上げることは、現実的で
なくなることが多いです。

また、サイクルのあるグラフは無向にしろ有向にしろ、読み取りが難しくなります。
どこから見始めて、どのように見終えたらよいかがわかりにくいからです。
ですから、サイクルのあるグラフを理解するときには、
そのグラフを特徴づけたり、説明したりする\textcolor{red}{指数}\index{しすう@指数}
を取り出すという作業が発生します。

\chapter{サンプルを集団として捉える}
前章では個々のサンプルを区別しました。
今度は、サンプルの集まりに着目します。
\section{分布として捉える}
\subsection{１次元}
複数のサンプルについて一つの量的データ型のデータがあるものとします。
そのデータの集まりの特徴を知るために、
\textcolor{green}{箱ひげ図}\index{はこひげず@箱ひげ図}と
\textcolor{red}{密度分布}\index{みつどぶんぷ@密度分布}と
\textcolor{red}{累積密度分布}\index{るいせきみつどぶんぷ@累積密度分布}を描いて見ます。
\begin{lstlisting}
n1<-1000 # サンプル数
# １峰性サンプル作成
popdata1<-rnorm(n1,0,0.5) # 正規分布からの乱数発生
par(mfcol=c(1,3)) #画面を１ｘ３に分割
boxplot(popdata1) # 箱ひげ図
plot(ecdf(popdata1)) # 標本の累積分布
plot(density(popdata1)) # 密度分布
par(mfcol=c(1,1))
summapry(popadata1) # データの基本統計
#２峰性サンプル作成
popdata2<-c(rnorm(n1,0,0.5),rnorm(n2,5,1))
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-008.eps}
   \includegraphics[width=50mm]{./Fig/PartII-009.eps}
\caption{左から、１峰性分布の累積分布、密度分布、箱ひげ図、２峰性分布の
累積分布、密度分布、箱ひげ図}

 \end{center}
 \label{fig:one}
\end{figure}



左の例では、密度分布プロットから、１峰性の分布であることが読み取れます。
累積分布プロットの勾配の具合からも同様のことを読み取れます。
一峰性であれば、箱ひげ図やそれが表している、\textcolor{green}{基本統計量}
\index{きほんとうけいりょう@基本統計量}
最小値(Min)・最大値(Max)・\textcolor{green}{四分位点}
\index{しぶんいてん@四分位点}(1st Qu,3rd Qu)、
\textcolor{green}{中央値}\index{ちゅうおうち@中央値}(Median)・
\textcolor{green}{平均}\index{へいきん@平均}(Mean)などの情報が
かいつまんだ\textcolor{green}{統計量}\index{とうけいりょう@統計量}として有用です。

右の例は、
２つの峰が並んでいて、小さい値の峰の方が高いことがわかります。
峰の幅は小さい値の峰の方が狭いです。
累積プロットの方で、この同じ情報を読み取れます。
２峰性であることは、急峻な部分が２箇所あることからわかります。
また、急峻な部分の長さが、値の小さい方で長く、大きい方で短いことから
密度分布の峰の高さが値の小さい峰で高いことがわかります。
急峻な部分の左右の変化が値の小さい方で小さいことから、峰の幅は狭い
ことが読み取れます。
このような場合には２峰性であることが最も重要ですから、
１峰性を想定したときにかいつまんだ情報を提供する箱ひげ図や、
基本統計量の情報が役に立っていません。
\subsection{２次元}
データが2次元になっても事情は同じです。
３峰性のデータを作って、散布図、濃淡での\textcolor{green}{散布図}
\index{さんぷず@散布図}、鳥瞰図、\textcolor{red}{クラスタリング}\index{くらすたりんぐ@クラスタリング}
図を描きます。
\begin{lstlisting}
# 正規乱数を用いてデータを作る
n1<-500;n2<-300;n3<-200;x<-c(rnorm(n1,0,0.5),rnorm(n2,5,1),rnorm(n3,8,2));y<-c(rnorm(n1,0,2),rnorm(n2,3,2),rnorm(n3,-3,1))
library(gregmisc) # hist2d()を持つパッケージ
h2d <- hist2d(x,y, show=FALSE,same.scale=TRUE, nbins=c(10,10)) # 2次元ヒストグラム情報を取る
plot(x,y) # 散布図
filled.contour( h2d$x, h2d$y, h2d$counts, nlevels=9,col=gray((8:0)/8) ) # 2次元ヒストグラムを濃淡で
persp( h2d$x, h2d$y, h2d$counts,ticktype="detailed", theta=60, phi=30,shade=0.5, col="cyan") # 2次元ヒストグラムを鳥瞰図で
\end{lstlisting}


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-010.eps}
   \includegraphics[width=50mm]{./Fig/PartII-011.eps}
   \includegraphics[width=50mm]{./Fig/PartII-012.eps}
   \includegraphics[width=50mm]{./Fig/PartII-013.eps}
\caption{左上が散布図、右上は濃淡で分布を表したもの。左下は密度を高さにとった
鳥瞰図、右下は、クラスタリングをして色分けをした散布図}

 \end{center}
 \label{fig:one}
\end{figure}


分布を2次元平面にプロットしたり、それを濃淡や、高低鳥瞰図にしたりして、
分布の特徴を捉えます。
一峰性であれば、その峰の特徴を捉える値でかいつまむことになります。
多峰性であれば、いくつの峰があるか、峰同士の相互位置関係はどうなっているか、
個々の峰の特徴はどうか、ということをデータから読み取る必要があります。
この例では、３峰性です。
次元が3以上になると、図にするのが難しくなりますが、考え方は同じでです。


\section{非階層的クラスタリング}
ここで示した２次元に広がっているサンプルが３つの峰を作っているようなので、
\textcolor{green}{非階層的クラスタリング}\index{ひかいそうてきくらすたりんぐ@
非階層的クラスタリング}手法でサンプルを３つの峰に帰属させてみることにします。
代表的な手法である\textcolor{green}{k-means法}\index{k-meansほう@k-means法}
では、まずクラスタ数 $k$ を与え、
サンプルをk個のクラスタのうちのどれかに暫定的に帰属させます。
次にクラスタの暫定的構成サンプルを用いて、クラスタの中心を定めます。
クラスタの中心が定まったら、個々のサンプルの位置と、クラスタとの位置関係から、
サンプルが帰属するべきクラスタを変更します。
こうすることで、クラスタの構成サンプルが変わります。
構成サンプルが変わるとクラスタの中心が動くので、中心を更新します。
このように
適当にクラスタ(峰)の中心を定め、
サンプルを$k$のクラスタに帰属させつつ、
帰属の変更を繰り返すことで
その中心を移動させていきます。
最終的サンプルの帰属クラスタが変わらなくなるまで続け、
その状態を、答えとする方法です。

\begin{lstlisting}
# 非階層的クラスタリング
m3<-matrix(c(x,y),ncol=2)
cl <- kmeans(m3, 3) # kmeans法で３群にクラスタリング
plot(m3, col = cl$cluster)
points(cl$centers,pch = 8, cex=10) # クラスタの中央に印をつける
\end{lstlisting}


\section{集団遺伝学}
生物個体が集まって集団を構成します。
この個体の集団の様子を取り扱う遺伝学の一分野として\textcolor{green}{集団遺伝学}
\index{しゅうだんいでんがく@集団遺伝学}という独立した分野があります。
いくつかのトピックスを取り上げて、その内容について触れていきます。

\subsection{不均一と不平衡}
ある一定以上の個人が構成する集団で出会いの機会が完全にランダムであることは考えにくく、
実際には、偏りが生じます。
偏る要因としては、物理的に遠いところに存在する場合や、往来を制約する地理的な要因(山・川・海)などの自然の
要因もあれば、言語・宗教・民族・国家などの人的要因もあります。
隔たりがあっても、完全に隔絶されていなければ、徐々に混ざり合って、隔たりがなかった場合と同じような状態に
向かいます。
また、人口が増加していく過程では、構成メンバーが十分に混じりあう暇がないので、
人口の増加も不均一化の要因になります。
ですから、不均一な集団を、均一になるまでの時間的途中経過の状態と捉えることも出来ます。

\subsection{均一な集団とHardy-Weinberg 平衡(HWE)　均一な集団の混合}
集団内でのメイティングがランダムであるときに生じる平衡が\textcolor{red}{HWE}
\index{HWE@HWE}です。
現実には、平衡に達していないことも多いです。
しかしながら、不均一な状態をそのまま取り扱うことは面倒なことが多く、
また理解もしにくいので、よく行われるのは、
HWEを満足する集団が複数存在し、
それが混ざったものとして、不均一な状態を表そうとする方法です。

一番、基本的な場合として、２集団の混合を考えます。

２つの集団があり、２アレル型多型A,aについて、それぞれ、Aのアレル頻度がp,qであり、
集団内部ではHWEであるときに、この２集団がr,(1-r)の比率で混合しているときを考えます。

第１集団ではジェノタイプ頻度が
\begin{equation*}
p^2,2p(1-p),(1-p)^2
\end{equation*}
で、第２集団では
\begin{equation*}
q^2,2q(1-q),(1-q)^2
\end{equation*}
です。
混合集団では、
\begin{equation*}
(gm1,gm2,gm_3)=(cp^2+(1-c)q^2, 2cp(1-p)+2(1-c)q(1-q),c(1-p)^2+(1-c)(1-q)^2))
\end{equation*}
となります。

Aに値1,aに値0を与えれば、それぞれの集団での、
平均と分散は、
\begin{align*}
m_1=2p,v_1=2p(1-p)\\
m_2=2q,v_2=2q(1-q)
\end{align*}
です。
混合集団での、アレル頻度は$cp+(1-c)q$と構成比率に比例した値になり、
ジェノタイプの値の平均もその２倍で
\begin{equation*}
mm=2(cp+(1-c)q)=2(c m_1+(1-c)m_2)
\end{equation*}
ですが、
分散の方はそれほど簡単にはいきません。
3章で見たように、\textcolor{red}{共分散}
\index{きょうぶんさん@共分散}の項が入ってくるからです。

このように均一な集団が混合した集団は、構造のある集団、
\textcolor{green}{構造化}\index{こうぞうか@構造化}した集団と呼びます。

\subsection{時間的な変化}
\subsubsection{\textcolor{green}{拡散方程式}\index{かくさんほうていしき@拡散方程式}}
今度は時間経過を考えます。

今、２つの島Ａ，Ｂがあって、２島はそれぞれ隔絶していたとします。
ある多型について、ＡはアレルMばかり、Ｂはアレルmばかりだったところに
その島が陸続きになった(地殻変動でも橋がかけられたでもなんでもよいのですが)とします。
Ａ、Ｂの人口は$P_A,P_B$で変わらず、両島の間では、単位時間あたり$d$の人がＡからＢへ移住し、
逆にＢからＡへも移住するとします。
Ａ、ＢのアレルMの頻度を時間$t$の関数で表し、$Fa(t),Fb(t)$とし、陸続きになったときを$t=0$とします。
$t\ le 0$のとき$Fa(t)=1,Fb(t)=0)$です。
ここで、$t=T$から$t=T+\delta$への変化を考えます。Ａ、Ｂそれぞれについて、$t=T$のときの
アレルＭの数のうち、移住しない分から、移住して出て行く分を引き、逆に移住して
入ってくる分を足しますから
\begin{equation*}
2P_A Fa(T+\delta)=2P_A Fa(T) -2d\delta Fa(T) + 2d \delta Fb(T)
\end{equation*}
\begin{equation*}
2P_B Fb(T+\delta)=2P_B Fb(T) -2d\delta Fb(T) + 2d \delta Fa(T)
\end{equation*}
という関係にあります。
２つの式にそれぞれ$P_B,P_A$をかけて引くと
\begin{equation*}
2P_A P_B (Fa(T+\delta)-Fb(T+\delta))=2P_A P_B (Fa(T)-Fb(T))-4d\delta (P_A+P_B)(Fa(T)-Fb(T))
\end{equation*}
となり、
\begin{equation*}
(Fa(T+\delta)-Fb(T+\delta))-(Fa(T)-Fb(T))=- \frac{2(P_A+P_B)}{P_A P_B} (Fa(T)-Fb(T))
\end{equation*}
となります。
ここで、$G(T)=Fa(T)-Fb(T)$と置けば、
\begin{equation*}
G(T+\delta)-G(T)=-  \frac{2(P_A+P_B)}{P_A P_B} G(T)
\end{equation*}
です。ここから
\begin{equation*}
\frac{d}{dt} G(T)=-  \frac{2(P_A+P_B)}{P_A P_B} G(T)
\end{equation*}
なので、
\begin{equation*}
G(T)= K e^{- \frac{2(P_A+P_B)}{P_A P_B}t}
\end{equation*}
今$G(0)=1$ですから$K=1$です。
$G(t)$の微分方程式は、アレル頻度の差の変化量が、
アレル頻度の差に比例していることを示した式です。
$P_AFa(t)+P_BFb(t)=P_AFa(0)+p_BFb(0)=P_A$\\
なので、
\begin{equation*}
Fa(t)=\frac{P_A + P_B e^{- \frac{2(P_A+P_B)}{P_A P_B}t}}{P_A+P_B}
\end{equation*}
\begin{equation*}
Fb(t)=\frac{P_A (1- e^{- \frac{2(P_A+P_B)}{P_A P_B}t})}{P_A+P_B}
\end{equation*}
となります。
このアレル頻度の変化の様子をＲで描いてみます。
\begin{lstlisting}
pa<-9000;pb<-1000;d<-100;t<-0:100 # pa,pb:２集団の人口,d:単位時間あたりの移住人数,t:世代
fa<-(pa+pb*exp(-2*d*(pa+pb)/(pa*pb)*t))/(pa+pb)
fb<-(pa*(1-exp(-2*d*(pa+pb)/(pa*pb)*t)))/(pa+pb)
plot(t,fa,ylim=c(0,1),type="l")
par(new=T)
plot(t,fb,ylim=c(0,1),type="l")
\end{lstlisting}
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-015.eps}
\caption{時間とともに２地域のアレル頻度の差が縮まって、最後には同じ値に収束します}

 \end{center}
 \label{fig:one}
\end{figure}


時間(横軸)とともに、２島のアレル頻度は同じ値に収束します。
収束する頻度は、移住開始前に２島を合わせた全員に占めるアレルの頻度です。
また、収束までの時間は、移住の人数が島人数に占める割合によります。
２島が隔絶していた状態から、交流のある状態に変わったことで、アレル頻度が変化しました。
時間が十分にあれば、移住のスピードによらずあるアレル頻度に落ち着いて、
変化しなくなります。
この状態が平衡状態です。

このときのアレル頻度の変化の量がアレル頻度に比例する形の微分の式が出てきました。
これは熱力学で拡散方程式と呼ばれる形式の式のもっとも単純なものです。
\subsubsection{推移行列でも表す}
島の数を増やします。
島が５つあり、それぞれの島から他の島へと移住をするとします。
移住する人数は、島の人口の一定の割合で、移住先を選ぶ割合も変化しないとすると、
Ａ島から、Ｂ，Ｃ、Ｄ、Ｅ島へ移住する人数は、Ａ島の人口$P_A$を用いて、
$P_A *m_{A\to X};X=B,C,D,E$と表されます。
移住しない割合を$m_{A\to A}=1-\sum_{X\in \{B,C,D,E\}}m_{A \to X}$と表せば、
$P_A(T+1)=P_A(T)m_{A\to A} +\sum_{X \in \{B,C,D,E\}} P_{X} m_{X \to A}=\sum_{X \in \{A,B,C,D,E\}} P_{X} m_{X \to A}$
となります。
これは、推移を表す行列を用いて
$$
\bordermatrix{     & \cr
               & m_{A\to A}&m_{B\to A}& m_{C\to A}& m_{D\to A}& m_{E\to A}\cr
               & m_{A\to B}&m_{B\to B}& m_{C\to B}& m_{D\to B}& m_{E\to B}\cr
		& m_{A\to C}&m_{B\to C}& m_{C\to C}& m_{D\to C}& m_{E\to C}\cr
		& m_{A\to D}&m_{B\to D}& m_{C\to D}& m_{D\to D}& m_{E\to D}\cr
		& m_{A\to E}&m_{B\to E}& m_{C\to E}& m_{D\to E}& m_{E\to E}\cr            }
\bordermatrix{  &    \cr
                &f_A(T) \cr 
                &f_B(T)  \cr 
                &f_C(T)  \cr 
                &f_D(T)  \cr 
 		&f_E(T)  \cr 
}
$$
$$
=\bordermatrix{  &    \cr
                & f_A(T+1) & f_B(T+1) & f_C(T+1) & f_D(T+1) & f_E(T+1) \cr 
}
$$
のように表されます。
これは、２章の\textcolor{red}{遺伝的浮動}
\index{いでんてきふどう@遺伝的浮動}のときのやりかたと同じです。

\subsection{空間の移動}
上の例では、個体が存在する位置は帰属する島以外には、特に気にしませんでした。
全員が島の一箇所に存在しているように扱ったとも言えます。
今度は、ある個体が存在する位置にも着目するとします。
簡単のために、１次元空間(直線上)に存在するとします。
そして、ある因子を持っていると、その空間において生活しやすさが異なるために、
より生活しやすい場所に移動するというような場合を考えます。
そして、その因子を持っているときには、より快適な方へ移動したがる傾向があるとします。
そうすると、その因子を持つ集団がある位置に集中的に存在していたとすると、
少しずつ、快適な方向へ移動し、最終的には、快適なところに集結します。
このような空間中での動きも拡散方程式の枠組みです。
集団は、ある場所に集中していましたが、移動を開始します。
移動中は存在範囲が広めになり、ゴールに到着した個体はそこから動かないので、最終的にすべての個体が
ゴールに終結して、動きが止まります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartII-017.eps}
\caption{左のピークに集中していた集団は、時間とともに条件のよい右へと動きます。
分布は左から右に向かって、時刻の遅い場合を表しています
}

 \end{center}
 \label{fig:one}
\end{figure}

\section{熱力学・統計力学・流体力学}
\subsection{時空間、有限と無限}
集団内の因子の頻度が時間が経過するにつれて\textcolor{green}{平衡}\index{へいこう@平衡}
状態に達することや、
因子を持つ個体が空間内を時間とともに移動する様子を見てきました。
\textcolor{red}{拡散方程式}\index{かくさんほうていしき@拡散方程式}も登場しました。
時空間を定める変数の関数として扱ったわけです。
ここで、空間というものが出てきましたが、これはわれわれの住んでいる３次元空間に
限るものではありません。
地球の表面を空間にするならば、それは、球面が空間です。
球面の特徴は、２次元の面としての特徴を持ちつつ、有限な面積をもっているのに、
果てに行き着くということがないことです。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-22.eps}

  %\includegraphics{./Figures/Ch6/yugenmugen/yugenmugen.eps}
\caption{有限な線と無限の線。果てがある線と果てのない線。
上段は果てがあって有限の長さの線。
中段は果てが無く、無限の長さの線。
下段は、果てが無く、有限の長さの線。}
 \end{center}
 \label{fig:one}
\end{figure}
生物個体が存在する時空間だけについて考えることはありません。
多変量のデータは
項目の数だけ次元を持たせることができるので、
非常に高次元な空間に存在します。。
データが作る時空間も、生物個体が生きる時空間も、
時間の軸だけは一方向性の唯一の軸とするのが適当かもしれませんが、
データの時間以外の空間の次元は自在です。
空間をの定め方は様々ですが、定め方によらず共通して注意する点があるとすれば、
次のような点が挙げられます。
空間は\textcolor{green}{有限}\index{ゆうげん@有限}か
\textcolor{green}{無限}\index{むげん@無限}か。
空間は閉じているか閉じていないか。
この２つは似ているようですが、少し違います。
先ほど挙げた地表面という空間は、面積を計算できることからも「有限」です。
しかしながら、どこまでも進んでいくことができて、果てはありません。
ぐるっと回って元に戻ってくるだけです(図6.5)。

何かをモデル化するときには、無限を仮定することが多いです。
数理的取り扱いが簡単になるからです。

例えば、人口の増加について、いくらでも増えられるように仮定することがよくあります。
それは、比較的少数の場合には現実とよく当てはまりますが、ある程度以上になると
増加に制約が出てその影響が無視できないことはよくあります。
化学反応の開始初期には、無限の仮定が当てはまるものの、反応産物の量が増えてくると、
頭打ちになる場合も同様で、\textcolor{green}{飽和}\index{ほうわ@飽和}現象と呼ばれます。
変異による遺伝子多型箇所の増大についても、同じことが言えます。
変異の発生確率は非常に低く、ＤＮＡは非常に長いので、どこも平等に無限に変異が起きると
仮定することがあります。
この仮定が有効な場合も多いですが、実際には、飽和の影響を考えなくてはならない場合も
出てきます。

\subsection{均一・平衡・定常}
時空間の次元の広さについて述べましたが、もう一度、時間も空間も広がりがない状態に話しを
戻します。
\textcolor{green}{物理学}\index{ぶつりがく@物理学}や\textcolor{green}{熱力学}\index{ねつりきがく@熱力学}
的な視点で考えます。

時間も空間も広がりを持たない世界では、
すべてのものは同じ場所に存在し、すべてのことは一瞬で起きるのと同じことです。
ＨＷＥを考える際に、すべての染色体はお互いに同じ確率で出会って、
ペアを作ると仮定しましたが、
これは、すべてのものが同じ場所に存在しているとみなしたともいえます。
また、平衡状態に達するには時間がかかりましたが、すべてのことが一瞬で起きるとすれば、
一瞬で平衡状態に達しますから、ＨＷＥを考えたときは、
時間もないものとしたと考えることもできます。

時間には広がりを持たせないままで、空間に広がりを持たせるとします。
そうすると、ものが空間に分布します。
分布すると濃い場所と薄い場所ができます。
濃度の分布や、密度の分布です。
非常に多くの構成要素があって、それに切れ目はないけれども、空間的に偏りがあるようなものとして、
\textcolor{green}{連続体}\index{れんぞくたい@連続体}という考え方があります。
物理・化学では、\textcolor{green}{流体}\index{りゅうたい@流体}という捉え方があります。
流体には気体、液体の両方が含まれますが、
液体は気体よりも、個々の分子の動きが制限されているので、
液体をイメージするほうがよいです。
このように、空間に広がりを持たせて解釈するときには、
連続体とか流体とかに関する概念やモデルを用いることが
出来ることがわかります。
この視点から、先ほどの空間の広がりのない状態を見直すと、これは、
\textcolor{green}{質点系}\index{しつてんけい@質点系}
で考えていたことに相当することがわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/2-23.eps}


  %\includegraphics{./Figures/Ch6/ryuutai/ryuutai.eps}
\caption{点には大きさがなく、内部構造もありません。
連続体には広がりがあります。内部は平衡に達することもありますが、外部との
交渉により内部にばらつきを持った状態で安定することもあります。}
 \end{center}
 \label{fig:one}
\end{figure}
しかしながら、時間の広がりがないとすると、すべてのことは
一瞬で起きてしまいますから、安定した状態に達するのも一瞬です。
ですから、このような時空間では、ある一定の状態をとります。
特に条件がなければ、内部は均一になるでしょう。
構成要素自体は絶えず変化しているかもしれませんが、全体でみたとき、変化がないように
見える状態のことで、\textcolor{green}{平衡}\index{へいこう@平衡}状態と呼びます。
もし、ある塊のある場所を熱し、それ以外のところで放熱すると、
いつしか、熱している近くは熱く、そこから遠いところはそれほどでもない状態に
落ち着くでしょう。
これも、落ち着いた一定状態です。
外と何かしらのやりとりをした常態での安定状態で、平衡状態とは区別して
\textcolor{green}{定常}\index{ていじょう@定常}状態と
呼ぶこともあります。
これは、時間に関して不変である場合ですが、もう少し、
時間について大きな目で見ることもできます。
個々の構成要素は変化しながらも、全体として時間的に無変化な状態は、
生命現象の一部しか説明しません。
個体に関して言えば、日周期で起きる現象もあれば、年単位での変化もあります。
そのほかにも、長短さまざまな
周期で変化する現象も多いです。
これらについては、周期を持って、一定の状態変化を繰り返しているわけですが、
周期的変化という安定状態に
ありますから、時間を周期単位まで大きくしてやれば、定常状態です。
これらの考え方は、熱力学の枠組みですが、
熱力学の考え方は個々の区別をせずに全体を捉えるときの道具立てです。
熱力学に個々の要素への考慮をして拡大したのが\textcolor{green}{統計力学}\index{とうけいりきがく@統計力学}です。
また、安定した状態に焦点をあててきましたが、
生物現象は、逆に、思いもよらない状態変化を対象にすることもあります。
熱力学・統計力学では、\textcolor{green}{非線形熱力学}\index{ひせんけいねつりきがく@
非線形熱力学}
\textcolor{green}{カオス}\index{カオス@カオス}理論などが、不安定な変化を理論だてており、それらも、
生命現象の取り扱いにあっては道具立てとして有効です。

\part{サンプルの集まりの特徴づけ}
\chapter{尺度・変数・自由度・次元}
サンプルについてデータ型を定めてデータレコード(レコード)を収集することとします。
サンプルを個別に扱うか、集団として扱うかという視点はあるにしろ、
レコードはたくさんありますから、これをどうやって把握するかという観点で考慮する項目を扱います。

\section{データをかいつまんで伝える}
\subsection{分割表の情報をかいつまんで伝える}
\subsubsection{情報を伝えるために必要な数値の数}
今、ある医療機関で治療薬の選択に関する集計をとることになったとします。
ある病気に対して、３種類の薬($x_1,x_2,x_3$)を処方することが可能であり、この医療機関では
２人の医師($d_1,d_2$)が診療をしています。
集計の結果、以下の表表のような結果だったそうです。\\

\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$& 計\\ \hline
$d_1$&25&23&12&60 \\ \hline
$d_2$&15&17&8&40 \\ \hline
計&40&40&20&100\\ \hline
\end{tabular}\\

この結果を報告する方法について考えてみます。
２人の医師が処方している人数を薬別に６個の数値(25,23,12,15,17,8)を用いて
\\
「$x_1,x_2,x_3$を$d_1$医師は25,23,12、$d_2$医師は15,17,8人ずつ処方していました」
\\
こんな風に報告することができます。

別のやり方でも報告してみます。
テーブルで書けばこんな具合です。

\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$&計 \\ \hline
$d_1$&■&■&■&60 \\ \hline
$d_2$&■&■&■&40 \\ \hline
計& 40 & 40 & 20 &100\\ \hline
\end{tabular}\\

医師別・薬別の細かい内訳はともかく、医師別の人数、
薬ごとの人数についての報告になっています。
これは\textcolor{red}{分割表}
\index{ぶんかつひょう@分割表}の
\textcolor{green}{周辺度数}\index{しゅうへんどすう@周辺度数}を報告したことになります。\\
今、これだけを報告されたときには、医師と薬とには特に関係がないと考えて、\\

\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$&計 \\ \hline
$d_1$&$60 \times 40 /100 =24 $&$ 60 \times 40 / 100 =24$&$ 60 \times 20 /100 =12$& 60 \\ \hline
$d_2$&$40 \times 40/100=16$&$40 \times 40/100=16$&$40 \times 20/100 = 8$&40  \\ \hline
計&40 &40 &20 &100\\ \hline
\end{tabular}\\
だと思うこととします。これを(無関係・
\textcolor{red}{独立}\index{どくりつ@独立}の仮定の下で)期待する、と言います。
これが\textcolor{green}{期待値}\index{きたいち@期待値}の表です。
医師別・薬別の内訳を詳しく報告することにすれば、
この期待値からのずれを使って、次の様に6個の\textcolor{green}{セル}\index{セル@セル}のうち、
2個のセルについて、期待値との差を報告すると、伏せてあるセルの値も計算によって知ることが
できます。
この方法を使えば、すべてのセルの値を知らせることができます。
実際には、次の表で伏せていない数値だけを伝えればよいので
６個の値がすべての情報を伝えていることになります。

\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$&計 \\ \hline
$d_1$&+1&-1&■&60\\ \hline
$d_2$&■&■&■&■ \\ \hline
計&40&40&■&100\\ \hline
\end{tabular}

\subsubsection{変数セットの間の関係}
６個の数値を用いた２つの報告方法の数値の間の関係は次のようになります。\\


\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$&計 \\ \hline
$d_1$&$\delta_{11}$&$\delta_{12}$&■&$n_{1.}$\\ \hline
$d_2$&■&■&■&■ \\ \hline
計&$n_{.1}$&$n_{.2}$&■&$n_{..}$\\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$ \\ \hline
$d_1$&$n_{11}$&$n_{12}$&$n_{13}$ \\ \hline
$d_2$&$n_{21}$&$n_{22}$&$n_{23}$ \\ \hline
\end{tabular}
\\
\begin{align*}
&n_{..}=\sum_{i=1}^2 \sum_{j=1}^3 n_{ij}\\
&n_{1.}=\sum_{j=1}^3 n_{1j},
n_{.1}=\sum_{i=1}^2 n_{i1},
n_{.2}=\sum_{i=1}^2 n_{i2}\\
&\delta_{11}=n_{11}- \frac{ n_{1.}\times n_{.1}}{n_{..}},
\delta_{12}=n_{12}-  \frac{ n_{1.}\times n_{.2}}{n_{..}}
\end{align*}

２つの報告の仕方それぞれが６変数を持ち、それらが、６個の連立方程式で
関係付けられていることが見て取れます。
つまり変数の付け替え作業をしているだけなわけです。

\subsubsection{大まかに伝えるか、細かく伝えるか}
分割表の情報をすべて伝えるとすれば、変数セットのとり方によらず、
必要な変数の数は同じでした。
さて、本当に「すべて」を伝えることがよいことでしょうか。
今、観察表は期待値表とほとんど一緒です。
こんな場合には、
\\
「全部で100人処方され、$x_1,x_2,x_3$の処方内訳は、40,40,■で、$d_1,d_2$の担当内訳は60,■で、
期待値表と大差ありませんでした」
\\
と報告してもかなり正確な報告になります。
６個の変数のすべてを報告する代わりに、
意味の大きな４個の変数だけで報告をまとめることの方が適切です。

変数セットを変えることは、
少ない変数で情報の大事な部分を取り出すための工夫であるということがわかります。

\subsubsection{大まかな報告の定量化と自由度}
さて、「大差ありませんでした」という報告の言葉に着目します。

「大差(ない)」とか「大いに違(う)」という言葉は人によって感じ方が違うかもしれないので、
この部分を誤解のないような数値に変えることを考えます。
期待値表と観測の表とがどれくらい違うかを数値で表しましょう。
それぞれの表は６個の数値でできていますので、それを使った「\textcolor{red}{距離}\index{きょり@距離}」を
使うとよさそうです。
その違いは$\delta_{11},\delta_{12}$の２変数のとり方で決まるので、
距離は２変数の関数です。
変数の数が多ければ多いほど距離は長くなる機会が増えるので、
距離が長いか短いかの判断はいくつの変数に由来するかを勘案して決めるのが適当です。
今、距離として\textcolor{red}{カイ自乗統計量}\index{かいじじょうとうけいりょう@
カイ自乗統計量}と呼ばれる値を用いることにすれば、
その値は変数の数に応じて評価してやることが適当だと
言うことになります。
これが、カイ自乗統計量の大小を評価するときに、
変数の数(\textcolor{red}{自由度}\index{じゆうど@自由度})を考慮して
\textcolor{red}{ｐ値}\index{pち@p値}という変数の数と無関係な\textcolor{red}{指数}
\index{しすう@指数}に
変換する理由です。
Rでこのプロセスを見てみましょう。
\begin{screen}
\begin{Schunk}
\begin{Sinput}
> obtable2 <- matrix(c(25, 23, 12, 15, 17, 8), nrow = 2, byrow = TRUE) # 2x3表を作る
> chisq.test(obtable2) # カイ自乗統計量で「距離」を測り、それに基づいてp値を計算する
\end{Sinput}
\begin{Soutput}
	Pearson's Chi-squared test

data:  obtable2 
X-squared = 0.2083, df = 2, p-value = 0.901 # カイ自乗統計量、自由度、p値
\end{Soutput}
\end{Schunk}
\end{screen}
のようになります。
これらをまとめて言うと、\\
「全部で100人処方され、$x_1,x_2,x_3$の処方内訳は、40,40,■で、$d_1,d_2$の担当内訳は
60,■で、
期待値表と大差ありませんでした($\chi^2=0.2083$,自由度=2,p=0.901)」\\
のように言えることになります。

ここまでの話しは次のようにまとめられます。
\begin{itemize}
\item{２ｘ３分割表では６個の変数がすべての情報を伝えることができること}
\item{総サンプル数と周辺度数のために、４個の変数が必要なこと}
\item{この４個の変数からは期待値表が期待されること}
\item{実際の観察表についての完全な情報を伝えるためには、残りの２個の変数を使えばよいこと}
\item{観察表が期待値表から離れているかどうかは、それぞれの表を報告するときに使う
変数の個数の差に応じて解釈すること}
\item{この変数の個数の差が、検定における自由度に相当すること}
\end{itemize}

\subsubsection{分割表の自由度}
自由に６個の値を使って、２ｘ３表を作るとき、\textcolor{green}{自由度}
\index{じゆうど@自由度}は６です。
では、「２ｘ３表の検定は自由度が２」であるというのは、
どういうことなのでしょうか。
「２ｘ３表で行と列に関して独立性の検定をするときに、その統計量が自由度２である」ことを、短縮して、
「２ｘ３表は自由度２」と言っています。
「行と列に関して」との但し書きがあります。これは、「行と列は与えられたものとする」ということでもあります。
「独立性の検定」との但し書きがあります。これは、「独立の仮定を基準とする」ということでもあります。
したがって、$n_{..},f_{1.},n_{.1},n_{.2}$は与えられたものとして、$\delta_{ij}$について考えよ、ということになります。
そのような状況では、変数は２個ですよ、というのが「自由度２」の意味です。
確かに、$\delta_{11},\delta_{12}$しか自由に変えられませんから、自由度は２です。

一般的に、
$N\times M$分割表について独立性の検定をするときの自由度は$(N-1)\times (M-1)$です。


\subsection{量的データをかいつまむ}
\subsubsection{ばらつきを使って情報を伝える}
薬$x_1,x_2,x_3$の服用者が$40,40,20$人ずついました。

それぞれが病気の重さを表す検査を受けたところ、図7.1の左に示すような分布を取ったと言います。
この結果を報告することを考えます。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-002.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-003.eps}
\caption{３種類の薬$x_1,x_2,x_3$と検査結果}

 \end{center}

\end{figure}

全100人の検査結果を順番に伝えることもできるでしょうし、
少しかいつまんで、
全員の平均値と99人の検査結果を伝えても良いでしょう。
100個の値ですべてを伝えることができます。
100人目の検査結果は平均値と99人の結果から逆算できますから伝えるには及びません。

図7.1右のように、３群に差があるときには、
３種類の薬ごとに平均を出して、その差を伝える方が、優れたかいつまみ方です。
全員の平均($m_w$)がわかってるとき、３種類の薬の服用人数($n_{1},n_{2},n_{3}$)と
３種類の薬のうち２種類の平均($m_{x1},m_{x2}$)がわかっていれば、
３番目の平均$m_{x3}$は逆算可能です。
また、それぞれの薬の服用者のうち、1人の値は残りの服用者の値を用いて逆算できるので、\\
$1(全平均)+2(2種類の薬の平均)+(第1薬の人数-1)+(第2薬の人数-1)+(第3薬の人数-1)=全人数$\\
やはり、全人数分の値を使って報告することができます。
全体の自由度が100と大きくなりましたが、考え方は、分割表のときと同じです。

\subsubsection{群別のまとまりのよさ}
さて、分割表のときにも問題にした通り、100個の変数を使って、完全な報告をするのか、
たいした違いがないときには、その分を省略して報告するのかについて考えて見ます。
３種類の薬で検査値に違いがないと考えるか、違いがあると考えるかは
それを判断するための数値(検定統計量)
とそれを評価するための自由度とそれらを使って出すｐ値を揃えるのが良いです。

2人の医師の処方の違いのありなしの判断にあたって、
観測表と期待値表との距離(カイ自乗統計量)と
それを評価するための自由度と、ｐ値で判断したのと同じことです。
今回は、ばらつきに着目します。

図7.1の右のような分布だと、薬別に報告することが適当だろうと思われます。
この場合に、群ごとの平均を報告するのが適当だと感じられるのは、
各群のまとまりがよいからです。
すべてのサンプルについて、群平均との差を大小を基準に、群別報告をするかしないかを
決めることを考えます。
まとまりをどうやって数値にするかを考えます。\\
すべてのサンプルの全平均$m_w$を中心にした2次\textcolor{red}{モーメント}
\index{モーメント@モーメント}は\textcolor{green}{全平方和}\index{ぜんへいほうわ@全平方和}
(SSw)と呼ばれ
\begin{equation*}
SSw=\frac{1}{n_{.}}\sum_{i,j} (v_{ij}-m_w)^2
\end{equation*}
です。
群ごとのまとまりのよさは\textcolor{green}{群内平方和}\index{ぐんないへいほうわ@群内平方和}
(SSi)と呼ばれ、群平均$m_{x_i}$を中心とした2次モーメントを全群について足し併せたもので、
\begin{equation*}
SSi=\sum_{i=1}^3 \sum_{j=1}^{n_{i}} (v_{ij}-m_{x_i})^2
\end{equation*}
で表されます。
群の平均がばらつく具合は、\textcolor{green}{群間平方和}\index{ぐんかんへいほうわ@群間平方和}
(SSw)と呼ばれ
\begin{equation*}
SSw=\sum_{i=1}^3 n_i \times (m_{x_i}-m_w)^2
\end{equation*}
と表されます。
実は、
\begin{equation*}
SSw=SSi+SSb
\end{equation*}
となっています。\\
データセットが与えられると、全平方和が決まります。
このときに、群内平方和が小さめで群間平方和が大きめなときに、
群ごとのまとまりがよいと感じますから、
\begin{equation*}
\frac{SSi}{SSw},\frac{SSb}{SSw}, \frac{SSb}{SSi}
\end{equation*}
などの大小を用いて評価することができるはずです。

実際には、サンプルの数や、群の数によらずに検定ができるようにさらに工夫がされています。
群に関する自由度$df_b$=群数-1、
群ごとのサンプルの自由度の和$df_i=\sum_{i}^3 (n_i-1)$で補正をして 
\begin{equation*}
F=\frac{\frac{SSb}{df_b}}{\frac{SSi}{df_i}}
\end{equation*}
という値が使われます。
この\textcolor{green}{$F$}\index{F@F}は、全平方和を群間平方和と群内平方和への分配割合と、
全体の自由度の大きさとその分配具合で決まる値です。
この$F$を算出して、群別の平均値を
報告するかしないかに目安を与える検定を(1次元配置)\textcolor{green}{分散分析}
\index{ぶんさんぶんせき@分散分析}と呼びますが、
それは、$\frac{SSb}{df_b},\frac{SSi}{df_i}$が、それぞれ、群間・群内分散の
\textcolor{green}{不偏推定量}\index{ふへんすいていりょう@不偏推定量}
\footnote{
分散の不偏推定量。
今、ある分布があって、平均がm、分散がvだったとします。
そこから、N個をサンプリングして標本データが得られたとします。
標本平均と標本平均がモーメントとして計算できます。
標本分散を計算するときに、もとの分布の平均mを中心とした
２次モーメントがvなのですが、mの代わりに
標本平均を中心とした２次モーメントを計算すると、vよりも小さい値になります。
そんなときに、標本分散を$\frac{N-1}{N}$倍すると、元の分布の分散のよい推定値になります。
これが不偏分散です。
$\frac{SSb}{df_b}$と$\frac{SSi}{df_i}$は群間・群内分散の不偏分散になっています。
}となっているからです。

このFは群間に差が無いとする仮説のもとで、\textcolor{green}{F分布}\index{Fぶんぷ@F分布}
に従うことが知られているので、
このFを算出してそれをF分布に照らして検定することが出来ます。
Rでは以下の通りです。

\begin{lstlisting}
nx <- c(40, 40, 20); mb<- c(50, 60, 45); sdb <- c(50, 50, 50)*0.01
t1 <- rnorm(nx[1], mb[1], sdb[1]); t2 <- rnorm(nx[2], mb[2], sdb[2]); t3 <- rnorm(nx[3], mb[3], sdb[3])
tb <- c(t1, t2, t3)
x <- c(rep("x1", nx[1]), rep("x2", nx[2]), rep("x3", nx[3]))
boxplot(tb ~ y)
\end{lstlisting}
\begin{screen}
\begin{Schunk}
\begin{Sinput}
> summary(aov(tb ~ x)) 
\end{Sinput}
\begin{Soutput}
            Df Sum Sq Mean Sq F value    Pr(>F)    
x            2 3587.6 1793.80  7237.6 < 2.2e-16 ***
Residuals   97   24.0    0.25                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
\end{Soutput}
\end{Schunk}
\end{screen}
この出力は次の様な２行５列の表になっており、１行目は５列、２行目は３列のみに
値が入ります。

\begin{tabular}[htb]{|c|c|c|c|c|c|} \hline
　&Df(自由度)&Sum Sq(平方和)&Mean Sq(平均平方)&F value(F値)&Pr(>F) (p値) \\ \hline
x(群間)&　&　&　&　&　\\ \hline
Residuals(群内)&  & & &値なし &値なし  \\ \hline
\end{tabular}
\\
群間と群内の自由度、平方和、Ｆ値、Ｐ値が
記されています。。
自由度が２行分(ｘ(群間)：２、Residuals(群内)：97)、出力されています。

\section{次元と独立と直交}
\subsection{自由度と次元}
自由度は変数の数の差であることがわかりました。
今、k個の変数でデータを考えているとします。
自由度を構成するk個の変数に１つずつ軸を割り振ることで、データはk次元座標上の
１点に対応付けることができます。
たとえば、２ｘ２分割表は自由度が１ですので、変数$x$を用いて、次のように表すことができます。
\begin{screen}
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a& 計\\ \hline
B&x&y=20-x&20 \\ \hline
b&z=25-x&w=15+x&40 \\ \hline
計&25&35&60\\ \hline
\end{tabular}\\
\end{screen}
$x$は0から20までの値をとることができますので、それぞれの値について、
\textcolor{red}{ピアソンの独立性検定}
\index{ピアソンのどくりつせいけんてい@ピアソンの独立性検定}の
ｐ値、
\textcolor{red}{フィッシャーの正確確率検定}\index{フィッシャーのせいかくかくりつけんてい@
フィッシャーの正確確率検定}ｐ値を
計算してプロットしてみます。
横軸が\textcolor{green}{自由度}\index{じゆうど@自由度}１、
\textcolor{green}{次元}\index{じげん@次元}１の空間です。(図7.2)
\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIII-005.eps}
\caption{横軸にxの値が0から20まで、縦軸に2方法の検定ｐ値。濃い色(赤)がピアソンの
独立性の検定、薄い色(灰色)が正確検定ｐ値}

 \end{center}

\end{figure}


同様に$2\times 3$分割表を作ってみると、
自由度が２なので、期待値$e_{ij}$と$\delta_{11},\delta_{12}$を使って、
観察表を次の様に表すことができます。\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
&AA&Aa&aa\\ \hline
ケース &$e_{11}+\delta_1$&$e_{12}+\delta_2$&$e_{13}-(\delta_{11}+\delta_{12})$&60\\ \hline
コントロール &$e_{21}-\delta_1$&$e_{22}-\delta_2$&$e_{23}+(\delta_{11}+\delta_{12})$&40\\ \hline
計& 9 & 42 & 49 &100 \\ \hline 
\end{tabular}
2つの変数$\delta_1,\delta_2$
を縦軸と横軸にとれば、観察表は2次元平面の点として表現できます。
このような表に関する検定が２次元平面に表現されている例が、13章(図13.1)に出ています。
このように、自由度を次元として扱うことが出来ます。

\subsection{分割表の自由度と線形独立 行列}
自由度を行列演算で考えてみます。
$2\times 3$ 分割表に戻ります。

周辺度数($R,S,t_1,t_2,t_3,T$)がわかっているとき、$2\times 3$個のセルの値を６変数で表します。
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$x_1$&$x_2$&$x_3$& 計\\ \hline
$d_1$&$r_1$&$r_2$&$r_3$&R \\ \hline
$d_2$&$s_1$&$s_2$&$s_3$&S \\ \hline
計&$t_1$&$t_2$&$t_3$&T\\ \hline
\end{tabular}\\

分割表の定義から
$$
\bordermatrix{ &\cr
   		&1 &1 &1 &0 &0 &0 \cr
   		&0 &0 &0 &1 &1 &1 \cr
		&1 &0 &0 &1 &0 &0 \cr
		&0 &1 &0 &0 &1 &0 \cr
		&0 &0 &1 &0 &0 &1 \cr
		&1 &1 &1 &1 &1 &1 \cr
       }
\bordermatrix{&\cr
		&r_1\cr 
		&r_2\cr 
		&r_3\cr 
		&s_1\cr 
		&s_2\cr 
		&s_3\cr 
}
=
\bordermatrix{    &\cr
		&R\cr 
                &S\cr 
                &t_1\cr 
		&t_2\cr 
                &t_3\cr 
		&T\cr 
}
$$
と書き表せる\textcolor{green}{行列}
\index{ぎょうれつ@行列}の積に関する\textcolor{green}{連立方程式}
\index{れんりつほうていしき@連立方程式}が成り立ちます。

連立方程式を解くにあたり、きれいに解けて、変数の値が全部決まるか、
値が定められないかは、行列の用語で言えば\textcolor{green}{一次独立}
\index{いちじどくりつ@一次独立}であるかないか、ということになります。

一次独立かどうかの判定では、
行列の\textcolor{green}{ランク}\index{ランク@ランク}(\textcolor{green}{階数}\index{かいすう@階数})
\footnote{
行列のランクとは、列ベクトルの一次独立なものの最大個数

}というものを使って表します。
ランクと変数の数が等しければ、すべての変数の値が決まり、ランクよりも
変数の数が多ければ、「値が定まらない＝自由」な変数であることになります。
これを使って自由度を求めれば、「自由度＝変数の数-ランク」です。
Rでは次の様にして、算出します(周辺度数だけを条件にすれば、自由度は２、
(1,1)と(1,2)のセルの値を与えれば、自由度は0になります)。
\begin{lstlisting}
m<-matrix(c(1,1,1,0,0,0,
		0,0,0,1,1,1,
		1,0,0,1,0,0,
		0,1,0,0,1,0,
		0,0,1,0,0,1,
		1,1,1,1,1,1, # 以上６行が周辺度数の制約条件に対応する
		1,0,0,0,0,0, # (1,1)セルに値を与える式に対応する行
		0,1,0,0,0,0), # (1,2)セルに値を与える式に対応する行
		ncol=6,byrow=TRUE)
# qr() はランクを求める関数
6-qr(m[1:6,])$rank # 第1-6行のみを用いると自由度は2になる
6-qr(m[1:8,])$rank # 第1-8行のすべてを用いると自由度は0になる
\end{lstlisting}
\subsection{確率的独立と直交}
自由度と変数の数と線形独立の話しをしてきました。
\textcolor{green}{独立}\index{どくりつ@独立}は確率の世界にもあります。
確率の世界での独立は、ある
\textcolor{red}{事象}\index{じしょう@事象}とある事象とが相互に独立に起きるとき、
その2つの事象が独立であるというのは、
両方が同時に生起する確率が、それぞれの確率の積で表わされることを言います。

今、相互に独立な事象Ａと事象Ｂの起きる確率$P(A),P(B)$と、両方の事象が
同時に起きる確率$P(A)\times P(B)$との関係は、図で表すように、
２つの大きな長方形の重なり部分になります。
２つの事象が独立であるときには、それぞれの事象に対応する軸を
\textcolor{red}{直交}\index{ちょっこう@直交}させることに相当します。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/3-1.eps}


  %\includegraphics{./Figures/Ch6/ProbRight/ProbRight.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

別の見方をします。
今、２つの相互に独立な連続型のデータを観測します。
２セットのデータの内積を使って、２ベクトルの成す角度が
約 $\frac{\pi}{2}$であること(直交している)ことが
確かめられます。
\begin{lstlisting}
Ns <- 10000; A <- rnorm(Ns); B <- rnorm(Ns) # 独立な値のNsペア
ip<-sum(A*B) # 内積
cos<- sum(A * B)/sqrt(sum(A * A) * sum(B * B)) #余弦
acos(cos)/pi # 角度(単位は$\pi$)。
\end{lstlisting}
このようにして算出した値が約0.5となり、
２つの独立な事象のデータベクトルが直交していることがわかります。
4章(4.2.5)でデータの似ている程度を角度で表すことを話したことと同じです。
また、複数の検定を実施するときに、検定相互の独立・依存関係の考慮が必要なことは、
13章(13.4.1)で扱います。

\subsection{線形独立と直交基底}
確率で考えると、独立とは直交と関係していましたが、
行列では独立と言えば、\textcolor{red}{線形独立}\index{せんけいどくりつ@線形独立}
のことで、\textcolor{red}{直交}\index{ちょっこう@直交}しているというわけではありません。
k次元のデータを説明するために必要な
最小のベクトルの数はk本であり、そのような
k本のベクトルが満足するべき条件が、
「k本のベクトルが線形独立である」ことです。

このようなk本のベクトルは
k次元空間を張る\textcolor{green}{基底}\index{きてい@基底}と呼ばれますが、
この基底の各ベクトルの長さや、ベクトル間の角度に制約はありません。
しかしながら、基底の構成ベクトル同士がなす角を揃えると、
対称となり、対称であることは、価値があることがあるので、
そのようにして取る基底を\textcolor{green}{直交基底}\index{ちょっこうきてい@直交基底}
と呼びます。
また、基底の構成ベクトルの長さが均一なことも、それだけで意味があるので、
そのような基底には名前があって、\textcolor{green}{正規直行基底}\index{せいきちょっこうきてい@
正規直交基底}と呼ばれます。

多次元のデータがあるときに、データのばらつきがよく見えるような正規直行基底を取り出すことがあります。
\textcolor{red}{特異値分解}
\index{とくいちぶんかい@特異値分解}や
\textcolor{red}{固有値分解}\index{こゆうちぶんかい@固有値分解}
(\textcolor{red}{主成分分析}\index{しゅせいぶんぶんせき@主成分分析})と呼ばれる手法です。

これらをＲで試してみることにします。

\subsection{正規直行基底を取り出す 固有値分解}
複数のグループに分けられる標本が多数あって、
それについて、比較的多くの量的項目についてデータを
とる状況を考えます。

データの作成方法とその処理は以下の通りです。
\begin{lstlisting}
#偏った集団構成(100人規模の亜集団４つと10人規模の亜集団を20個)で
#100項目のデータを作成
Nm<-100 #項目数
Ns<-c(rpois(4,100),rpois(20,10)) # 亜集団人数発生
Npop<-length(Ns) #亜集団数
M<-NULL #全ジェノタイプデータを納める行列
#亜集団別に平均を振ってシミュレーション
for(j in 1:Npop){
 tmpM<-matrix(rep(0,Nm*Ns[j]),ncol=Nm)
 for(i in 1:Nm){
  af<-rnorm(1)
  tmpM[,i]<-rnorm(Ns[j],af)
 }
 #全データ行列に格納
 M<-rbind(M,tmpM)
}
# データを標準化
wholemean<-mean(M)
M<-M-wholemean
mu<-apply(M,2,mean)
M<-t(t(M)-mu)
# 固有値分解
svdout<-svd(M)
M2<-svdout$u%*%diag(svdout$d) # 分解後データ行列
par(mfcol=c(1,2))
# 固有値分解前後をimage()プロット
image(1:sum(Ns),1:Nm,M,xlab="サンプル(大集団→小集団)",ylab="項目")
image(1:sum(Ns),1:Nm,M2,xlab="サンプル(大集団→小集団)",ylab="PCA後eigen項目")
df1<-as.data.frame(M);df2<-as.data.frame(M2) # データフレーム化
L<-1:5;par(mfcol=c(1,1))
plot(df1[,L]) # ５軸がなす軸ペアでサンプルをプロット。分離しない
plot(df2[,L]) # 固有値分解後に分離力のあるトップ５軸でのプロットは分離する
vM1<-apply(M,2,var)
vM2<-apply(M2,2,var)
ylim<-c(min(vM1,vM2),max(vM1,vM2))
# 固有値分解前の各項目の分散はどれも同じ程度だが
# 固有値分解には、分散の大きいものと小さいものとのコントラストが大きくなっている
plot(vM1,ylim=ylim,type="b")
par(new=T)
plot(vM2,ylim=ylim,type="b",col="red")
\end{lstlisting}


項目数の次元空間に標本がばらついて存在していることになります。
まず、標本には構成標本の多いグループが４つと、構成標本の少ないグループが２０個で、
観察項目１００個でデータをシミュレーションしています。
観察データそのものを、Rのimage()関数で表示すると、横軸に集団ごとのある程度のまとまりは
見えますが、まとまりがよいとは言えません(図7.4左)。
これに固有値分解を実行します。
固有値分解を施すと、正規直行基底が取り出されます。
直交基底なので、データ全体の分散は、軸ごとの分散の和になるわけですが、
このとき、できるだけ軸ごとの分散にコントラストが出るように基底をとります。

実際、図7.4右では固有値分解によってとりだされた少数の軸が
下辺に集中させてありますが、この軸によって大半のばらつきが説明されています。
それはimage()図の少数の下部では濃淡コントラストが大きいのに較べ、
それ以外は濃淡がほぼ均一であることからわかります。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIII-012-2.eps}
\caption{左図では、左3分の2くらいが粗い濃淡となり、右3分の1くらいが
細かい濃淡となっています。
右図では、それを固有値分解することで、各列について、濃淡のばらつきが下辺に
集中してきています}

 \end{center}
 \label{fig:one}
\end{figure}
図7.5を見てください。
固有値分解の前に、適当な５つの項目を選んで、それらを縦横軸に取ってサンプルをプロットしても
サンプルが群に分かれているようには見えませんが、
固有値分解実施後に、分離力のあるトップ５軸を取り出してで同様にプロットした方では、
サンプルが分離されているのがわかります。
ここでは、４つの大きなグループと多くの小さなグループを想定してシミュレーションした
データを使っているので、第３軸までが効果的であることもわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-013.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-014.eps}
\caption{左が固有値分解前、右が後。サンプルがクラスタリングしていることがわかる}

 \end{center}
 \label{fig:one}
\end{figure}

特異値分解前の項目の分散と分解後の軸ごとの分散をプロットすると図7.6のようになります。
分解前は、どの項目も似たり寄ったりの分散を持っていますが、
分解後の項目の分散は、大きいものからだんだんに小さくなっています。
これは、ばらつきが大きくなる軸を第１軸とし、
残ったはらつきをなるべく大きく取れるような軸を第２軸としてとるように
正規直交系を取り出しているからです。

固有値分解では、サンプルのばらつきを大きく説明する軸から選び出していき、最終的に、
項目数と同じだけの軸が得られますが、その新たな軸が説明するばらつきは、大きいほうから降順に並びます。
その様子を見てみます。
分解の前の場合は、すべての軸が似たり寄ったりの分散しかもっていないのに対して、
分解が選んだ軸は、大から小へと変化しています。
なお、それぞれの軸の取り方で、軸別の分散の振り分け状態は違いますが、
全部の軸について分散を足し合わせれば、一致します。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-015.eps}
\caption{固有値分解前の軸ごとの分散(黒)と分解後の分散(赤)}

 \end{center}
 \label{fig:one}
\end{figure}






\section{変数の構造と意味}

\subsection{変数の構造}
自由度の数だけある変数で、データのすべてを表現すること、
また、かいつまんで、少数の変数だけで表現することについて考えてきました。
また、変数のセットは、線形変換を施すことによって、
使いやすくできることも見てきました。

変数の取り方は、どのように取ってもよいのですが、よくやるやり方というものがあります。
\begin{itemize}
\item すべてを平等に扱う方法
\item 順序をつけて並べる方法
\item 階層化する方法
\end{itemize}
がよく使われます

Ｎカテゴリの比率を扱うとします。
Nカテゴリの占める比率をN個の数値で表現するときには、
すべてのカテゴリが平等に扱われています。
自由度がＮ−１です。
この方法の良いところは、興味の対象に直属する性質そのものを対象としていることです。
一方、Ｎ個の変数のすべてが、平等なので、
少ない変数でかいつまんで全体を捉えることが難しいです。

階層化する方法というのは、全体をだんだんに小分けにしていく方法です。
\textcolor{red}{分岐木}\index{ぶんきぎ@分岐木}的な構造です。
大づかみにしたいときには、木の根元寄りの変数だけでかいつまめば、
データ全体の様子を大づかみにすることが可能です。
分布を\textcolor{red}{平均}\index{へいきん@平均}、\textcolor{red}{分散}
\index{ぶんさん@分散}で捉えるのは、１次からだんだんと高次になる階層のうちの、
１次と２次の\textcolor{red}{モーメント}\index{モーメント@モーメント}で捉えることです。

全体の分散を、\textcolor{red}{分散共分散行列}
\index{ぶんさんきょうぶんさんぎょうれつ@分散共分散行列}の成分に分けるのも、同じ発想です。
\textcolor{red}{分散分析}\index{ぶんさんぶんせき@分散分析}も
\textcolor{red}{群内分散}\index{ぐんないぶんさん@群内分散}と
\textcolor{red}{群間分散}\index{ぐんかんぶんさん@群間分散}への分解であって、階層化処理をしています。
また、集団の分集団構造を\textcolor{green}{$F_{ST}$}
\index{$F_{ST}$@$F_{ST}$}を含む、複数の$F$に分解するのも、階層的な処理です。

\subsection{意味から選ぶ変数　データ構造で決める変数}
変数のセットによって、データに関して理解するとき、
変数が何を表しているのかは重要です。
ですから、変数のセットを選ぶときに、何かしら意味のあるものに
変数を割り当てるというやり方があります。
Ｎカテゴリのそれぞれの頻度に変数を割り当てる、というのも、「あるカテゴリの頻度」
という意味があるので、その値の解釈に迷うことはありません。
また、遺伝・生命現象を説明する数理モデルを立てて、データとのあてはまりを
検討することも可能です。
この場合の変数も、モデルの中で意味を持っていますから、値の
解釈は容易です。そのモデルを構成する変数を選ぶ方法です。
変異率や組み換え率を変数化してモデルを立てたり、
因子のリスクの大きさを変数化するなどがこの方法に当たります。

他方、データの塊に対して、その塊を説明しやすい角度から眺めるという方法もあります。
行列型データを特異値分解して、説明力の大きな軸を選りだすのが
これにあたります。
この方法の利点は、相互に直交する複数の軸が見いだされ、
しかも、説明力の大きいものが得られることです。
それらは、直交する因子となっています。
しかしながら、得られた軸(因子)自体には自明な意味がないので、
その軸の生物現象における意味を見出す
必要があります。
\chapter{統計量・指数・確率・尤度}
データの特徴を取り出したら、その取り出した値に
意味を持たせて解釈したくなります。
この章では
値に意味を持たせるための仕組みについて考えます。
\section{確率分布}
\subsection{分布とは}
本書では、ところどころで分布という用語を使って来ました。
ここで分布について整理しましょう。

\textcolor{green}{分布}\index{ぶんぷ@分布}は、離散的にしろ連続的にしろ、順序ありにしろなしにしろ、
取り得る値の範囲が決まっているときに、
その範囲の中のどの値をどのくらいとるかを説明したものです。
いわゆる\textcolor{red}{距離}\index{きょり@距離}
(\textcolor{red}{ユークリッドの距離}\index{ユークリッドきょり@ユークリッド距離})だったら、０以上の実数が範囲です。
さいころの目だったら、１から６までの自然数が範囲です。
ＤＮＡを構成する塩基だったら、Ａ，Ｔ，Ｇ，Ｃの４カテゴリが範囲です。
この範囲を\textcolor{green}{確率空間}\index{かくりつくうかん@確率空間}とも呼びます。

確率空間内の値を\textcolor{green}{確率変数}\index{かくりつへんすう@確率変数}
がとるときに、めったにとらない値があったりしばしばとる値があったりしますが、
その様子を表わしたのが\textcolor{green}{確率分布}\index{かくりつぶんぷ@確率分布}です。
いろんな値をとる可能性がありますが、かならず、確率空間内のどれかの値をとります。
確率空間内の値について、とりやすさの程度を数値で表すことにします。
とる可能性がないときの数値を０とします。
少しでも取る可能性のある値には正の値を与えます。
とりやすさの程度に比例した値を与えることにします。
その上で、範囲内のすべてについてとりやすさの値を足し合わせたら、
それが１になるように補正します。

このように、定まった範囲に定められたとりやすさの分布を持ったものを
確率変数と呼んで、そのとりやすさの分布をその確率変数の確率分布と呼びます。


\subsection{離散的な確率分布}
赤玉４個、白玉６個の入った袋から、１個とりだすときに、
赤玉である\textcolor{red}{確率}\index{かくりつ@確率}が0.4、白玉のそれは0.6です。
確率は合わせて１です。
４色になれば、赤0.3,白0.2,青0.4,黒0.1のようになります。
これを図示すれば
\begin{lstlisting}
barplot(c(0.4,0.6),names.arg=c("赤","白"),ylab="確率")
pie(c(0.3,0.2,0.4,0.1),labels=c("赤","白","青","黒"))
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-017.eps}
\caption{2カテゴリの確率分布が2本の棒で表わされています(左)。
４カテゴリの
確率分布が円グラフで表わされています(右)}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{連続的な場合 指数分布}
これは、確率空間が離散的な場合でした。
次に連続的な確率変数の場合を考えます。
連続的な場合には、数え上げて、足し合わせるわけに行きません。
確率空間が連続的な例として、0以上の実数を確率空間とする統計量の
確率分布を作ってみることにします。
実数直線という１次元空間において、
確率空間は$0\le x \le \infty$で定義されているとします。
この確率変数は
$x=0$のときの確率を基準として、
単位距離$L$増えるごとに\textcolor{red}{生起確率}\index{せいきかくりつ@生起確率}
が半分になるような確率変数を想定すると、次が成り立ちます。
\begin{equation*}
P(x+L)=\frac{1}{2}\times P(x) 
\end{equation*}
$P(x)$は定義された範囲で正なので、両辺の対数をとって、それを$Q(x)$とすれば
\begin{equation*}
Q(x+L)=log(P(x+L))=log(P(x))-log(2) = Q(x)-log(2)
\end{equation*}
ここで、$\lambda=\frac{log(2)}{L}$とすれば
\begin{equation*}
P(x)=P(0)\times e^{-\lambda x} 
\end{equation*}
となります。
\footnote{
少し変形して、
$Q(x+L)-Q(L)=-log(2)$。
ここから$x$の増加に比例して$Q(x)$が変化するので
$Q(x)=Q(0) - \frac{x}{L} log(2)$
となり、これを$P(x)$に戻すと
$log(P(x))=log(P(0))-\frac{x}{L} log(2)$
となって、
$P(x)=P(0)\times exp(-\frac{x}{L}log(2))$
であることがわかります。
}
ここで、確率空間全体について、この$P(x)$を足し合わせると、1になることから
\begin{equation*}
\lim_{X\to \infty} \int_0^X P(x) dx = \lim_{X\to \infty} P(0) \bigl[ -\frac{1}{\lambda} e^{-\lambda x} \bigr] ^X_0 
= P(0) \times \frac{1}{\lambda} =1
\end{equation*}
これを満足する条件から、$P(0)=\lambda$が得られて、結局
\begin{equation*}
P(x)=\lambda e^{-\frac{x}{\lambda}};\lambda=\frac{log(2)}{L}
\end{equation*}
が得られます。
これは、\textcolor{green}{指数分布}\index{しすうぶんぷ@指数分布}と呼ばれる分布の
\textcolor{green}{確率密度関数}
\index{かくりつみつどかんすう@確率密度関数}です。
このように、連続な確率空間では、確率密度が関数で表せることがあり、
確率密度関数と、それの足し合わせである\textcolor{green}{累積分布関数}
\index{るいせきぶんぷかんすう@累積分布関数}との間は、
\textcolor{red}{微分}\index{びぶん@微分}と
\textcolor{red}{積分}\index{せきぶん@積分}とでつながっています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-018.eps}
\caption{指数分布とラプラス分布。ラプラス分布は正負に値を持ち、指数分布は正にのみ値を持つ
指数分布の高さはラプラス分布の高さの2倍になっている}

 \end{center}
 \label{fig:one}
\end{figure}

$P(x)=\lambda e^{-\lambda x}$の式の由来を確認します。
原点から遠ざかる方向にある距離を進むと、生起確率が$\frac{1}{e}$になるような分布でした。
時間のように1方向性の数直線ならばこうなりますが、両方向性の１次元空間の場合には、原点から遠ざかる方向が
２つありますから、確率空間を$-\infty \le x \le \infty$としてやって、\\
$P(x)=\frac{1}{2} \lambda e^{-\lambda |x|}$\\
としてやることにします。$\frac{1}{2}$は１方向性から2方向性への変化、$|x|$と絶対値を取っているのは、
xが負の場合に距離にするためです。
これは2方向性の指数分布ですが、\textcolor{green}{ラプラス分布}
\index{ラプラスぶんぷ@ラプラス分布}とも呼ばれます。
指数分布とその折り返しを含むラプラス分布の両方を重ねて描いた図8.2から、
高さが半分であること、対称性が出ていることが読み取れます。
\subsection{指数分布と正規分布との違い}

さて、原点からの距離がある単位だけ変化すると、生起確率が$\frac{1}{e}$になる場合を
考えましたが、では、原点からの距離の２乗がある単位だけ変化すると、生起確率が$\frac{1}{e}$になる
ような確率変数だったら、どうなるのでしょうか？\\
指数分布・ラプラス分布をちょっと変形して、
\begin{equation*}
P(x)=C e^{-\lambda |x|^2}
\end{equation*}
という形をしていることが予想され、実際、
\begin{equation*}
\int_{0}^{\infty} P(x)=\frac{1}{2}
\end{equation*}
を満足するように、$C$を求めてやると、
\begin{equation*}
P(x)=\frac{1}{\sqrt{\pi}}\lambda^{\frac{1}{2}} e^{-\lambda |x|^2}
\end{equation*}
であることが知られています。
実は、この式は、分布の紹介で最初に出てくることが多い、平均0の\textcolor{green}{正規分布}
\index{せいきぶんぷ@正規分布}の式と同じです。
正規分布のよくある表記法で書き直せば、
\begin{equation*}
P(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{|x|^2}{2\sigma^2}}
\end{equation*}
です。
\begin{equation*}
k=2,\lambda=\frac{1}{2\sigma^2}
\end{equation*}
の関係があります。

\subsection{一様分布・指数分布・正規分布・矩形分布　一般正規分布}
実際、$e^{-\lambda |x|^k}$という関数は、k=1のときに指数分布(ラプラス分布)、
k=2のときに正規分布、
kが0に近くなるとx=0のときだけ1で、他は$e^{-1}$で一様(\textcolor{green}{一様分布}
\index{いちようぶんぷ@一様分布})です。
kが大きくなると、$x<1$で1、x=1では$e^{-1}$、x>1ではf(x)=0であるような、
\textcolor{green}{矩形分布}\index{くけいぶんぷ@矩形分布}です。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-019.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-020.eps}
\caption{一般正規分布。$e^{\lambda |x|^k}$のグラフ(左)と、それを確率密度分布にしたもの(右)。
k=-10,-9,...,0,1,...10}

 \end{center}
 \label{fig:one}
\end{figure}


これらは、一様分布、指数分布(ラプラス分布)、正規分布、矩形分布の確率密度分布は、確率空間での積分が
１になるように補正を施すことで確率密度分布と出来ます。
実際、ラプラス分布$\frac{1}{2}\lambda e^{-\lambda x}$と
正規分布$\frac{1}{\sqrt{\pi}}\lambda^{\frac{1}{2}}e^{-\lambda x^2}$を
よく眺めると、$C \lambda^{\frac{1}{k}}e^{-\lambda x^k}$というように表せそうです。
実際、次のようにすると、確率空間全体での積分が１になることが示されています。
\begin{equation*}
P(x;k)=\frac{1}{2} \frac{1}{\Gamma(\frac{1}{k}+1)} \lambda^{\frac{1}{k}} e^{-\lambda |x|^k}
\end{equation*}
ただし、$\Gamma()$はガンマ関数
\footnote{
ガンマ関数は、ここで示した分布の関数にも登場しますし、$\Gamma(n+1)=n!$
であること、$\Gamma(\frac{1}{2})=\pi$ であるなど、場合の数の計算や、
円・球などとも関係する関数です。
}
と呼ばれる関数です。

これを\textcolor{green}{一般正規分布}\index{いっぱんせいきぶんぷ@一般正規分布}
と呼んだり、\textcolor{green}{一般誤差分布}\index{いっぱんごさぶんぷ@一般誤差分布}
と呼んだりします。
さまざまな対称な分布がこの表記で表現できることがわかりました。
図8,3にあるように、
矩形の場合は、幅が２高さが0.5です。一様分布は高さが０です。
指数分布の折り返しであるラプラス分布は、原点で最高点0.5を持ち、そこが尖っています。
正規分布は、指数分布よりも最高点が高く、頂上が滑らかです。

\subsection{正規分布、カイ分布と次元}
平均0の正規分布の確率密度分布の表記法として
\begin{equation*}
\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2\sigma^2}}
\end{equation*}
がよく用いられるのは、この正規分布の分散が$\sigma^2$であり、分布を分散から
特定できるというメリットがあるからです。
では、平均０、分散１の正規分布(
\textcolor{green}{標準正規分布}\index{ひょうじゅんせいきぶんぷ@
標準正規分布})の式を示しましょう。
$\sigma^2=1$ですから
\begin{equation*}
\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
\end{equation*}
です。
さて、指数分布を考えるときに、原点からの距離について考え始めました。
その上で、1次元の数直線上で負の領域にも話を広げてラプラス分布の式を作りました。
原点を中心に対称にするために、確率密度を半分にしたのでした。

正規分布について、この指数分布からラプラス分布への変化の逆を行います。
今、正規分布は、正負の実数が確率空間ですが、原点からの距離のみに
興味があるとしますと、0以上の範囲だけが確率空間となります。
確率密度は、2倍にすればよいです。
\begin{equation*}
2\times \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}=\frac{1}{\sqrt{\pi}}e^{-\frac{x^2}{2}}
\end{equation*}
です。
これは、自由度１の\textcolor{green}{カイ分布}\index{カイぶんぷ@カイ分布}
と呼ばれる分布の確率密度関数です。
「カイ分布」であって、「カイ自乗分布」ではありません。
「カイ自乗分布」の方が統計的検定でよく使われるほうの分布です。

さて、「自由度１」のカイ分布でした。
自由度１のカイ分布は、１つの変数が自由に動かせるときの分布であって、自由度を１以外にも
変えることができることがすでに示唆されています。
自由度が１なのは、数直線という1次元空間を扱っているからです。
標準正規分布から、自由度１のカイ分布を作ったときに、行ったことは、
数直線上の座標を、「原点からの距離」に変更するための処理でした。
1次元数直線上の座標を距離にするには、同じ距離の点が２つあるので、2倍したのです。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-021.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-022.eps}
\caption{球面。ある点からのユークリッド距離が等しい点の集まり。
1次元球は２点、2次元球は円}

 \end{center}
 \label{fig:one}
\end{figure}


では、自由度が2のときはどうしたらよいのでしょうか？
自由度２ということは、2次元空間です。
２次元空間で、同じ距離の点を集めると、それは、円になります。
何個の点が構成しているかと言っても、数えられません。
その代わりに長さは計算して求めることが出来ます。
\begin{equation*}
2\pi x
\end{equation*}
です。$x$は原点からの距離で\textcolor{green}{半径}\index{はんけい@半径}
です。
では、この一周を構成する点がすべて$\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$の確率で発生しますから、
自由度２のカイ分布の確率密度関数は、
\begin{equation*}
2\pi x \times \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}= \sqrt{2\pi} x e^{-\frac{x^2}{2}}
\end{equation*}
でしょうか。
幸い、この関数の積分は$\frac{d e^{ax^2}}{dx} = 2ax e^{ax^2}$であることを利用すれば、簡単ですから、
実際に、確率空間$0 \le x \le \infty$について積分して求めてみて、それが１になるかどうかで確認が出来ます。
\begin{equation*}
\int_{0}^{\infty} \sqrt{2\pi} x e^{-\frac{x^2}{2}} dx = \sqrt{2\pi} [(-e^{\frac{x^2}{2}})]_0^{\infty}=\sqrt{2\pi}
\end{equation*}
1次元のときに2つ折にした場合と異なり、距離xに応じて、対応する点の数が異なるために、
すべて足し合わせて１になっていません。
しかしながら、距離$x_1$と$x_2$の点を集めた確率の比は、
\begin{equation*}
\sqrt{2\pi} x e^{-\frac{x_1^2}{2}}/  \sqrt{2\pi} x e^{-\frac{x_2^2}{2}}
\end{equation*}
でよいのですから、全体の積分が１になるように、$\sqrt{2\pi}$で補正してやればよいでしょう。
したがって、2次元平面において、原点から遠ざかるにつれて、生起確率が正規分布的に
小さくなっていくようなときに、
距離が等しい点(生起確率が等しい点)の確率を足し合わせたような確率変数の確率密度関数は
\begin{equation*}
\frac{1}{\sqrt{2\pi}} \sqrt{2 \pi} e^{-\frac{x^2}{2}}=e^{-\frac{x^2}{2}}
\end{equation*}
となるはずです。
そして、これは、確かに、自由度kのカイ分布の一般式にk=2をあてはめた式
\begin{equation*}
\frac{2^{1-\frac{k}{2}}}{\Gamma(\frac{k}{2})} x^{k-1} e^{-\frac{x^2}{2}} =e^{-\frac{x^2}{2}}
\end{equation*}
と一致しています。

では、正規分布から、任意の自由度kのカイ分布を作ってみることにしましょう。
1次元のときは、２倍、2次元のときは$2\pi x$倍しました。
これは、1次元と2次元空間の半径xの「円周」です。
一般次元の「円周」(\textcolor{green}{多次元球}
\index{たじげんきゅう@多次元球}\index{きゅう@球}の大きさ)は、
\begin{equation*}
S(x;k)=2\frac{1}{\Gamma(\frac{k}{2})} \pi ^ {\frac{k}{2}} x^{k-1}
\end{equation*}
であることが知られています。

したがって、自由度kのカイ分布の確率密度分布は
\begin{equation*}
S(x;k) \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}=
\frac{1}{\sqrt{2\pi}}\frac{2\pi^{\frac{k}{2}}}{\Gamma(\frac{k}{2})} x^{k-1} e^{-\frac{x^2}{2}} 
\end{equation*}
に比例して、空間全体の積分が１になるような補正を加えられた式になっていることでしょう。\\
実際、自由度kのカイ分布は
\begin{align*}
&C \times S(x;k)\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} = (\frac{1}{\sqrt{2\pi}})^k S(k) e^{-\frac{x^2}{2}}\\
&C=(\frac{1}{2\pi})^{\frac{k-1}{2}}
\end{align*}
と書き表せることがわかります。
これは、$e^{-\frac{x^2}{2}}$が表すように、
原点から、遠ざかると確率が小さくなる分布であって、
その小さくなり方が、正規分布と同じように、$x^2$が一定量、増えると、$\frac{1}{e}$になるような
小さくなり方であるような分布で、次元がkであるような分布は、k次元球の「表面積」$S(x;k)$に関する要素を除けば、
$(\frac{1}{\sqrt{2\pi}})^k $ で補正することで、空間全体の積分が１にできることを示しています。

1次元の正規分布のときに$(\frac{1}{\sqrt{2\pi}}) $が同様に補正項であったことを考えれば、それがk次元に展開されたことを
思えば、納得がいきます。
実際、\textcolor{green}{多変量正規分布}
\index{たへんりょうせいきぶんぷ@多変量正規分布}の確率密度関数でも、補正の項として、$(\frac{1}{\sqrt{2\pi}})^k$が現われるのですが、
同じことです。

\subsection{カイ分布からカイ自乗分布}
今、
正規分布を多次元に展開して、距離ごとに確率を足し合わせることで、
多次元を１変数で取り扱っている
分布としてカイ分布が出てきました。
どちらの分布も原点からの距離xを変数として扱いました。
しかしながら、
これらは、距離の２乗の多寡を気にする分布です。
それが$e^{-\lambda x^2}$を式に含む理由です。
ですが、気になるものが$x^2$なのだったら、いっそのこと、$X=x^2$の$X$、原点からの距離の２乗そのものを
変数として確率分布にしてしまうのも、すっきりしそうです。
そのようにして得られた確率密度分布関数が、
\begin{equation*}
Pr(\chi^2=X)=\frac{1 }{2^{\frac{k}{2}} \Gamma(\frac{k}{2})} X^{\frac{k}{2}-1} e^{-\frac{X}{2}}
\end{equation*}
です。
検定でしばしば登場する
\textcolor{green}{カイ自乗分布}\index{カイじじょうぶんぷ@カイ自乗分布}です。

\subsection{もっとも観察されそうなカイ自乗値}
カイ自乗分布の自由度別の確率密度分布を見てみます。
自由度が1,2,3,4,5の場合を図示します(図8.5)。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-023.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

自由度が１，２のカイ自乗分布では確率密度が最大なのは、距離が０のときですが、
自由度が３以上になると、ある程度距離のあるときに確率密度が最大になっている様子が
見て取れます。

原点からの距離(距離の２乗)が大きくなると、生起確率は小さくなりますが、
自由度が大きくなると、距離が小さい場合よりも、距離がある程度大きい場合の方が、
空間が広い(多次元球の表面積として広い)ので、
もっともとりやすいカイ自乗値が大きいほうにシフトしているのです。

\chapter{確率と尤度}
\section{確率}
ここまで、確率というものがわかったものとして話しを進めてきました。
\textcolor{green}{確率}\index{かくりつ@確率}と\textcolor{green}{尤度}\index{ゆうど@尤度}
というよく似た概念に焦点を当てて整理してみます。

AとBとの２種類にラベルづけされた玉が全部で40個(A 12個、B 28個)入った袋から、
全部で10個の玉を取り出す場合を考えます。
2種類の玉の取り出し方の場合の数は、Aの個数に着目して0から10個の11通りあります。
１個取り出してラベルを確認しては、
取り出した玉を袋に戻して、再度、取り出すという作業を10回、繰り返す場合と
10個を一度に取り出す場合とを考えます。
どちらのやり方でも、一番確率が高いのは、3個がAで7個がBのときです。
もちろん、0個から10個までの場合の確率を足し合わせると1です。

\begin{lstlisting}
# 1個ずつ取り出しては戻すのを10回繰り返しす場合
p <- 0.3; x <- 0:10 # A玉の割合は0.3
pr <- choose(10, x) * p^x * (1 - p)^(10 - x) # choose(a,b)はa個からb個を取り出す場合の数
A <- 12; B <- 28; S <- A + B
x <- 0:10; y <- 10 - x
# 10個を一度に取り出す場合
pr2 <- exp(lgamma(10 + 1) + lgamma(30 + 1) + lgamma(A + 1) + lgamma(B + 1) -  (lgamma(S + 1) + lgamma(x + 1) + lgamma(y + 1) + lgamma(A - x + 1) + lgamma(B -  y + 1)))
ylim <- c(0, 0.5)
plot(x-0.5, pr, ylim = ylim, type = "s")
par(new = T); plot(x - 0.5, pr2, ylim = ylim, type = "s", col = "red")
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-025-2.eps}
\caption{40個中12個がA玉であるときに、10個の玉を取り出してそのうちA玉の個数が
いくつになるかの確率。１個、取り出しては戻すのを10回繰り返したときが黒、
10個を一度に取り出すのが赤}

 \end{center}
\end{figure}

40個の玉がA,B２種類である点は変えずに、その内訳がいろいろであることにします。
Aは0個から40個までのいずれかです。
Bは逆に40個から0個です。
そうすると、全体のA,Bの内訳が変わると、取り出される10個のA,Bの内訳も
もちろん変わります。
袋全体の中味にAが多いときには、取り出される10個もAが多くなりがちです。
その確率を図にしたのが図9.2です。
\begin{lstlisting}
exProb<-function(m){
 m1<-apply(m,1,sum)
 m2<-apply(m,2,sum)
 s<-sum(m)
 exp(sum(lgamma(m1+1))+sum(lgamma(m2+1))-(lgamma(s+1)+sum(lgamma(m+1))))
}

S<-40;A<-0:S;B<-S-A;N<-10;x<-0:N

probmat<-matrix(0,length(A),length(x))
for(i in A){
 for(j in x){
  y<-N-j;  z<-i-j;  w<-S-(j+y+z);
  if(j>=0 & y>=0 & z>=0 & w>=0){
   data<-c(j,y,z,w)
   probmat[i+1,j+1]<-exProb(matrix(c(j,y,z,w),nrow=2))
  }
 }
}

phi<-80;theta<-0;shade<-0.3
persp(A,x,probmat,xlab="No. A in 10", ylab="No. A in 40",zlab="probability",phi=phi,theta=theta,shade=shade)
plot(A - 0.5, probmat[, 4], type = "s")
abline(h = 0, col = "red")
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIII-027.eps}
\caption{全部で40個の2種類の玉のセットから10個を取り出したときの、
玉の内訳別確率。横軸が袋の玉40個のうちA玉の個数、縦軸が10個を一度に抜き出したときの
A玉の個数}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{尤度}
引き続きA,B、２種類の玉が合計40個入っている袋から、10個を取り出すことを考えます。
今、10個を一度に取り出したところ、A,Bが3個と7個だったとします。
袋全体の40個のA,Bの内訳について、どんなことがいえるでしょうか？

Aが3個、取り出されたことから、元の袋にはAが3個以上はあったことが「確実」に言えます。
また、Bが7個、取り出されたことから、元の袋にはBが7個以上あったことも「確実」です。
このことから、(A,B)の内訳は、(3,37),(4,36),...,(33,7)の全部で31通りの可能性があることがわかります。
前節では、元の袋のA,Bの内訳ごとに、10個を取り出したときのA,Bの内訳の確率を計算してありましたから、
それを見てみることにします。
この３次元グラフでは、横軸に袋の中のA玉の数(0から40)が、
縦軸に取り出された10個の中のA玉の個数が対応しています。
その縦横軸で2次元の格子が作られ、
高さは、袋のA玉の数を条件として与えたときに、取り出されるA玉の数に関する確率です。
袋の中のA玉の数を「仮説」、取り出されるA玉の数を「観測」とすれば、
観測の仮説ごとの確率を、すべての条件と、可能なすべての観察についてプロットしたもの
と言えます。
ある仮説に着目するとき、３次元グラフの太い縦線の部分を見ればよいです。
袋の中にA玉が5個の場合について、抜き出したのが、図の右側のグラフです。
縦軸が抜き出されるA玉の数、横軸が確率となっています。
これは、「袋の中にA玉が５個の場合」の「取り出されるA玉の数」に関する確率密度分布
のグラフです。
別の方向に切り出してみます。
「A玉が3個取り出される」という観察について、「袋の中のA玉の数」について抜き出したのが、
図の上部のグラフで、それに対応するのが、３次元グラフでは太い横線になります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/3-2.eps}


  %\includegraphics{./Figures/Ch6/KakuritsuYuudo/KakuritsuYuudo.eps}
\caption{3次元グラフは横軸に袋の中のA玉の個数、縦軸に取り出した玉のうちのA玉の個数。
袋の中のA玉の個数が５個のときのグラフの断面が上部に、取り出した
玉のうち3個がA玉だったときの場合についての断面が右側に表示してある}

 \end{center}
 \label{fig:one}
\end{figure}


A,Bの取り出し個数が3,7個のときの部分だけを抜き出して描いた、図9.3の右のグラフ
を見ると、次のようになります。

Aが0,1,2,34,35,...,40の場合は確率が0です。
ある情報(10個を抜き出したらその内訳が3,7だった)が与えられたときに、
知りたいこと(40個入りの袋の内訳)に関する知識が増えて、それが、
このグラフに表されているということです。
40個のうちわけのうち、ある場合は、「ありえなく」て、
ある場合は、ある場合にくらべて、「ありえる程度が大きい」という知識です。
この「ありえる程度」を「尤度(likelihood)」と言います。
尤度を元の袋のA玉の数aの関数として表すと
\begin{equation*}
L(A=a|(3,7))=\frac{a!(40-a!)10!30!}{40!3!7!(a-3!)(33-a)!}
\end{equation*}
となり、これを\textcolor{green}{尤度関数}\index{ゆうどかんすう@尤度関数}と言います。



\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=30mm]{./Fig/PartIII-028.eps}
   \includegraphics[width=30mm]{./Fig/PartIII-030.eps}
\includegraphics[width=30mm]{./Fig/PartIII-031.eps}
\caption{左:(3,7)個の観測のときの尤度。中央:(3,7)個の観察と(7,3)個の観察のときの尤度。
右:2度の観察情報の両方を使った尤度}

 \end{center}

\end{figure}

\subsection{確率の和は1 尤度の和は1ではない}
この尤度関数のグラフは両端で値が0になった、山形のグラフです(図9.4)。
形が確率密度関数のグラフに似ていますが、
確認の意味で、このグラフの線のした部分の面積(積分)をしてみると、和が１にはなりません。

\begin{lstlisting}
sum(probmat[, 4]) # 確率・尤度の行列のうち、10個中3個がA玉だったという観測に対応する列
\end{lstlisting}

尤度の分布は確率密度分布ではないことがわかります。
足して1にはなりませんが、袋全体の中のAの個数に関する尤度は、次のように
相対的に意味があります。
Aが12個の尤度は、$L(A=12|(3,7))=3.073032e-01$、それに対して、Aが3個の尤度は
$L(A=3|(3,7))=1.214575e-02$です。
$L(A=12|(3,7))/L(A=3|(3,7))=25.3013$
12個の尤度は、3個の尤度の25.3倍です。
これが、2つの可能性(Aが12個か3個か)の尤度の比(尤度比)です。


さて、ここで、抜き出した10個を元に戻して、再度、10個を
抜き出しなおしたら、今度は、A,Bが7個と3個だったとします。
この情報を基にした尤度を描いてみます(図9.4 中央)。

では、元のAが0から40までのそれぞれの尤度を見てみましょう。
2度の抜き取りは相互に独立しているので、１度目の尤度と2度目の尤度を掛け合わせます。
\begin{lstlisting}
# 確率・尤度の行列のうち、10個中3個がA玉だったという観測に対応する列と
# 確率・尤度の行列のうち、10個中7個がA玉だったという観測に対応する列との積
plot(A - 0.5, probmat[, 4] * probmat[, 8], type = "s")
\end{lstlisting}

2度の結果を合わせると、尤度が最も大きいのは、
40個のうちA,Bが半分ずつの場合でした(図9.4右)。



\subsection{尤度の指数化　尤度比と事前確率・事後確率}
抜き取りを繰り返したとき、抜き取り1回分が教える尤度を掛け合わせて、統合した尤度
を求めました。
尤度は絶対値よりも比に意味がありましたから、可能性のある仮説全体について
尤度を足し合わせたときに１になるに調整してやることにします。
こうすると、1は何回掛け合わせても1ですので、好都合です。

このように調整してやった上で、観察するたびに、
尤度をかけ算で計算してやると、ある仮説の尤度がL1からL2に変わったとき、
L1とL2の大小を比較することができます。

今、観測の前後で、比較可能にした観測前後の尤度を観測という「事」の「前」と
「後」の尤度であって、「確率と同じように、足して１になっている」という意味で、
仮説の\textcolor{green}{事前確率}\index{じぜんかくりつ@事前確率}と
\textcolor{green}{事後確率}\index{じごかくりつ@事後確率}と呼びます。
この事前確率と「事」の観察と事後確率の関係のことを、\textcolor{green}{ベイズの定理}
\index{ベイズのていり@ベイズの定理}と言います。

前項の例では、第１回目の抜き取りの前は、40個の玉のうちのA玉の数は、
0,1,...,40のいずれも等しいと考えていました。
第１回の抜き取りの結果、Aが少なめであると考えました。
この考えの変化が事前確率から事後確率への変化です。
第２回目の抜き取りに際しては、第１回目の抜き取りの後の事後確率が、第２回目の抜き取りの前の
事前確率となり、抜き取り結果を見たあとで、
Aの数は半分くらいという事後確率に変化したわけです。
この例では、一番最初には、すべての仮説が同等にもっともらしいとして
いましたが、そうする必要はありません。
たとえば、「風のうわさ」でA玉が少なめだと知っていれば、
A玉の数が0,1,2,...19の尤度を20,21,...,40の尤度の2倍としてスタートすることも可能だからです。

\section{条件付確率、確率、尤度、非独立}
２つの因子A/a, B/bで４グループに分けられた集団を考えます。
全体を１とすると、４グループの比率を下のような表にすることができます。
２通りの表を作ります。
片方は、AとBとが\textcolor{red}{独立}\index{どくりつ@独立}
な場合、もう片方は独立ではない場合です。


\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a&計 \\ \hline
B& 0.3  & 0.2 & 0.5\\ \hline
b& 0.3 & 0.2 & 0.5 \\ \hline
計& 0.6 & 0.4 & 1 \\ \hline
\end{tabular}独立な場合\\

\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a&計 \\ \hline
B& 0.4  & 0.1 & 0.5 \\ \hline
b& 0.2 & 0.3 & 0.5 \\ \hline
計& 0.6 & 0.4 & 1 \\ \hline
\end{tabular}独立で無い場合

比率の様子を次のような図で表してみます(図9.5)。
面積が比率です。
独立な場合には、格子状になり、独立で無い場合には、がたぼこすることがわかります。
\begin{lstlisting}
t1 <- as.table(matrix(c(0.3, 0.2, 0.3, 0.2), nrow = 2, byrow = TRUE))
t2 <- as.table(matrix(c(0.4, 0.1, 0.2, 0.3), nrow = 2, byrow = TRUE))
plot(t(t1))
plot(t(t2))
\end{lstlisting}



\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-010-2.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-011-2.eps}
\caption{左は独立な場合、右は独立でない場合}

 \end{center}
 \label{fig:one}
\end{figure}


この表・図から、情報を引き出してみます。
Aの比率は、0.6、aは0.4、Bは0.5、bは0.5です。
どちらの表でも同じです。
では、このような２種類のラベルを持つ集団から、ある１つのサンプルを抜き出したとします。
"A,B","A,b","a,B","a,b"の４通りのラベルパターンの取り出される確率は、
独立な表の場合には、(0.3,0.3,0,2,0.2)ですし、独立でない表の場合には
(0.4,0.2,0.1,0.3)です。

今、ある１つのサンプルを抜き出して、それがBかbかを当てたいとします。
何の情報もなければ、B:b=0.5:0.5=1:1で予想するのがよいでしょう。
今、このサンプルはAであることがわかったとします。
このとき、Bなのかbなのかを予想しなおすこととします。
図で言えば、左側の列に注目すればよいです。
独立な場合には、B:b=0.3:0.3=1:1と予想します。
残念ながら、Aであることがわかったものの、Bについての予想に変化がありませんでした。
独立で無い場合には、B:b=0.4:0.2=2:1と予想します。
Aに関する情報が無かったときとずいぶんと予想が変わりました。
$\frac{0.4}{0.4+0.2}=\frac{2}{3}$。
これは、Aであるという条件の下での、B/bの\textcolor{green}{条件付確率}
\index{じょうけんつきかくりつ@条件付確率}です。


今、横軸も縦軸も２カテゴリ型で考えました。
カテゴリ数を増やしても同じことです。
横軸に20カテゴリ(A={A1,A2,...,A20})、縦軸に15カテゴリ(B={B1,B2,...,B15})として、20x15通りの
ラベルパターンのすべての頻度がわかっているときには、
20x15表がわかっていて、
その図も描けます。
そして、そこからサンプリングしたときに、Aの20カテゴリのうちのどれであるか(Ai)が
判明したときに、Bの14カテゴリのうちのどれがどれくらいの確率かは、
Aiという条件付確率としてわかります。
縦軸と横軸が独立であるときには、Aに関する情報が判明しても、
Bについて得られる情報はありませんでしたが、
縦軸と横軸が独立でないときには、Bについての情報を増やしてくれる、という点も、
2x2の表の場合と同様です(図9.6左)。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-046-2.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-047.eps}
\caption{20カテゴリ$\times$ 15カテゴリの場合(左)、量的変数$\times$ 量的変数の場合(右)}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{量的な軸での条件付き確率}
縦軸と横軸について、たまたま、カテゴリ型で話しを進めてきました。
そうする必要はないかもしれません。
縦軸と横軸とに入るべきものは、ある尺度の可能性のあるものすべて(確率変数の空間全体)であれば
よいです。
たとえば、横軸に身長、縦軸に体重として、次に示すような分布があるとき、
身長の情報が得られれば、体重がどれくらいかの予想は、身長の
情報がないときよりも、予想が正確になるでしょう。
それは、身長と体重が独立ではないからなのは、カテゴリ型の場合と同じです
図9.6右)。

 
\subsection{事前確率『当初の予想』と陽性的中率(PPV)と陰性的中率(NPV)}
ここまでの例では、ある集団が２つの
\textcolor{red}{確率変数}\index{かくりつへんすう@確率変数}を持っているときに、
2つの確率変数の組み合わせの確率が、
縦と横の軸にとってできる長方形部分の面積であるように図にできました。
このような図があって、２つの確率変数同士が
非独立であるなら、片方の変数を観察すると、もう片方の変数についての
情報が増えるという話しでした。

今度は、少し事情を変えます。
２つの確率変数を使って、2軸に対応させる点は同じです。
ただし、わかっているのは、片方の変数を条件としたときの
もう片方の変数の条件付確率だけです。

２つの変数がA/a、B/bの2カテゴリ型とします。
BであるときにAである確率(\textcolor{red}{条件付確率}
\index{じょうけんつきかくりつ@条件付確率})をPr(A|B)とし、
それ以外の場合も、同様にPr(a|B),Pr(A|b),Pr(a|b)とします。
これらがわかっています。
たとえば、Pr(A|B)=0.8,Pr(a|B)=0.2、Pr(A|b)=0.4,Pr(a|b)=0.6とします。
A/aが男女、B/bが1年生と2年生とします。
1年は男が8割、2年は男が4割です。
今、1年生と2年生の比率が0.5対0.5とすると、\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a&計 \\ \hline
B& 0.4 & 0.1 & 0.5 \\ \hline
b& 0.2 & 0.3 & 0.5 \\ \hline
計& 0.6 & 0.4 & 1 \\ \hline
\end{tabular}\\
という表ができます。
このときに、ある人が男か女かという情報が得られれば、
その人の学年に関する予測の精度が上がるのは前節と同じです。
男(A)と観察したら、1年生の確率は、$Pr(B|A)=\frac{0.4}{0.6}=\frac{2}{3}$です。

もしも、1年生と2年生の比率が0.8対0.2のときには、表が変わって\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a&計 \\ \hline
B& 0.64 & 0.16 & 0.8 \\ \hline
b& 0.08 & 0.12 & 0.2 \\ \hline
計& 0.72 & 0.28 & 1 \\ \hline
\end{tabular}\\
となります。
表は変わりましたが、1年生(Ｂ）の男女比、２年生(ｂ）の男女比は変わっていません。
この表から、男女を観察したときに学年を予測することもできます。
この表から、男である(A)と観察したとき、
1年生である(B)確率は、$Pr(B|A)=\frac{0.64}{0.72}=\frac{8}{9}$
です(図9.7)。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-011-2.eps}
   \includegraphics[width=50mm]{./Fig/PartIII-011-3.eps}
\caption{左はB:b=0.5:0.5。右はB:b=0.8:0.2}

 \end{center}
 \label{fig:one}
\end{figure}


２つの表は、A/aの比率を変えたことで、長方形の中身が変わり、
横軸に関する条件付確率は変わっていないけれども、
縦軸に関する条件付確率が変わったわけです。

このように、観察前のB/bの比率(初めの例での0.5/0.5、2つ目の例での0.8/0.2)を、
\textcolor{green}{事前確率}\index{じぜんかくりつ@事前確率}と言い、それを用いて作った表からわかる$Pr(B|A),Pr(b|A),Pr(B|a),Pr(b|a)$
を
\textcolor{green}{事後確率}\index{じごかくりつ@事後確率}と言います。

医療の現場では、異なった用語を使います。
A/aを検査が「陽性/陰性」として、B/bを病気で「ある/ない」とします。
表を作るために必要だったPr(A|B)(疾患ありの場合の検査陽性率),Pr(a|B),Pr(A|b),Pr(a|b)(疾患なしの場合の検査陰性率)は、\\
Pr(B|A)が検査の\textcolor{green}{感度}
\index{かんど@感度}、Pr(b|a)が検査の
\textcolor{green}{特異度}\index{とくいど@特異度}と呼ばれます。
また、B/bの事前確率は、検査前に集めた症状などの情報から予測した病気である予想確率
です。
そして、B/bの事後確率は、
Pr(B|A)が、「検査が陽性だったときに病気であると考える確率」で、これを
\textcolor{green}{陽性的中率}\index{ようせいてきちゅうりつ@陽性的中率}(
\textcolor{green}{PPV}\index{PPV@PPV})です。
また、Pr(b|a)が、「検査が陰性だったときに病気でないと考える確率」で、これが
\textcolor{green}{陰性的中率}\index{いんせいてきちゅうりつ@陰性的中率}(
\textcolor{green}{NPV}\index{NPV@NPV})です。

ここでは、縦横2軸に2カテゴリ型変数を用いましたが、カテゴリ数が増えたり、量的変数
にしたりしても、考え方は同じです。

\subsubsection{仮説空間と観測空間の条件付確率}
さて、検査と病気の２ｘ２表では、
列についての処理は、検査結果がわかったときの、病気のありなしの尤度を表し、
行についての処理は、病気のありなしがわかっているときの、検査の陽性・陰性の確率を
表していました。
袋からの玉の抜き取りのときに作った、
確率と尤度のグラフを見てみましょう(図9.3)。
列に確率が、行に尤度が出ていたのと同じ構図です。

ここまでの例で、縦軸と横軸にとったものがどういうものだったかを確認します。
分割表の例では、あるカテゴリ変数の取りうるカテゴリすべてを縦軸に、
もう1つの\textcolor{red}{確率変数}\index{かくりつへんすう@確率変数}
のカテゴリすべてを横軸にとりました。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/3-3.eps}


  %\includegraphics{./Figures/Ch6/KakuritsuYuudo2/KakuritsuYuudo2.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}
確率と尤度とは、図9.8に示すように、同じグラフを縦軸を基準にするか横軸を
基準にするかの違いを反映したものです。
身長・体重の例では、縦軸に体重としてとりうる値の範囲のすべてをとり、
横軸に身長として取りうる値の範囲のすべてをとりました。
そして、縦軸と横軸の間に非独立な関係があることを利用して、
片方の観察によって、もう片方に関する情報を改善しました。

２つの確率変数を取り上げて、それぞれの確率空間を軸に置いてやり、
片方の確率変数を観察することによって、もう片方の確率変数に関する情報を
改善すること、とまとめられます。

袋に2種類の玉が入った袋から、抜き出して、袋の中の玉の内訳をあてる、という作業は、
この考え方で行くと、次のようにいえます。
横軸に、袋の中の玉の比率の取りうる範囲を取る。
縦軸には、取り出すという試行で起こりうる事象の場合を取る。
袋の中の玉の比率と取り出す事象の起こる確率は、もちろん関連しているので、縦軸の観察によって、横軸の情報が
改善するわけです。
袋から玉を取り出すことを繰り返して、袋の中の玉の比率を予測する作業は、
次のような作業であることがわかります。
まず、玉の比率の事前確率を考えます。
次に観察をして、事後確率を得ます。
そして、2回目の取り出しにあたっては、事後確率を事前確率と考え直します。
事前確率が変わったによって、縦軸と横軸で作られた長方形部分の分布を変えます。
2回目の観察をします。
事後確率が得られます。
もう一度取り出すなら、事前確率を事後確率に置き換えてから取り出しを・・・
という繰り返しです。

\chapter{連鎖解析に見る尤度と変数}
\section{尤度を使った形質マッピング 連鎖解析}

\textcolor{red}{尤度}\index{ゆうど@尤度}の計算は、取りうる仮説のすべてについて、観察データをもたらす確率を計算することから始まりました。
\textcolor{green}{連鎖解析}
\index{れんさかいせき@連鎖解析}とは、ＤＮＡ配列上にある、
\textcolor{red}{形質}\index{けいしつ@形質}の原因変異の位置を探索する手法の一つで、
数多くの、遺伝因子の同定に成功してきた手法ですが、
家系情報と\textcolor{red}{フェノタイプ}\index{フェノタイプ@フェノタイプ}
情報と\textcolor{red}{ジェノタイプ}\index{ジェノタイプ@ジェノタイプ}
情報とから、非常に多くの場合について、
尤度を計算する解析手法です。
ここでは、尤度を利用することの一環として、その手法の概要を見てみることにします。

連鎖解析は大きく２つに分けられ、\textcolor{green}{パラメトリック}
\index{パラメトリック@パラメトリック}手法と
\textcolor{green}{ノンバラペトリック}\index{ノンパラメトリック@ノンパラメトリック}
手法と呼ばれます。
対象とする形質としては、前者が比較的少数の大家系に認められる強い遺伝因子に向いているのに対して、
後者が、比較的弱い遺伝因子を小規模家系を多数集めて解析するのに向いています。
また、前者は、尤度を計算するべき\textcolor{red}{仮説空間}\index{かせつくうかん@仮説空間}
が広く、込み入っていますので、尤度についての
理解を深める題材として適当であり、
後者は、遺伝因子を変数化することについて考えるのに好適と思われますので、
それぞれ、順を追って説明することとします。
\section{パラメトリック連鎖解析と尤度}
\subsection{マーカーの伝達木と原因座位の伝達木}
\textcolor{green}{パラメトリック連鎖解析}
\index{パラメトリックれんさかいせき@パラメトリック連鎖解析}
では、染色体の家系内での受け渡しと組み換えとのパターンをすべて数え上げます。
そして、フェノタイプの情報とジェノタイプの情報を利用して、
どの染色体のどの位置に原因因子が存在していると考えることがもっともらしいかを調べます。
そして、その原因因子の存在場所としてもっともらしい箇所が、
どれくらい原因因子のありかとして
信憑性があるかを数値で示します。

まず、染色体の家系内での受け渡しと組み換えのパターンとはどういうことかを見てみましょう。
家系図は、個人のつながりでしたが、それは、染色体の伝達のグラフを中に隠しもっているものだ、
ということは5章で述べました。
そして、染色体上の１点について限れば、かならず、木が描けることも確認しました。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/3-4.eps}


%\includegraphics{./Figures/Ch6/Linkage/Linkage.eps}
\caption{両親と2人の子の例。優性遺伝形式で父親と娘が表現型陽性}
 \end{center}
 \label{fig:one}
\end{figure}
今、家系図が与えられたとき、可能性のある木のパターンは、
家系図での辺の数(伝達の数)nに対して$2^n$あります。
図10.1の例では、4人はV1,2,3,4の4点と、それを結ぶ４本の辺E1,2,3,4とで出来ているグラフを構成します。
\textcolor{red}{染色体}\index{せんしょくたい@染色体}・
\textcolor{red}{アレル}\index{アレル@アレル}に着目すれば、、各個人には、２つの要素があります。
染色体・アレルの伝達関係でグラフにすると、辺１本につき、2通りの引き方があるので、
$2^{辺の数}=16$パターンが作れます。
これが、ゲノム上の１箇所に関する、\textcolor{red}{木の形の数}
\index{きのかたちのかず@木の形の数}です。
ゲノム上のすべての箇所は、同様に$2^n$パターンの可能性があります。
この木のパターンを\textcolor{red}{行列}\index{ぎょうれつ@行列}で表すことにします。
染色体・アレルのうち、父親から受け取ったものを０、
母親から受け取ったものを１として、図に表すときには、
父由来のそれを左に母由来のそれを右に置くことにしましょう。
図の染色体の伝達パターンの左上隅のパターン(P1)は、$(E1,E2,E3,E4)=(0,0,0,0)$、と表せて、
その隣のパターン(P2)は$(0,0,0,1)$と表せます。
Rでパターンを作ってみます。
\begin{lstlisting}
library(gtools)
permutations(2,4,c(0,1),repeats=TRUE) # 長さ4のベクトルを作る。抜き出し元は要素数2個で、それはc(0,1)。抜いては戻し(repeats)をTRUEで実行する
\end{lstlisting}
\begin{screen}
\begin{Schunk}
\begin{Soutput}
      [,1] [,2] [,3] [,4]
 [1,]    0    0    0    0
 [2,]    1    0    0    0
 [3,]    0    1    0    0
 [4,]    1    1    0    0
 [5,]    0    0    1    0
 [6,]    1    0    1    0
 [7,]    0    1    1    0
 [8,]    1    1    1    0
 [9,]    0    0    0    1
[10,]    1    0    0    1
[11,]    0    1    0    1
[12,]    1    1    0    1
[13,]    0    0    1    1
[14,]    1    0    1    1
[15,]    0    1    1    1
[16,]    1    1    1    1
\end{Soutput}
\end{Schunk}
\end{screen}

全部で、$L$箇所のことを考えれば、$16^L$と、膨大な数になります。
2箇所(M1,M2)で考えます。
親子４人という小さい単位で考えます。
M1は、この16パターンのどれかを取ります。M2も同様です。
したがって、すべての可能性を考えると$16^2$のパターンを考慮すればよいです。
M1がP1パターンでM2がP2パターンだったとします。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/3-5.eps}
%\includegraphics{./Figures/Ch6/Linkage2/Linkage2.eps}
\caption{上段はM1,M2の木。下段はそれの重ね合わせの図}
 \end{center}
 \label{fig:one}
\end{figure}
辺E4に相当する部分で、Vの字が生じています。
これは、\textcolor{red}{組換え}\index{くみかえ@組換え}が起きたことを表しています。
$(E1,E2,E3,E4)$の表し方でいうと、P1は$(0,0,0,0)$、P2は$(0,0,0,1)$です。
P1とP2の間のマンハッタン距離が１なので、それがこの家系全体での組み換えの伝達数です。
全部の伝達(全部の辺)で組み換えが起きるのが、最大値なので、
組み換えの起きていない伝達数も計算が出来ます。
RでM1の16パターン対M2の16パターン($16^2$パターン)のすべてについて、
組み換えありの伝達の数と組み換えなしの伝達の数を表す行列を作って見ます。
\begin{lstlisting}
RecNumberMat<-as.matrix(dist(m,method="manhattan",diag=TRUE,upper=TRUE)) #隣接ツリー間の組み換え回数
NonRecNumberMat<-n-RecNumberMat #隣接ツリー間の非組み換え回数
RecNumberMat
\end{lstlisting}
組換えあり伝達数行列はこうです。
\begin{screen}
\begin{Schunk}
\begin{Soutput}
   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
1  0 1 1 2 1 2 2 3 1  2  2  3  2  3  3  4
2  1 0 2 1 2 1 3 2 2  1  3  2  3  2  4  3
3  1 2 0 1 2 3 1 2 2  3  1  2  3  4  2  3
4  2 1 1 0 3 2 2 1 3  2  2  1  4  3  3  2
5  1 2 2 3 0 1 1 2 2  3  3  4  1  2  2  3
6  2 1 3 2 1 0 2 1 3  2  4  3  2  1  3  2
7  2 3 1 2 1 2 0 1 3  4  2  3  2  3  1  2
8  3 2 2 1 2 1 1 0 4  3  3  2  3  2  2  1
9  1 2 2 3 2 3 3 4 0  1  1  2  1  2  2  3
10 2 1 3 2 3 2 4 3 1  0  2  1  2  1  3  2
11 2 3 1 2 3 4 2 3 1  2  0  1  2  3  1  2
12 3 2 2 1 4 3 3 2 2  1  1  0  3  2  2  1
13 2 3 3 4 1 2 2 3 1  2  2  3  0  1  1  2
14 3 2 4 3 2 1 3 2 2  1  3  2  1  0  2  1
15 3 4 2 3 2 3 1 2 2  3  1  2  1  2  0  1
16 4 3 3 2 3 2 2 1 3  2  2  1  2  1  1  0
\end{Soutput}
\end{Schunk}
\end{screen}
さて、M1,M2のジェノタイプデータがあると、M1,M2のそれぞれについて、
16パターンの\textcolor{red}{条件付確率}
\index{じょうけんつきかくりつ@条件付確率}(ジェノタイプデータという条件)がわかります。
今、M1では、V1,V2,V3,V4のジェノタイプが、(0,1),(0,1),(0,0),(1,1)だったとすると、
図で言えば、M1は、E1とE3の値は互いに異なり、E2とE4の値も異なるようなパターンがありえるパターンです。
このように、ジェノタイプの情報から、マーカーの木には、ありえる木と
ありえない木があります。
ありえないパターンは確率が0で、ありえるパターンはみな同じ条件付確率を持ちます。

一方、病気の原因座位(G)については、別の理由で、木の条件付確率が決まります。
こちらの理由は、家系の表現型です。
今、病気の\textcolor{red}{遺伝形式}\index{いでんけいしき@遺伝形式}を決めると、フェノタイプのパターンを起こしうる伝達
パターンと起こしえないパターン、より起こしやすいパターンの条件付確率が決まります。
\textcolor{red}{優性}\index{ゆうせい@優性}遺伝形式であるとか、
\textcolor{red}{劣性}\index{れっせい@劣性}遺伝形式であるとかを仮定して、
ジェノタイプが決まると、完全にフェノタイプが決まるとしますと、
家系図上の個人の「原因遺伝子のジェノタイプ」が決まります。
このようにジェノタイプが決まるとフェノタイプが完全に決まることを
\textcolor{green}{浸透率}\index{しんとうりつ@浸透率}が１である、と言います。
ジェノタイプが決まるとフェノタイプが決まるとしても、その逆は必ずしも正しくはありません。
優性遺伝形式のときは、フェノタイプが出ているときに、
\textcolor{red}{ホモ接合体}\index{ホモせつごうたい@ホモ接合体}
なのか
\textcolor{red}{ヘテロ接合体}
\index{ヘテロせつごうたい@ヘテロ接合体}
なのかがわからないような場合
のことです。
このようなときに、
その人の「原因遺伝子のジェノタイプ」は確率的に割り当てます。
また、浸透率が１ではないときには、それに応じて、個々人の「原因遺伝子のジェノタイプ」
を確率的に割り当てる
こともが必要です。

さて、ここまでで、M1,M2におけるありえる伝達パターンと、
M1,M2の間にあるかもしれない、「原因座位(G)」の伝達パターンとがわかりましたから、
「Ma,Mb,Gの伝達パターンがそれぞれ、Pi,Pj,Pkである場合」がありえるかどうかもわかります。
「Pi,Pj,Pk」の３つ組が、起き易い組み合わせかそうでないかは、
PiとPkの間と PjとPkの間で組換え数と非組換え数がそれぞれ何回ずつか
に応じて決まります。
MaとGの間で組み換えが起きる確率が$\theta_{aG}$、同様にGとMbの間のそれが$\theta_{Gb}$とします。
組換えが起きる確率は、0から0.5の値をとります。
0.5の場合は、相互に\textcolor{red}{独立}\index{どくりつ@独立}な場合で、異なる染色体に乗っているような場合ともいえます。
PiとPjの間で、組み換えが起きた伝達数と組み換えの起きなかった伝達数を$Nrec_{i,j},Nnon_{i,j}$とすれば、
\begin{equation*}
L(Pi,Pj,Pk)=\theta_{aG}^{Nrec_{i,k}}\times (1-\theta_{aG})^{Nnon_{i,k}} \times \theta_{Gb}^{Nrec_{k,j}}\times (1-\theta_{Gb})^{Nnon_{k,j}}
\end{equation*}
が尤度です。
これにより、「ありそうなPi,Pj,Pkの組み合わせ」と「ありそうでないPi,Pj,Pkの組み合わせ」に高低がつきました。
組換えあり伝達数が多い区間は距離が長いほうが、尤度が上がりますし、
組換えなし伝達数が多い区間は、距離が短いほうが尤度が上がります。
これをすべてのPi,Pj,Pkの組み合わせについて足し合わせます。
考慮するべきのはM1とGの間とM2とGの間です。
足し合わせるときには、Pi,Pj,Pkの条件付確率(M1,M2のジェノタイプ条件とGのフェノタイプ条件)を
$pi,pj,pk$としてこれらを考慮します。
\begin{equation*}
L=\sum_{all Pi,Pj,Pk} l(Pi,Pj,Pk)pipjpk
\end{equation*}
これが、すべての可能性のあるパターンに関して得られる尤度です。
家族4人の例では、全部で$16^3$パターンあります。\\
\subsection{マーカーと原因座位の間の組み換え}
今、MaとMbの間の組換え確率$\theta_{ab}$は、
マーカーと原因座位の間の組換え確率$\theta_{aG},\theta_{Gb}$と次のような関係にあります。
\begin{equation*}
\theta_{ab}=\theta_{aG}(1-\theta_{bG})+\theta_{bG}(1-\theta_{aG})
\end{equation*}
という関係にあります。
Ma-G間で組換えがおきて、Mb-G間で起きなかった場合と、Ma-G間では起きずにMb-G間で起きた場合
との和が、M1-M2間で起きた場合だからです。
少し工夫をして、
\begin{align*}
\theta_{aG}=0.5-\sqrt{\frac{0.5-\theta_{12}}{2}\frac{cos(t)}{sin(t)}}\\
\theta_{bG}=0.5-\sqrt{\frac{0.5-\theta_{12}}{2}\frac{sin(t)}{2cos(t)}}
\end{align*}
のように変数tの関数で表すことが出来ます。
ここで、tは$\theta_{aG},\theta_{bG}$が0以上になるような範囲に取ります。
tを変化させてやって、
\begin{equation*}
L=\sum_{all Pi,Pj,Pk} l(Pi,Pj,Pk)pipjpk
\end{equation*}
を最大になるようなtを探すことが連鎖解析です。

考えなくてはいけないパターンはたくさんありましたが、
場合の数が多いだけです。
唯一、知りたくて、変化させたいのはGの位置です。
そして観察されたジェノタイプとフェノタイプをもたらす尤度を最大にするGの位置を知ることが目的です。
Gの位置は$t$で変数化しましたから、
この値をたくさん調べて、最大の尤度が出る値を見つけてやりましょう。
Gの位置を動かすと、MaとGの間とMbとGの間で組み換えあり・なし伝達が多いほうがよいのか、
少ないほうがよいのかの値が変わるために、尤度が変化します。

実際の連鎖解析では、ゲノム上のマーカーを用います。
それは、マーカー間の組み換え割合がわかっているからです。
その値と、Gの位置(t)から、Ma-G,Mb-Gの間の組み換え割合が決まりますから、
Gがその場所だったときの尤度をすべてのパターンに関して計算してやりましょう。

残念ながら、4人家族では、伝達に関する情報が大変少ないので、ためしに計算するにしても、つまらないので
次のようにして、勝手に、ありえる伝達パターンを作ってみます。
作り方の考え方としては、Ma,Mbのマーカーの伝達パターンとしては、
「ありえる」か「ありえないか」の二者択一を適当にしてやる。
Gのそれについては、Ma,Mbの伝達パターンと似ていることが、Gがこのあたりに存在するということ
なので、両者の伝達パターンに似させることとし、
浸透率などの影響で、どのパターンも可能性が若干はあるようにしています。

まずは、尤度を計算して、Gの位置に関してプロットしてみましょう。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-059.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}


ここで、この原因遺伝子のありかと目されるところが、本当に、
そうなのかどうかの判断は、原因遺伝子があったとしても、どのマーカーとも同じ染色体上に乗っていない
とした場合(帰無仮説)の尤度と較べて、十分に高い尤度が得られるかどうかで判断します。
図では、水平線が帰無仮説での尤度です。
Gの位置によらないため、水平になっています。
尤度は比で判断すればよいので、帰無仮説の場合の尤度に較べて、10の何乗倍尤度が高いかを
数値で表すことが多く、それを\textcolor{green}{ロッドスコア}\index{ロッドスコア@ロッドスコア}
(\textcolor{green}{LOD}\index{LOD@LOD},Logarithm of odds)と呼びます。

実際の計算では、木のパターンの組み合わせの網羅をどういう手順で計算するか、とか、
木のパターンの組み合わせとはいっても、全マーカーでの組み合わせはせず、近いところのマーカーについてのみ
パターンの組み合わせを考慮するなどの実際上の工夫がなされます。
\begin{lstlisting}
# 以下の引数を用いて対立仮説と帰無仮説の尤度を計算
# n: 木の枝の数
# m1,m2,G: ２マーカーの木の確率分布とジェノタイプが決める木の確率分布
# theta: マーカー間の組換え率
# k: マーカー間の尤度計算地点数
CalcLike<-function(n,m1,m2,G,theta,k){
 x<-seq(from=0,to=1,by=1/k)
 t<-x*pi/2
 theta1<--sqrt((0.5-theta)/sin(2*t))*cos(t)+0.5
 theta2<--sqrt((0.5-theta)/sin(2*t))*sin(t)+0.5
 range<-which(theta1>=0 & theta2>=0)
 x<-x[range]
 theta1<-theta1[range]
 theta2<-theta2[range]
 library(gtools)
  trvec<-permutations(2,n,c(0,1),repeats=TRUE)

 RecNumberMat<-as.matrix(dist(trvec,method="manhattan",diag=TRUE,upper=TRUE)) #隣接ツリー間の組み換え回数
 NonRecNumberMat<-n-RecNumberMat #隣接ツリー間の非組み換え回数
 Lalt<-rep(0,length(x)) # 対立仮説の場合の尤度を格納
 for(i in 1:length(Lalt)){
  m1G<-m1%*%t(G) #MaとGとの確率の積
  m1Gx<-m1G*theta1[i]^RecNumberMat*(1-theta1[i])^NonRecNumberMat #組み換えあり伝達数を考慮
  m1Gx<-apply(m1Gx,2,sum) #(2^n)^2の行列から、Gのパターン (2^n)パターンに集約
  m2G<-m1Gx%*%t(m2) #Mbの確率との積
  m2G<-m2G*theta2[i]^RecNumberMat*(1-theta2[i])^NonRecNumberMat #GとMbとの組み換えあり伝達数を考慮
  Lalt[i]<-sum(m2G) #全パターンについて足し合わせ
 }
# 帰無仮説の場合
 m12<-m1%*%t(m2)
 m12x<-m12*theta^RecNumberMat*(1-theta)^NonRecNumberMat/(2^n)
 Lnull<-rep(sum(m12x),length(Lalt))
 list(logLikeAlt=Lalt,logLikeNull=Lnull,location=x)
}
n<-4 # 木の枝の数
set.seed(65432) # 適当なデータを作るために疑似乱数列の種を与える
m1<-sample(c(0,1),n^2,replace=TRUE,prob=c(0.8,0.2))
m1<-m1/sum(m1) # マーカー１での木パターンの確率分布
m2<-sample(c(0,1),n^2,replace=TRUE,prob=c(0.8,0.2))
m2<-m2/sum(m2) # マーカー２での木パターンの確率分布
G<-0.9*m1+0.1*m2
G<-G/sum(G) # ジェノタイプが決める木パターンの確率分布
theta<-0.4 # ２マーカー間の組換え率
k<-100 # マーカー間を100等分した場所で尤度を計算
LL<-CalcLike(n,m1,m2,G,theta,k)
ylim<-c(min(LL$logLikeAlt,LL$logLikeNull),max(LL$logLikeAlt,LL$logLikeNull))
ylim<-c(0,max(LL$logLikeAlt,LL$logLikeNull))
plot(LL$location,LL$logLikeAlt,type="l",ylim=ylim) # 対立仮説の尤度のグラフ
par(new=T)
plot(LL$location,LL$logLikeNull,type="l",col="red",ylim=ylim) # 帰無仮説の尤度のグラフ
\end{lstlisting}

\subsection{隠れマルコフモデルと連鎖解析の尤度計算}
２つのマーカーに挟まれた座位の位置決めをするために
座位数３個(マーカー２個、原因座位1個)に関する伝達パターンと、
位置をずらす変数との４変数が作る
場合をすべて網羅するのは気が利かないので、気を利かせる方法を考えて見ます。
\textcolor{green}{隠れマルコフモデル}\index{かくれマルコフモデル@隠れマルコフモデル}と呼ばれる方法です。

M1,G,M2と並んだ座位について考えます。
M1のパターンから、Gのパターンへ変化すると考えます。
M1のパターンは複数あって、Gのパターンも複数あります。
M1のどのパターンからも、Gのすべてのパターンへと変化する可能性はあります。
問題は、M1とGとの間の組み換え割合の多寡によって、移りやすいパターンと
移りにくいパターンがあることです。
実際、
\begin{lstlisting}
i <- 3
theta1G[i]^RecNumberMat * (1 - theta1G[i])^NonRecNumberMat
\end{lstlisting}
とすればM1の$2^n=16$パターンとGの$2^n=16$パターンの組み合わせで、$16^2$の
パターンを考えています。
次に、GからM2への変化を考えます。
このとき、M1→Gのパターン$16^2$通りのすべてについて、M2の$2^n=16$パターンへの推移を
考えるわけで、このようにすると、$16^3$通りを考えなくてはなりません。
しかしながら、
GとM2との関係(組換え割合と伝達パターンの変化)は、Gのパターンにはよりますが、
M1のパターンにはよりません。
\footnote{
実際の染色体の組換えでは、MaとGの間で組み換えが起きていると、GとMbの間で組み換えは
起きにくくなりますが(紐をねじるときに、続けざまにねじるには力が要ります）、
ループを回して計算しているときにも、そこまで考慮していないので、
今回も考慮するには及びません。
}
したがって、
M1とGの$16^2$パターンのうち、Gのパターンが共通するパターン(それは$16$パターンあります)は
M2との関係において、同じに扱ってしまうことが出来ます。
同じに扱うということは、まとめてしまうわけです。
$16\times 16$行列から、Gのパターンが同じものを合算して、長さ$16$の
ベクトルにしてやります。
その上で、Mbの$16$パターンとの関係を組み換えに考慮して計算すればよいです。
この方法のよいところは、増えたパターンをもとのパターン数に戻してから、
次のステップに進めることで、
これならば、次から次へと、処理を続けていくときに、同じことを繰り返すだけで済みます。

このように、順番に移り変わっていくときに、次のステップへの変化は、
現在の状態のみから(確率的に)決められるような
移り変わりを\textcolor{green}{単純マルコフ連鎖}
\index{たんじゅんまるこふれんさ@単純マルコフ連鎖}と言います。
\footnote{
単純でないマルコフ連鎖として次のステップへの変化が、
現在とそれより前の何段階の状態から決まるような
移り変わりが定義されますが、いずれにしろ、次のステップが、
限定された前段階のみから決められるような
ものがマルコフ連鎖です
}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/3-6.eps}

%\includegraphics{./Figures/Ch6/HMM1/HMM1.eps}
\caption{3座位のそれぞれに$2^n$の伝達パターンがある。左から右へと、伝達パターンが推移する。
パターン間の推移しやすさは、組み換えあり伝達数(k)と組み換え無し伝達数(n-k)と座位間の組み換え割合($\theta$)
とで決まる。推移しやすさは、パターン数ｘパターン数の行列である。
2番目の座位のパターンと３番目の座位のパターンとの推移には、1番目の座位のパターンは関係していない。
伝達パターンは、すべて推定されたもので、観察できないので、「隠れ」ている}
 \end{center}
 \label{fig:one}
\end{figure}
座位のアレルの伝達パターンの塩基配列上での移り変わりは、
座位間の組み換え割合によって、決まりますので、
マルコフ連鎖です。
マルコフ連鎖の定義にあるように、「次のステップに関係しない前段階」はあってもなくても、
「次のステップ」に
ついて決められます。
それを利用して、パターンが増えた後で、少ないパターンに戻してやることが出来ました。

今、観察しているのは、マーカーのジェノタイプや、個人のフェノタイプです。
これらは、「隠れて」いません。
一方、一番興味のある原因座位Gの位置はもちろん、観察することはできませんし、
その位置のありそうな場所を知るための情報である、
各座位の$2^n$の伝達パターンと、座位間のパターン
同士をつなぐ組換えのありなしも、観察されておらず、「隠されて」います。
「隠れていない」観察データから、興味の対象である
「隠された」状態の推移を探索する作業なので、
「隠れたマルコフ連鎖」と呼ばれます。

\section{ノンパラメトリック連鎖解析 罹患同胞対解析}
\subsection{相対危険度を変数とする}
連鎖解析では、伝達パターンについて推定することを基本にします。
前項では、遺伝形式や浸透率などをパラメタとして仮定して尤度を調べる
ことで、原因座位の位置を探しました。
本項では、\textcolor{green}{ノンパラメトリック}\index{ノンパラメトリック@ノンパラメトリック}
と言う名前が示す通り、
遺伝形式や浸透率を仮定せずに解析を進める方法を扱います。
遺伝形式などを変数として与える代わりに、
こちらの方法では、ジェノタイプがフェノタイプに影響しているか否か、
影響しているとしたら、その強さはどれくらいかを変数とします。
そして、影響がないとみなすよりもあるとみなす方がもっともらしい位置を
ゲノム上から探します。
このノンパラメトリック連鎖解析手法は、ある病気を、\textcolor{red}{同胞}
\index{どうほう@同胞}(兄弟姉妹)で揃って発病している
ペア(\textcolor{green}{罹患同胞対}\index{りかんどうほうつい@罹患同胞対}
)を収集して解析する研究で用いられる例が多いので、
それに沿って話しを進めます。
では、ジェノタイプがフェノタイプに影響している強さを変数にする方法と、
それが、伝達パターンとどのような関係にあるかをみていくことにします。

ある座位のあるアレルがジェノタイプを起こしやすくしているとします。
そのアレルをA,それ以外のアレルをaとすると、３つの遺伝子型AA,Aa,aaが考えられます。
それぞれのジェノタイプのときに、$R_2,R_1,R_0$の確率でフェノタイプを起こすとします。
この座位がフェノタイプと関係がなければ$R_2=R_1=R_0$です。

罹患同胞対のジェノタイプの組み合わせは、$3^2=9$通りあります。
それぞれの場合で、同胞対がそろって、病気である確率は、
１人目のジェノタイプから決まる病気の確率と
２人目のジェノタイプから決まる病気の確率の積です。
表にすると以下の通りです。

\begin{screen}
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&AA&Aa&aa\\ \hline
Risk & $R_2$ & $R_1$ &$ R_0$\\ \hline
\end{tabular}
\end{screen}
\begin{screen}
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&AA&Aa&aa\\ \hline
AA& $R_2^2$ & $R_2\times R_1$ &$R_2\times R_0$\\ \hline
Aa&  $R_1 \times R_2$ & $R_1^2 $ &$R_1\times R_0$\\ \hline
aa&   $R_2 \times R_0$ & $R_1\times R_0$ &$R_0^2 $\\ \hline
\end{tabular}
\end{screen}\\

\subsection{IBD数ごとに場合分けする}
ここで、同胞対について伝達パターンを考えます。
１人につき、親子２人からの伝達があるので、伝達数は４つあります。
４つの伝達が、それぞれ、親のもつ２つの染色体のどちらからの伝達かの区別をするので、
$2^4=16$の伝達パターンがあるのでした。
それを$4\times 4$の行列で表してみます。
父親の２つの染色体を$F1,F2$、母親のそれを$M1,M2$と書くことにします。
１人は$F1M1,F1M2,F2M1,F2M2$の４パターンありえます。
同胞２人の一人目を縦軸に、２人目を横軸にすると、各軸に４パターンを
対応させます。

ここで、同胞対がそろって、フェノタイプを持っている場合について考えるためには、
同胞対のジェノタイプを考慮する必要があります。
同胞対のジェノタイプを考慮するためには、親が持つ２アレルがそれぞれ、
リスク型かそうでないかの区別が必要ですから、そのことを考えます。

両親の４染色体の中に、リスク型(A)がいくつあるかで分類します。
０，１，２，３，４の５通りあります。
０の場合、子どもは必ず、非リスク型の\textcolor{red}{ホモ接合体}
\index{ホモせつごうたい@ホモ接合体}(aa)です。
１の場合、子どもは非リスク型のホモ接合体(aa)か、
\textcolor{red}{ヘテロ接合体}\index{ヘテロせつごうたい@ヘテロ接合体}(Aa)です。
２の場合、その２本を片親が持っている場合には、子どもは必ずヘテロ接合体(Aa)です。
２の場合で、両親が１本ずつ持っている場合には、
子どもは非リスク型のホモ接合体(aa)も、リスク型のホモ接合体(AA)も、
ヘテロ接合体(Aa)もありえます。
３の場合、子どもはリスク型のホモ接合体(AA)か、ヘテロ接合体(Aa)です。
４の場合、子どもは必ず、リスク型のホモ接合体(AA)です。
両親のリスク型保有数別に考えていくこととして、代表パターンを
決めます。
両親リスクアレル数が０の場合は、$F1=M1=F2=M2=a$です。
両親リスクアレル数が１の場合は、$F1=A,M1=F2=M2=a$を代表にします。
両親リスクアレル数が２の場合は、２通り考える必要があります。
リスクアレルが片親に偏っていて、子どもが必ず、ヘテロ接合体の場合は
$F1=F2=A,M1=M2=a$を代表にします。
他方、両親がリスクアレルを１本ずつ持つ場合は、$F1=M1=A,F2=M2=a$を代表にします。
両親リスクアレル数が３の場合は、$F1=a,M1=F2=M2=A$を代表にします。
両親リスクアレル数が４の場合は、$F1=M1=F2=M2=A$です。

16伝達パターンごとに、2人の子どものジェノタイプと、
2人がそろって、病気である確率を示します。

F1=M1=F2=M2=aの場合(2人揃って発病する確率が、伝達パターンによりません)　\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& aa aa & aa aa &  aa aa & aa aa \\ \hline
F1M2& aa aa &aa aa & aa aa & aa aa \\ \hline
F2M1&   aa aa &aa aa & aa aa & aa aa \\ \hline
F2M2&  aa aa &aa aa & aa aa & aa aa \\ \hline
\end{tabular} 
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& $R_0^2$ & $R_0^2$ & $R_0^2$ & $R_0^2$ \\ \hline
F1M2& $R_0^2$ & $R_0^2$ & $R_0^2$ & $R_0^2$ \\ \hline
F2M1& $R_0^2$ & $R_0^2$ & $R_0^2$ & $R_0^2$ \\ \hline
F2M2& $R_0^2$ & $R_0^2$ & $R_0^2$ & $R_0^2$ \\ \hline
\end{tabular}\\



F1=A,M1=F2=M2=aの場合(2人揃って発病する確率が、伝達パターンによりません)　\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& Aa Aa & Aa Aa&  Aa AA& Aa AA\\ \hline
F1M2& Aa  Aa&Aa Aa& Aa AA & Aa AA \\ \hline
F2M1&   AA Aa &AA Aa & AA AA& AA AA \\ \hline
F2M2&   AA Aa&AA Aa& AA AA & AA AA \\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& $R_1^2$ & $R_1^2$ & $R_0\times R_1$ & $R_0\times R_1$ \\ \hline
F1M2& $R_1^2$ & $R_1^2$ & $R_0\times R_1$ & $R_0\times R_1$  \\ \hline
F2M1& $R_0\times R_1$ & $R_0\times R_1$ & $R_0^2$ & $R_0^2$\\ \hline
F2M2& $R_0\times R_1$ & $R_0\times R_1$ & $R_0^2$ & $R_0^2$ \\ \hline
\end{tabular}\\



F1=F2=A,M1=M2=aの場合(2人揃って発病する確率が、伝達パターンによりません)　\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& Aa Aa & Aa Aa &  Aa Aa & Aa Aa \\ \hline
F1M2& Aa Aa &Aa Aa & Aa Aa & Aa Aa \\ \hline
F2M1&  Aa Aa &Aa Aa & Aa Aa & Aa Aa \\ \hline
F2M2&   Aa Aa &Aa Aa & Aa Aa & Aa Aa \\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& $R_1^2$ & $R_1^2$ & $R_1^2$ & $R_1^2$ \\ \hline
F1M2& $R_1^2$ & $R_1^2$ & $R_1^2$ & $R_1^2$ \\ \hline
F2M1& $R_1^2$ & $R_1^2$ & $R_1^2$ & $R_1^2$ \\ \hline
F2M2& $R_1^2$ & $R_1^2$ & $R_1^2$ & $R_1^2$ \\ \hline
\end{tabular}\\

F1=M1=A,F2=M2=aの場合\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& AA AA & AA Aa&  AA Aa& AA aa\\ \hline
F1M2& Aa  AA&Aa Aa& Aa Aa & Aa aa \\ \hline
F2M1&   Aa AA &Aa Aa & Aa Aa& Aa aa \\ \hline
F2M2&   aa AA&aa Aa& aa Aa & aa aa \\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& $R_2^2$ & $R_1\times R_2$ & $R_1\times R_2$ & $R_0\times R_2$ \\ \hline
F1M2& $R_1\times R_2$ & $R_1^2$ & $R_1^2$ & $R_0\times R_1$ \\ \hline
F2M1& $R_1\times R_2$ & $R_1^2$ & $R_1^2$ &$R_0\times R_1$ \\ \hline
F2M2& $R_0\times R_2$ & $R_0\times R_1$ & $R_0\times R_1$ & $R_0^2$ \\ \hline
\end{tabular}\\

F1=a,M1=A,F2=M2=Aの場合\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& Aa Aa & Aa Aa&  Aa AA& Aa AA\\ \hline
F1M2& Aa  Aa&Aa Aa& Aa AA & Aa AA \\ \hline
F2M1&   AA Aa &AA Aa & AA AA& AA AA \\ \hline
F2M2&   AA Aa&AA Aa& AA AA& AA AA\\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& $R_1^2$ & $R_1^2$ & $R_1\times R_2$ & $R_1\times R_2$ \\ \hline
F1M2& $R_1^2$ & $R_1^2$ & $R_2\times R_1$ & $R_2\times R_2$  \\ \hline
F2M1& $R_1\times R_2$ & $R_1\times R_2$ & $R_2^2$ & $R_2^2$\\ \hline
F2M2& $R_1\times R_2$ & $R_1\times R_2$ & $R_2^2$ & $R_2^2$ \\ \hline
\end{tabular}\\

F1=M1=F2=M2=Aの場合(2人揃って発病する確率が、伝達パターンによりません)　\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& AA AA & AA AA&  AA AA& AA AA\\ \hline
F1M2& AA  AA&AA AA& AA AA & AA AA \\ \hline
F2M1&   AA AA &AA AA & AA AA& AA AA \\ \hline
F2M2&   AA AA&AA AA& AA AA & AA AA \\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& $R_2^2$ & $R_2^2$ & $R_2^2$ & $R_2^2$ \\ \hline
F1M2& $R_2^2$ & $R_2^2$ & $R_2^2$ & $R_2^2$ \\ \hline
F2M1& $R_2^2$ & $R_2^2$ & $R_2^2$ & $R_2^2$ \\ \hline
F2M2& $R_2^2$ & $R_2^2$ & $R_2^2$ & $R_2^2$ \\ \hline
\end{tabular}\\

両親のアレルの持ち方を場合分けした上で、
子どものアレルの持ち方16通りごとに、同胞がそろって病気になる
確率がわかりました。

伝達パターン16通りのそれぞれの尤度を考えることが可能です。
もう少し工夫してみることにします。

16伝達を場合分けします。
\textcolor{red}{IBD}\index{IBD@IBD}という考え方がありました(2章 2.1.3)。

アレルの由来染色体が同じであることでした。
このIBDの数で16パターンを分類してみます。
IBDは2本とも同じ、１本だけ同じ、同じなアレルはない、の３パターンです。
16パターンよりは、3パターンの方が、考えるのが簡単そうです。

IBD数を同胞対のパターンの組み合わせごとに表に記します。
IBD数が２の場合が４、１の場合が８、０の場合が４通りあります。
16パターンはどれも等確率でおきますから、偏りがなければ、IBD数が、2,1,0になる確率は、
0.25,0.5,0.25であることがわかります。
\begin{screen}
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&F1M1&F1M2&F2M1&F2M2 \\ \hline
F1M1& 2 & 1 &  1 & 0 \\ \hline
F1M2& 1  &2 & 0& 1 \\ \hline
F2M1&   1  &0 & 2& 1 \\ \hline
F2M2&   0 &1 & 1& 3 \\ \hline
\end{tabular}
\end{screen}\\
同胞対が罹患しているときに、IBD数が0,1,2のどれに、どれくらいなりやすいか
の計算は、16伝達パターンのIBD数と、16伝達パターンごとの2人揃って病気になる確率
とから、計算が可能です。
表にすると以下のようになります。

\begin{tabular}[htb]{|c|c|c|c|c|c|c|c|} \hline
F1&M1&F2&M2&IBD数2 &IBD数1&IBD数0　\\ \hline
a&a&a&a& $\frac{1}{4}$ &$\frac{1}{2}$&$\frac{1}{4}$\\ \hline
A&a&a&a& $\frac{1}{4} (1+(\frac{R_1-R_0}{R_1+R_0})^2)$ &$\frac{1}{2}$&$\frac{1}{4}(1-(\frac{R_1-R_0}{R_1+R_0})^2)$ \\ \hline
A&a&A&a&  $\frac{1}{4}$ &$\frac{1}{2}$&$\frac{1}{4}$\\ \hline
A&A&a&a& $\frac{1}{2}((1-S)+(\frac{R_2-R_0}{R_2+2R_1+R_0})^2 )$ & $S=\frac{1}{2}(1-(\frac{R_2-2R_1+R_0}{R_2+2R_1+R_0})^2) $ & $\frac{1}{2}((1-S)-(\frac{R_2-R_0}{R_2+2R_1+R_0})^2 )$ \\ \hline
a&A&A&A& $\frac{1}{4} (1+(\frac{R_2-R_1}{R_2+R_1})^2)$ &$\frac{1}{2}$&$\frac{1}{4}(1-(\frac{R_2-R_1}{R_2+R_1})^2)$ \\ \hline
A&A&A&A&  $\frac{1}{4}$ &$\frac{1}{2}$&$\frac{1}{4}$\\ \hline
\end{tabular}
\\



\begin{tabular}[htb]{|c|c|c|c|c|c|c|c|} \hline
F1&M1&F2&M2&IBD数2 &IBD数1&IBD数0　\\ \hline
a&a&a&a& $\frac{1}{4}$ &$\frac{1}{2}$&$\frac{1}{4}$\\ \hline
A&a&a&a& $\frac{1}{4} (1+\Delta_{1>0}^2)$ &$\frac{1}{2}$&$\frac{1}{4}(1-\Delta_{1>0}^2)$ \\ \hline
A&a&A&a&  $\frac{1}{4}$ &$\frac{1}{2}$&$\frac{1}{4}$\\ \hline
A&A&a&a& $\frac{1}{4}(1+\Delta_{2>1,0>1}^2+2\Delta_{2>1>0}^2)$ & $\frac{1}{2}(1-\Delta_{2>1,0>1}^2) $ & $\frac{1}{4}(1+\Delta_{2>1,1>0}^2-2\Delta_{2>1>0}^2)$ \\ \hline
a&A&A&A& $\frac{1}{4} (1+\Delta_{2>1}^2)$ &$\frac{1}{2}$&$\frac{1}{4}(1-\Delta_{2>1}^2)$ \\ \hline
A&A&A&A&  $\frac{1}{4}$ &$\frac{1}{2}$&$\frac{1}{4}$\\ \hline
\end{tabular}
\begin{align*}
\Delta_{1>0}=\frac{R_1-R_0}{R_1+R_0}\\
\Delta_{2>1}=\frac{R_2-R_1}{R_2+R_1}\\
\Delta_{2>1,0>1}= \frac{R_2-2R_1+R_0}{R_2+2R_1+R_0} = 
\frac{(R_2-R_1)+(R_0-R_1)}{(R_2+R_1)+(R_0+R_1)}  =  
\frac{(R_2+R_0)-2R_1}{(R_2+R_1)+(R_0+R_1)}  \\
\Delta_{2>1>0}= \frac{R_2-R_0}{R_2+2R_1+R_0}= 
 \frac{(R_2-R_1)+(R_1-R_0)}{(R_2+R_1)+(R_1+R_0)}
\end{align*}

16パターンのすべてで2人揃って病気になる確率が等しい場合があります。
両親のリスクアレル保有数が、０の場合と４の場合、それから、２であって、その２つの
リスクアレルが片親に偏っている場合です。
この場合には、IBD数が0,1,2の尤度は0.25,0.5,0.25です。
両親の４アレルのうち、１アレルだけがリスクアレルの場合と、１アレルだけが非リスクアレルの
場合とは、IBD数１の尤度は0.5ですが、IBD数２の尤度が、0.25より大きくなり、それと引き換えに、
IBD数０の尤度が小さくなります。
この出入りの大きさは、表では$\Delta^2$で表しているように、IBD数が２のときに0.25より大きくなり、
IBD数が０のときには0.25より小さくなります。
その逆はありません。
両親がそろってAaの場合を見てみます。
この場合だけ、IBD数が１の尤度が0.25からずれています。
そして、そのずれは、$\Delta^2$と表している通り、0.25より小さくなる方向にずれます。
このとき、IBD数が２の場合と、IBD数が０の場合はIBD数が１の場合のずれの分を
均等に引き受けて大きくなり($\Delta_{2>1,0>1}^2$の項)、その上で、IBD数２が大きくなり($\Delta_{2>1>0}^2$の項)、
その分だけ、IBD数０が小さくなります。
ずれの項の大きさがどのように決まるかを見てみましょう。
両親がリスクアレルを１つだけもつときのずれの項は、$\Delta_{1>0}^2=(\frac{R_1-R_0}{R_1+R_0})^2$です。
これは、Aaとaaとのリスクの差が０のときに０となり、それ以外は正の値を持つ項です。
これは、\textcolor{red}{優性遺伝形式}\index{ゆうせいいでんけいしき@優性遺伝形式}の場合にもっともよくあてはまります。
優性遺伝形式では、両親のうちの片方だけが有病のことがあることが多く、
また、AaのリスクとAAのリスクは同じです。
実際に優性遺伝形式の場合の有病者はたいてい、リスクアレルを１本だけ持ちますので、
Aaとaaのリスクの差が問題となることと、符合します。
両親が３本のリスクアレルを持つ場合のずれの項は、$\Delta_{2>1}^2=(\frac{R_2-R_1}{R_2+R_1})^2$です。
これは、AAのリスクとAaのリスクの差で、\textcolor{red}{尤度}\index{ゆうど@尤度}のずれが決まります。
\textcolor{red}{劣性遺伝形式}\index{れっせいいでんけいしき@劣性遺伝形式}
の場合にAAとAaの違いが問題となります。
実際には、劣性遺伝形式の場合には、両親がそろって、リスクアレルを１本保有していることが多いので、
このパターンを調べることは多くありませんが、IBD数の尤度に関しては、このパターンが、
劣性形式に対応していることがわかります。
最後に両親がそろってAaの場合の尤度のずれの項についてです。
まず、IBD数が１の場合のずれの項、$\Delta_{2>1,0>1}^2=(\frac{(R_2+R_0)-2R_1}{(R_2+R_1)+(R_0+R_1)})^2$は、
$R_2+R_0=2R_1$の場合に0になります。
AaのリスクがAAとaaの中間のときのことです。
この項は、IBD数が０のときも２のときも、尤度のずれをもたらします。
リスクアレルを１本持つ場合のリスクが、リスクアレルを２本持つ場合のちょうど半分であるか、それよりずれるかが、
IBD数のすべての場合に影響することになります。
リスクアレルが相加的にリスクを持つ状態を基準にして、
優性遺伝形式寄りか劣性遺伝形式寄りにずれていることを表す項です。
最後に、IBD数が０と２のときに、さらにずれる項、$\Delta_{2>1>0}^2=(\frac{R_2-R_0}{R_2+2R_1+R_0})^2$です。
これは、AAとaaのリスクの差の項です。
つまり、AA,Aa,aaのリスクは、AAとaaのリスクの差を問題にし、
その上で、Aaのリスクについては、AAとaaの中間を基準にしたうえで、
そこからのずれについて考慮している、という構図になっています。

$R_2,R_1,R_0$が相互に異なっていれば、両親から罹患同胞対への伝達パターン１６通りの
尤度が影響を受け、その16パターンをIBD数によってグループ分けしたときの、
３グループの尤度が影響を受ける様子を確認しました。
罹患同胞対を用いたノンパラメトリック連鎖解析では、
このことを使って、原因座位の位置を探します。
$\Delta$の値を変えれば、IBD数0，1，2の尤度は、増減しますから、
それを変化させた上で、原因座位がどこにありそうかを尤度で表し、
さらに、それが、帰無仮説と比較して有意なのかどうかを判定します。

\chapter{指数(インデックス)とは}
\section{指数は相対的な値}
データをかいつまむ話しの最後として、\textcolor{green}{指数}\index{しすう@指数}
(\textcolor{green}{インデックス}\index{インデックス@インデックス})というものについて考えます。
ここでの指数というのは、相対評価して数値化したもののことです。
基準に照らしてそれとの相対評価をするときに使います。

ある分子の機能が十全な場合と、まったく失われた場合と、
ある程度失われた場合とを評価するようなものを
想定することができます。
２つの集団の混ざり具合として、
全く混ざっていない状態と均一な一集団にまで混ざった状態の２状態を両極として、
その間としてどのくらいの混ざり具合か、というような評価も、そのような例です。
かならずしも、上限と下限とがあって、その範囲で評価する場合とは限りません。
原点に対して、ある基準値を定め、そこからの増減を割合で表すような場合があります。
経済成長率などはよくこうした値を用います。
これらに共通するのは、原点とそれ以外の基準点があることです。
下限と上限があれば、それを０と１にして指数は０から１の値をとります。
原点と基準点があれば、原点を０、基準点を１として、基準点より原点に近ければ、０から１の値、
基準点より原点から遠ければ１より大きい値を与えます。
また、基準点１からの増減を、増減率とすることもあります。
１から０．９への変化を「０．９倍」と評価したり、「０．１減少」と評価することです。
指数の良いところは、数値を見るだけで、その大きさが理解しやすいことです。
これは、確率分布が無限の値に対応しているときに、変数の値の大きさがイメージしにくいことと
対照的です。
\section{不平衡の指数}
\subsection{ハーディ・ワインバーグ平衡(HWE)}
\textcolor{red}{ハーディ・ワインバーグ平衡}\index{ハーディワインバーグへいこう@
ハーディワインバーグ平衡}の評価で用いたF(３章 3.3.3)は指数です。
平衡なときに0、最も偏ったときに1をとっています。

\begin{tabular}[htb]{|c|c|c|c|} \hline
 &A&a&計\\ \hline
A& $f_{AA}$ & $f_{aA}$  &$f_{A}$\\ \hline
a& $f_{Aa}$  & $f_{aa}$ &$f_{a}$\\ \hline
計&$f_A$&$f_a$&$1$\\ \hline
\end{tabular}

\begin{tabular}[htb]{|c|c|c|c|} \hline
 &A&a&計\\ \hline
A& $f_{AA}=f_A$ & $0$  &$f_A$\\ \hline
a& $0$  & $f_{aa}=f_a$ &$f_a$\\ \hline
計&$f_A$&$f_a$&$1$\\ \hline
\end{tabular}

\subsection{連鎖不平衡}
\textcolor{red}{連鎖不平衡}\index{れんさふへいこう@連鎖不平衡}でも指数を使います。
平衡なときが0で、もっとも偏ったときが1です。
３章で扱った$r,r^2$もこれを満足する指数です。
連鎖不平衡の場合には、「もっとも偏ったとき」の定義を変えることで、
別の指数が使われることもあります。
２多型のアレル頻度が等しくなければ、$r=1$は実現しない状態ですので、
アレル頻度が異なる場合にも実現する範囲での平衡からのずれの最大状態のときに
１となるように調整することができます。
\textcolor{red}{連鎖不平衡係数}\index{れんさふへいこうけいすう@連鎖不平衡係数}として知られる
\textcolor{green}{$D'$}\index{D'@D'}は、そのような指標で、次に示すような
ハプロタイプ頻度状態(４ハプロタイプのうち、１ハプロタイプが頻度０)を
最も極端な状態としています。\\

\begin{tabular}[htb]{|c|c|c|c|} \hline
 &A&a&計\\ \hline
B& $f_A$ & $f_B-f_A=f_a-f_b$  &$f_{B}$\\ \hline
b& $0$  & $f_b$ &$f_{b}$\\ \hline
計&$f_A$&$f_a$&$1$\\ \hline
\end{tabular}


\subsection{Ｐ値　確率変数を指数化する}
確率分布は、変数の値が無限大まで広がっており、
指数と較べて値の大きさがわかりにくいと述べました。
それを解消するのが、Ｐ値です。
\textcolor{red}{確率密度関数}\index{かくりつみつどかんすう@確率密度関数}
は\textcolor{red}{確率空間}\index{かくりつくうかん@確率空間}について
\textcolor{red}{積分}\index{せきぶん@積分}すると１になることを利用します。
確率密度関数を変数の小さいほうから累積していった関数を、
\textcolor{red}{累積分布関数}\index{るいせきぶんぷかんすう@累積分布関数}と呼びます。
こうすることで、確率変数の値は、その確率分布の
\textcolor{red}{クオンタイル}\index{クオンタイル@クオンタイル}(
\textcolor{red}{分位数}\index{ぶんいすう@分位数})に対応づけることができます。
クオンタイルは、確率変数が最小のときに０であり、最大のときに１となります。
また、確率分布を統計学的\textcolor{red}{検定}\index{けんてい@検定}
に用いるときには、値が大きいほど、仮説を信じないほうがよい(仮説を棄却する)こと
を表しますので、クオンタイルが１に近いほど、仮説が棄却するべきということになります。
Ｐ値は１-クオンタイルと反転することで、
Ｐ値が小さいほど仮説が棄却されるべきであるというようにしています。
図11.1は
自由度４のカイ自乗分布の確率密度関数と累積分布関数(クオンタイル値)とＰ値のプロットです。
確率密度関数の積分が累積分布関数となり、P値は、累積分布関数と上下対称なグラフです。
\begin{lstlisting}
df<-4;x <- seq(from = 0, to = 15, by = 0.1)
d <- dchisq(x, df);q <- pchisq(x, df);p <- pchisq(x, df, lower.tail = FALSE)
plot(x, d, ylim = ylim, type = "l")
par(new = T);plot(x, q, ylim = ylim, type = "l")
par(new = T);plot(x, p, ylim = ylim, type = "l")
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIII-024.eps}
\caption{自由度４のカイ自乗分布の確率密度(最も背の低いグラフ)、累積密度(右上がりのグラフ)、
P値(左上がりのグラフ)}

 \end{center}
 \label{fig:one}
\end{figure}




\part{推定・仮説・棄却・関連・因果}
このパートはデータに潜む知りたい真実を推定し、
データに基づいて、仮説の真偽を判断(棄却と検定)し、
現象の原因なのか結果なのか(因果)を検討すること
に関する話しです。
\chapter{推定}
まずはじめに推定についてです。
\section{最尤推定}
ある大規模な集団について、２カテゴリ型(因子ありとなし)の調べものをしているとします。
何人かをサンプルとして調べることにします。
20人を調べたところ、因子ありが6人、なしが14人だったとします。
さて、大規模集団では、因子ありの割合はどれくらいと考えればよいでしょうか。
$\frac{6}{6+14}=0.3$ですから、0.3(30\%)が因子ありと\textcolor{green}{推定}\index{すいてい@推定}します。
それでよいです。
因子ありの割合が0.3であるとき、抜き出しを20回繰り返して、そのうち、６回が因子ありである
\textcolor{red}{確率}\index{かくりつ@確率}は、\\
\begin{equation*}
\frac{20!}{6!14!} 0.3^6 \times (1-0.7)^{14}
\end{equation*}
です。これは20人中6人が因子を持っていたときの\textcolor{red}{尤度関数}
\index{ゆうどかんすう@尤度関数}です。
今、集団での因子ありの割合を$p$とすると、20人中6人が因子ありの確率は\\
\begin{equation*}
f(p)= \frac{20!}{6!14!} p^6 \times (1-p)^{14}
\end{equation*}
です。
これを、$p$で\textcolor{red}{微分}\index{びぶん@微分}します。
微分するのは、最大値をとるときには、傾きが０であるからです。
\begin{align*}
f'(p)&= \frac{20!}{6!14!} \times 6\times p^5 \times (1-p)^{14} - 14 \times p^6(1-p)^{13} \\
&= \frac{20!}{6!14!} \times p^5 \times (1-p)^{13} (6(1-p)-14 p )\\
&= \frac{20!}{6!14!} \times p^5 \times (1-p)^{13} (6-20p)
\end{align*}
です。
\begin{equation*}
f'(p=0.3)=0
\end{equation*}
と、$f(p)$は$p=0.3$で唯一の\textcolor{green}{極値}\index{きょくち@極値}
(\textcolor{red}{最大値}\index{さいだいち@最大値}もしくは
\textcolor{red}{最小値}\index{さいしょうち@最小値})をとることがわかります。
実際、この極値が最大値であることは、$f(p=0.3)>0$で、
$f(p=0)=f(p=1)=0$であることから、容易にわかります。
ですから、$p=0.3$というのは、20人中６人が因子ありだったという情報があるときに、、
元の集団の因子あり割合のうちで、尤度を最大にするものであることがわかります。
尤度を最大にする$p$の値はわかりました。
これは、元の集団の因子あり割合について推定した値です。
この場合は尤度を最大にする推定値なので\textcolor{green}{最尤推定値}
\index{さいゆうすいていち@最尤推定値}といいます。
\section{信頼区間}
元の集団の因子あり割合は0.3かもしれませんが、0.3以外がありえないわけでは
ありません。
推定範囲に幅を持たせて考えることにします。
「幅があるのはよいとして、おおよそ、これより高く、これより低い」という目安の値が
知りたいものとします。
今、推定したいpの取りうる値は0から1まであって、今、尤度のピークは0.3です。
尤度関数$f(p)$は、pの値ごとに、何倍、ありそうかを表した関数ですから、
定数$C$を使って、
\begin{equation*}
f_c(p)=Cp^6\times(1-p)^{14}
\end{equation*}　と書けます。
ここで、$C$を
\begin{equation*}
\int_{0}^{1} f_c(p) dp = \int_{0}^{1} C p^6 \times (1-p)^{14} dp =1
\end{equation*}
を満足するようにとることとして、
\begin{equation*}
\int_{L}^{H} f_c(p) dp : \int_{0}^{L} f_c(p) dp+
\int_{H}^1  f_c(p) dp = \alpha : (1-\alpha)
\end{equation*}
となるような値$L$と$H$があれば、$p$は$L \le p \le H$の範囲に、$\alpha$の確率で
入るだろう、と言えるでしょう。
実際、
\begin{equation*}
C=\frac{1}{\int_0^1 p^6\times (1-p)^{14}}dp
\end{equation*}
とすればよいです。
この値は、割合が$p$のときに、20人中因子ありが6人である確率
\begin{equation*}
\frac{20!}{6!14!}p^6\times (1-p)^{14}
\end{equation*}
と関係のある係数で
\begin{equation*}
C=\frac{(20+1)!}{6!14!}=\frac{21!}{6!14!}
\end{equation*}
であることが知られています。

さらに一般化しておきます。
ここで用いている\textcolor{green}{階乗}
\index{かいじょう@階乗}($n!$)を、
整数以外でも計算できるようにした関数である
\textcolor{green}{ガンマ($\Gamma$)関数}\index{ガンマかんすう@ガンマ関数}
と\textcolor{red}{ベータ関数}\index{ベータかんすう@ベータ関数}(B)を使って、
次のように書き換えることができます\footnote{
これらの間の関係は19章を参照}
。
N人中k人の場合として、一般に
\begin{equation*}
C=\frac{(N+1)!}{k!(N-k)!}= \frac{\Gamma((N+1)+1)}{\Gamma(k+1)\Gamma((N-k)+1)}
= \frac{\Gamma( (k+1)+(N-k+1) )}{\Gamma(k+1)\Gamma((N-k)+1)}
=\frac{1}{B(k+1,(N-k+1))}
\end{equation*}
となります。従って
\begin{equation*}
f_c(p)=\frac{1}{B(k+1,(N-k+1))}p^k(1-p)^{N-k}=\beta(p;k+1,N-k+1)
\end{equation*}
が求める分布です。
そしてこれには$\beta$分布

という名前がついています。
ここで、
\begin{equation*}
\int_{0}^{L} f_c(p) dp = \int_{H}^1 f_c(p) dp
\end{equation*}
であること、という条件を付け加えれば、
\begin{equation*}
\int_{L}^{H} f_c(p) dp : \int_{0}^{L} f_c(p) dp+
\int_{H}^1  f_c(p) dp = \alpha : (1-\alpha)
\end{equation*}
を満足するような$\alpha$の値に対して、
$L,H$の値が確定します。
これが、元の集団の因子あり割合の推定値を、尤度関数に基づいて決めた$\alpha$
\textcolor{green}{信頼区間}\index{しんらいくかん@信頼区間}です。
では、実際にRを使ってこの信頼区間を求めてみることにします。
pの値について、グラフを描けば、最大の尤度を与える$p=0.3$にピークが来ることも
見て取れます。
また、pの95％信頼区間
\begin{lstlisting}
set.seed(.Random.seed[1]) # 疑似乱数を揃えるため
N <- 20;k <- 6 # 観測情報
p <- seq(from = 0, to = 1, by = 0.01) # 数値を計算するpのリスト
v <- dbeta(p, k + 1, N - k + 1) # β関数の確率密度
plot(p, v, type = "l")
abline(v = k/N) # 最尤推定値
cirange <- 0.95 # 信頼区間を与え上下 0.025の範囲を指定します
# β関数のクォンタイル
ci <- qbeta(c((1 - cirange)/2, 1 - (1 - cirange)/2), k + 1,   N - k + 1)
abline(v = ci) # 95%信頼区間
\end{lstlisting}
95％信頼区間が、 0.1458769 0.5217511
となります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-001.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

20人中6人が因子あり、という観測に基づいて、$p$の確率密度分布が
\begin{equation*}
\frac{1}{B(7,15)}p^6\times (1-p)^14 =\beta(p;k+1=\alpha=7,N-k+1=\beta=15)
\end{equation*}
というベータ分布であると推定しています。
ここで、再度、1人をサンプリングすることとして、そのサンプルが
因子ありである確率を予想してみることとします。
ベータ分布では、
\textcolor{green}{最頻値}\index{さいひんち@最頻値}が$\frac{\alpha-1}{\alpha-1+\beta-1}=0.3$、
平均が$\frac{\alpha}{\alpha+\beta}=0.318$となることが知られていますから、
もっとも可能性が高いのは、0.3のときです。
また、期待値は
$0.318$となります。
これは、図12.1のピークが$p=0.3$のところにあることと、分布が右に裾を引いていることと
符合します。
\section{色々な信頼区間}
元の集団の因子あり割合の推定値を、尤度関数に基づいて決めた$\alpha$信頼区間についてみてきました。
ここで、「元の集団の因子あり割合の推定値の」「尤度関数に基づいて決めた」信頼区間と書きました。
なぜなら、信頼区間には、いろいろな見方の信頼区間があり、
「何を」「どうやって」求めた信頼区間かがいくつもあるからです。

Rで見てみましょう。「あり・なし」のような２つの値をとるかどうかの分布を\textcolor{green}{二項分布}
\index{にこうぶんぷ@二項分布}と
呼びますが、そのパッケージ"binom"を使います。
\begin{screen}
\begin{Schunk}
\begin{Sinput}
> library(binom)
> binom.confint(6, 20, prior.shape1 = 1, prior.shape2 = 1)
\end{Sinput}
\begin{Soutput}
          method x  n      mean      lower     upper
1  agresti-coull 6 20 0.3000000 0.14315926 0.5212908
2     asymptotic 6 20 0.3000000 0.09916346 0.5008365
3          bayes 6 20 0.3181818 0.14587694 0.5217511
4        cloglog 6 20 0.3000000 0.12252643 0.5013504
5          exact 6 20 0.3000000 0.11893159 0.5427892
6          logit 6 20 0.3000000 0.14140583 0.5272397
7         probit 6 20 0.3000000 0.13522492 0.5212242
8        profile 6 20 0.3000000 0.13184834 0.5165045
9            lrt 6 20 0.3000000 0.13178688 0.5165046
10     prop.test 6 20 0.3000000 0.12839086 0.5433071
11        wilson 6 20 0.3000000 0.14547724 0.5189728
\end{Soutput}
\end{Schunk}
\end{screen}
11種類の方法があることがわかります。
3番目の"bayes"が前節の方法で求めた値と一致しています。
それぞれについて、推定値を１つの値で答えるなら、この値という出力"mean"と、
信頼区間の上下値(upper, lower)が出力されます。

これらの手法の意味と求め方は、ここでは触れないこととしますが、
既知の分布を仮定して算出する方法と、そうでない方法に分かれることと、
上下限値が最尤推定値から同じだけ離れている信頼区間(対称性)と
そうでないものとがあることに
着目すると、それぞれの特徴がつかみやすいです。
例えば、上下限値の平均と最尤推定値が等しいことから、2番目のasymptoticという方法が
対称性であることがわかります。
方法は色々ありますが、いずれの方法でもサンプル数が増えると、
平均が0.3に収束し、信頼区間も
同じ値に収束します。その様子をプロットしたのが図10.2です。
11種類の方法のうち、4方法のみについてプロットしてあります。
横軸は、10,20,...,100人を調べて、それぞれ、3,6,...,30人が因子ありだったときの、割合の推定値(平均)と
95％信頼区間です。
サンプル数が多くなると、手法による差がなくなるということは、
逆に言うと、
サンプル数が少ないときには方法によって、推定に差があるということです。
そして、サンプル数が少ないときには、推定値として、どのような挙動をするのが望ましいかによって、
使い分けをることが適当である、ということでもあります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-005.eps}
\caption{横軸が総人数。総人数のうち３割が因子ありだったときの
推定結果。縦軸は平均と信頼区間}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{ベイズ推定 観察していないとき}
因子のありなしについて、母集団の割合の推定についてみてきました。
２項分布に関して割合$p$についての尤度関数としてベータ分布が登場しました。
また、最尤推定値と期待値・平均値がずれていることが示されました。
信頼区間の計算も行い、そのときに"bayes(\textcolor{red}{ベイズ}
\index{ベイズ@ベイズ})"という名前のついて区間推定方法が出てきました。

ベイズという名のつく推定は、\textcolor{red}{ベイズの定理}\index{ベイズのていり@ベイズの定理}
(\textcolor{red}{事前分布}\index{じぜんぶんぷ@事前分布}と観察から
\textcolor{red}{事後分布}\index{じごぶんぷ@事後分布}を計算すること)が基本です。
その意味で、先ほどの最尤推定・信頼区間の算出を見直します。
\begin{lstlisting}
binom.confint(6, 20, prior.shape1 = 1, prior.shape2 = 1)
\end{lstlisting}
と実行しました、6,20は２０人中６人という意味ですし、conf.level=0.95というのは、信頼区間の指定です。
"prior.shape1=1,prior.shape2=1"というのは何でしょうか。
これは、ベータ分布 
\begin{equation*}
\frac{1}{B(a,b)} p^{a-1}(1-p)^{b-1}
\end{equation*}
の"a=prior.shape1,b=prior.shape2"
に相当する変数です。
20人中６人が因子ありという観察をしたときの、母集団の因子保有者率の尤度の分布を
描くときに、図12.1を描くRのコマンドではベータ分布の関数を用いて、
"v<-dbeta(p,k+1,N-k+1)"と入力しました。
この"k+1=a,N-k+1=b"です。
k=6,N-k=14(a=k+1=7,b=N-k+1=15)の場合と、
a=1,b=1(k=a-1=0,N-k=b-1=0)の場合をプロットしてみると図12.3になります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-006.eps}
\caption{a=7,b=15の場合とa=1,b=1の場合のベータ関数の確率密度分布(前者が山型、後者が水平線)}

 \end{center}
 \label{fig:one}
\end{figure}

"a=1,b=1"の方は、\textcolor{red}{均一分布}\index{きんいつぶんぷ@均一分布}であることが見て取れます。
そのことは定義式からもわかります。
\begin{equation*}
\frac{1}{B(1,1)}p^{1-1}(1-p)^{1-1}=\frac{\Gamma(1+1)}{\Gamma(1+1)\Gamma(1+1)}\times1=1
\end{equation*}
また、別の見方をすれば、"a=1,b=1"は、"k=0,N-k=0"のことですが、これは、１人も調べていない
ことを意味しています。
何の情報が何もない状態という意味です。
そんなときは$p$の値に優劣をつけられないために均一分布を推定しているわけです。

\subsection{ベイズ推定 事前確率 共役事前分布}
今、何がしかの情報があって、因子ありの方が少なめだという\textcolor{red}{事前確率}\index{じぜんかくりつ@事前確率}
があるとします。
因子ありが因子なしよりも少なめな事前分布としてどんな分布を仮定しても、構わないのですが、
ここで、ベータ分布を事前分布にしてみることにします。
"dbeta()"関数の引数として、"x<y"のような２数を与えれば、因子ありが少なめになりますから、
適当にx,yを変えてやって、その分布の形をみてみて、この分布が、思っている分布だ、というのがあれば、
それを採用すればよいです。
\begin{lstlisting}
x <- 1.2; y <- 1.5; p<-seq(from=0,to=1,by=0.01)
v3 <- dbeta(p, x, y)
plot(p, v3, type = "l", ylim = c(0, 4))
v4 <- dbeta(p,x+6,y+14)
par(new=TRUE)
plot(p, v4, type = "l", ylim = c(0, 4))
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-007.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-009.eps}
\caption{左はx=1.2,y=1.5の事前分布。右はその事前分布を受けて、6:14の観察を
した後の事後分布}

 \end{center}
 \label{fig:one}
\end{figure}

たとえば、こんな分布(図12.4左)が思い描いている分布だとします。
\begin{equation*}
\frac{1}{B(x=1.2,y=1.5)}p^{x-1}(1-p)^{y-1}
\end{equation*}
という分布です。
この事前分布を思い描いているときに、20人中、6人が因子あり、という情報が得られたとすると、
事後分布は、
\begin{equation*}
\frac{1}{B(x,y)} p^{x-1}(1-p)^{y-1} \times p^6 (1-p)^{14}=\frac{1}{B(x,y)} p^{x+6-1}(1-p)^{y+14-1}
\end{equation*}
に比例することになります。

このようなpの関数で、pが取り得る範囲($0 \le p \le 1$) について積分して１になるように
補正してやれば、
\begin{equation*}
\frac{1}{B(x+6,y+14)} p^{x+6-1}(1-p)^{y+14-1}
\end{equation*}
となるので、
結局、事前分布にベータ分布を仮定すると、観察結果(N人中k人が因子あり)の情報によって、
\begin{equation*}
\frac{1}{B(x,y)}p^{x-1}(1-p)^{y-1} \to \frac{1}{B(x+k,y+(N-k))} p^{x+k-1}(1-p)^{x+(N-k)-1}
\end{equation*}
という変化になります。
関数を決めているパラメタのみに着目すれば
\begin{equation*}
(x,y) \to (x+k,y+(N-k))
\end{equation*}
という変化です。
もし、この後、さらに、情報が追加されて、$N'$人中$k'$人が因子ありだったら
\begin{equation*}
(x,y) \to (x+k,y+(N-k)) \to (x+k+k', y+(N-k)+(N'-k'))
\end{equation*}
というようにパラメタの変化だけを気にすればよいです。

このように、事前分布と事後分布が同じ分布であると、分布を
変化させていくときに、分布のパラメタの簡単な計算のみを考えればよくなって、好都合です。
確率的に起きる現象が、２項分布であるときには、ベータ分布を事前分布とおくと
便利なわけです。
この２項分布とベータ分布との便利な関係を、「二項分布の\textcolor{green}{共役事前分布}
\index{きょうやくじぜんぶんぷ@共役事前分布}はベータ分布である」、
と言います。
次項で説明する\textcolor{red}{多項分布}
\index{たこうぶんぷ@多項分布}の共役事前分布は
\textcolor{red}{ディリクレ分布}\index{ディリクレぶんぷ@ディリクレ分布}ですし、
\textcolor{red}{ポアッソン分布}\index{ポアッソンぶんぷ@ポアッソン分布}
のそれは\textcolor{red}{ガンマ分布}\index{ガンマぶんぷ@ガンマ分布}です。
これを用いて、"x=1.2,y=1.5"の事前分布からスタートして、20人中６人
という観察に基づいて求めた事後分布は図12.4右のようになります。

\subsection{多項分布とその共役事前分布 ディリクレ分布}
３以上のカテゴリに関して、\textcolor{green}{多項分布}\index{たこうぶんぷ@多項分布}があります。
３以上カテゴリがある比率であるときに、そこからいくつかを取り出したときに
ある内訳となる確率に関する分布です。
\textcolor{red}{二項分布}\index{にこうぶんぷ@二項分布}を拡張をカテゴリ数が３以上でも使えるように拡張した分布です。
\begin{equation*}
\frac{N!}{\prod_{i=1}^k n_i!} \prod_{i=1}^k (p_i )^{n_i}
\end{equation*}
ただし、$\sum_{i=1}^k n_i=N,n_i \ge 0$　と表されます。

二項分布の\textcolor{red}{共役事前分布}\index{きょうやくじぜんぶんぷ@共役事前分布}は
\textcolor{red}{ベータ分布}\index{ベータぶんぷ@ベータ分布}でしたが、
多項分布の共役事前分布は\textcolor{green}{ディリクレ分布}\index{ディリクレぶんぷ@ディリクレ分布}
と呼ばれます。

今、母集団の３カテゴリの比率を推定しようとしているとします。
20人をサンプリングして観察したら、内訳が10,7,3人ずつだったとします。
事前分布として、まったく無情報のときには、全部で0人観察して、
各カテゴリに0人ずつの情報がある状態です。
ベータ分布のときには、このような場合に、"(1,1)"を関数に渡しました。
ディリクレ分布のときにも同様に、"(1,1,1)"を渡します。
そして、観測したならば、ベータ分布のときには、"(1+6,1+14)"のように、観察人数をそれぞれ足してやりました。
ディリクレ分布も同様で、"(1+10,1+7,1+3)"としてやります。
以下のＲのコマンドに相当する部分を抜き出すと以下の通りです。
ベータ分布のときの"dbeta()"関数に相当するのが"ddirichlet"関数です。
\begin{lstlisting}
library(MCMCpack) # パッケージが必要です
pre<-c(1,1,1)
obs<-c(10,7,3)
densityPre<-ddirichlet(xyz,pre)
...
densityPost<-ddirichlet(xyz,pre+obs)
\end{lstlisting}
３カテゴリは２次元\textcolor{red}{正単体}\index{せいたんたい@正単体}
である正三角形の頂点に配置することができます。
\footnote{
$k$カテゴリに$\mathbf{v_i}(i=1,2,...,k)$を対応させたとき、
これらの線形結合
$\mathbf{x}=\sum_{i=1}^k a_i \mathbf{v_i}$ で表される$\mathbf{x}$は
$\sum_{i=1}^k a_i =1$という制約をつけると、このk個のベクトルの頂点を通る面の上の点である。
さらに制約$a_i \ge 0$を入れると、k個のベクトルの頂点同士を
結んだ直線に囲まれた領域(\textcolor{green}{凸包}\index{とっぽう@凸包})となる
}
３カテゴリの内訳分布はこの正三角形の内部に描けますから、
それを描いて、濃淡で分布を示せば、
事前分布は、一様で(図12.5左)で、ある観察に基づいて推定した事後分布は図12.5中央のように
濃淡ができます。

\footnote{
※ここの扱いをどうするかは追って検討
これを描図するかんすうは、ウェブサイト\url{cccc}から関数....を
■　DrawDirichletDensity.R　
■ DrawCI.R
}
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=30mm]{./Fig/PartIV-011.eps}
   \includegraphics[width=30mm]{./Fig/PartIV-012.eps}
   \includegraphics[width=30mm]{./Fig/PartIV-013.eps}
\caption{左が未観測状態の事前分布(均一分布)、中央が観測(10,7,3)後の事後分布。
右は、乱数による95%CI区間の表示(中央の黒が95%CI内、周辺の淡い領域がCIの外側)
}
 \end{center}
\end{figure}

では、この場合の信頼区間について、考えてみることにします。
ベータ分布を用いて95％信頼区間を考えたときには、両端にそれぞれ、5％の半分を取ることで、
信頼区間を決めました。
今回は、２次元平面に広がっていて「両端」がありません。
その代わりに、「周辺地帯」があります。
「周辺地帯」が合算して５％になるようにして、中央付近に95％が来るように線引きをしたいです。
ある分布から、ランダムにサンプリングができるときには、
たくさんのサンプルを発生させて、その分布から、信頼区間を決めることもできます。
今の例では、(10+1,7+1,3+1)をパラメータとしたディリクレ分布が対象ですが、
これから、乱数を発生させることもできますし(rdirichlet())、
そのようにして発生させた、３カテゴリの比率のデータについて、
そのディリクレ分布に基づく生起確率も求められます(ddirichlet())。
発生させた、サンプルの確率が小さい方から5%と、大きいほうから95％を塗り分ければ、
境界線が、95％信頼区間の辺縁になります。

\subsection{最尤推定とハプロタイプ頻度推定　連鎖不平衡係数推定}
\subsubsection{疑似乱数を用いた最尤推定}
２個の2アレル型多型(A/aとB/b)があるとします。
２多型が作るハプロタイプとしては、AB,Ab,aB,abの４種類があります。
今、２多型のそれぞれについて、\textcolor{red}{２倍体}\index{にばいたい@２倍体}
\textcolor{red}{ジェノタイプ}\index{ジェノタイプ@ジェノタイプ}を観察するとします。
９タイプの人数($n_{ij}$)が観察されたとしましょう。
\begin{table}
ジェノタイプ観察表\\
\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&AA&Aa&aa&計 \\ \hline
BB& $n_{11}=2$&$n_{12}=5$&$n_{13}=2$ &9\\ \hline
Bb&$n_{21}=10$&$n_{22}=18$&$n_{23}=14$ &42\\ \hline
bb&$n_{31}=4$&$n_{32}=25$&$n_{33}=20$ &49\\ \hline
計&16&48&36 &100\\ \hline
\end{tabular}\\
ジェノタイプ頻度表\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&AA&Aa&aa \\ \hline
BB& $F_{11}=f_1^2$&$F_{12}=2f_1f_2$&$F_{13}=f_2^2$ \\ \hline
Bb&$F_{21}=2f_1f_3$&$F_{22}=2(f_1f_4+f_2f_3)$&$F_{23}=2f_2f_4$ \\ \hline
bb&$F_{31}=f_3^2$&$F_{32}=2f_3f_4$&$F_{33}=f_4^2$ \\ \hline
\end{tabular}\\
\end{table}
$n_{ij}$の値は、総人数を100とし、AA,Aa,aaは、Aのアレル頻度$p_A=0.4$でＨＷＥを満足するように、定め、
BB,Bb,bbは、Ｂのアレル頻度$p_B=0.3$でＨＷＥを満足するように定め、
その上で、$n_{ij}$の内訳を、(AA,BB),(aa,bb)が多めになり
(AA,bb),(aa,BB)が少なめになるように適当に足し引きして作った表です。

さて、この集団の４ハプロタイプAB,aB,Ab,abの頻度を推定してみることとします。
４ハプロタイプの頻度を$f_1,f_2,f_3,f_4, \sum_{i=1}^4 f_i=1$として、HWEを仮定すれば、
ディプロタイプの頻度$F_{ij}$はジェノタイプ頻度表のように与えられます。
これを用いて、９ディプロタイプの観測の尤度$L(F)$の対数(\textcolor{green}{対数尤度}
\index{たいすうゆうど@対数尤度})$log(L(F))$を計算すると
\begin{equation*}
log(L(F))= \sum_{i=1}^3 \sum_{j=1}^3 log(F_{ij}^{n_{ij}}) = \sum_{i=1}^3 \sum_{j=1}^3 n_{ij}logF_{ij}
\end{equation*}
です。

４ハプロタイプの頻度$f_{k},k=1,2,3,4$をディリクレ分布を使って、ランダムに発生させて、
「$log(L(F))$を一番大きくするような$f_{k}$」
を見つけてやることにしましょう。

\begin{lstlisting}
library(MCMCpack)
Make3x3 <- function(f) {# 4ハプロタイプの頻度$f$から、９ジェノタイプのHWE下頻度を作成
    tmp <- f %*% t(f)
    matrix(c(tmp[1, 1], 2 * tmp[1, 2], tmp[2, 2], 2 * tmp[1, 
        3], 2 * (tmp[1, 4] + tmp[2, 3]), 2 * tmp[2, 4], tmp[3, 
        3], 2 * tmp[3, 4], tmp[4, 4]), nrow = 3, byrow = TRUE)
}
# ４ハプロタイプ頻度$f$の下での９ジェノタイプカウント$g$を観察する対数尤度を算出
CalcLike3x3 <- function(g = matrix(1, 3, 3), f = c(0.25, 0.25, 0.25, 0.25)) {
    F <- Make3x3(f)
    sum(g * log(F))
}
CalcR <- function(h) { # 4ハプロタイプ頻度から連鎖不平衡係数rを計算
    p_A <- h[1] + h[3]
    p_B <- h[1] + h[2]
    (h[1] - p_A * p_B)/sqrt(p_A * (1 - p_A) * p_B * (1 - p_B))
}
ns <- matrix(c(2, 5, 2, 10, 18, 14, 4, 25, 20), nrow = 3,  byrow = TRUE) # 初期９ジェノタイプカウント
N <- 10000 # 試行回数
p <- c(1, 1, 1, 1) # ディリクレ事前分布パラメタ(一様分布)
sampled <- rdirichlet(N, p) # ディリクレ乱数
loglikes <- apply(sampled,1,CalcLike3x3,g=ns)
maxset <- sampled[which(loglikes == max(loglikes)), ] # 最大尤度を返した4ハプロタイプ頻度
maxp_A <- maxset[1] + maxset[3] # 最大尤度を返したハプロタイプ頻度から、SNPAのアレル頻度を計算
maxp_B <- maxset[1] + maxset[2] # 同じくSNPBのアレル頻度を計算
maxr <- CalcR(maxset) # 同じく連鎖不平衡係数rを計算
\end{lstlisting}
\begin{screen}
\begin{Schunk}
\begin{Sinput}
> maxset
\end{Sinput}
\begin{Soutput}
[1] 0.1520910 0.1296524 0.2513286 0.4669280
\end{Soutput}
\end{Schunk}
\end{screen}
４ハプロタイプ頻度として、 AB,aB,Ab,abの頻度として、0.16,0.14,0.24,0.46
くらいの値が返り、A,Bの頻度としては0.4前後0.3前後の値(提示例では0.4034196,0.2817434)
となります。

４ハプロタイプの頻度も、N回の試行をした限りでは、尤度が最大な値なわけですから、
まずまずの\textcolor{red}{最尤推定値}\index{さいゆうすいていち@最尤推定値}と言えます。\\
\subsubsection{制約を入れて最尤推定}
別のやり方を考えてみます。
今、AA,Aa,aaの人数、16,48,36が与えられています。
このとき、Aアレル本数は、$16\times 2+48=80$と数え上げることが可能で、
この数値から、Aアレルの標本頻度が0.4であることはわかります。
同様に、Bアレルの標本頻度は0.3です。
これらはそれぞれのアレル頻度の最尤推定値です。
\footnote{HWE条件の下、$log(L(p))=n_{AA} log(p_A^2)+n_{Aa}log(2p_Ap_a)+n_{aa}log(p_a^2)=(2n_{AA}+n_{Aa})log(A)+(2n_{aa}+n_{Aa})log(a)+n_{Aa}log(2)$
pで微分すると、$(log(L(p)))'=\frac{2n_{AA}+n_{Aa}}{p}-\frac{2n_{aa}+n_{Aa}}{1-p}$
$(log(L(p)))'=0$を解いて、$p=\frac{2n_{AA}+n_{Aa}}{2(n_{AA}+n_{Aa}+n_{aa}}$}
では、$p_A,p_B$はこの多型ごとの最尤推定値を用いるとして、
９ジェノタイプの人数がわかっているときに４ハプロタイプの頻度の推定をしてみましょう。

４ハプロタイプの頻度は、２多型のアレル頻度$p_A,p_a,p_B,p_b$と、
\textcolor{red}{連鎖不平衡係数}\index{れんさふへいこうけいすう@連鎖不平衡係数}$r$を用いて、\\
\begin{align*}
h_1=p_Ap_B+r\sqrt{p_Ap_ap_Bp_b}\\
h_2=p_Ap_b-r\sqrt{p_Ap_ap_Bp_b}\\
h_3=p_ap_B-r\sqrt{p_Ap_ap_Bp_b}\\
h_4=p_ap_b+r\sqrt{p_Ap_ap_Bp_b}
\end{align*}
と表せたことを思い出せば、$p_A,p_B$が与えられているときには、
$r$のみで4ハプロタイプ頻度が決まります。
従って
尤度関数も$r$のみの関数になります。
今度は、rを$-1 \le r \le 1$の範囲で、適当な刻み幅でしらみつぶしに
調べてみることにしましょう。図12.6のようになります。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-017.eps}
\caption{横軸に$r$、縦軸に対数尤度}

 \end{center}
\end{figure}
尤度がピークを作ることが示されました。
このピークをもたらす$r$が最尤推定値です。

実際、求めた尤度を、先に実施したディリクレ分布乱数によって得られた尤度と較べると、
今回の尤度の方が高くなります。
\begin{lstlisting}
# p_A,p_Bを与えて、$r$を変数として４ハプロタイプの頻度を作る
p_A <- 0.4;p_B <- 0.3
f1 <- p_A * p_B; f2 <- (1 - p_A) * p_B; f3 <- p_A * (1 - p_B); f4 <- (1 - p_A) * (1 - p_B)
rs <- seq(from = -1, to = 1, by = 0.001)
ds <- rs * sqrt(p_A * (1 - p_A) * p_B * (1 - p_B))
f1 <- ds + f1; f2 <- -ds + f2; f3 <- -ds + f3; f4 <- ds + f4
F <- matrix(c(f1, f2, f3, f4), nrow = length(f1))
minF <- apply(F, 1, min) # ４ハプロタイプとも正の頻度を持つ場合を取り出す。
rs <- rs[minF > 0]
F <- F[minF > 0, ]
loglikes2 <- apply(F,1,CalcLike3x3,g=ns)
maxset2 <- F[which(loglikes == max(loglikes2)), ]
maxp_A2 <- maxset2[1] + maxset2[3];maxp_B2 <- maxset2[1] + maxset2[2]
maxr2 <- CalcR(maxset2)
plot(rs,loglikes2,type="l") 
\end{lstlisting}

\section{EMアルゴリズム}
二通りの方法で、最尤推定値に近づいてみました。
一つ目の方法では変数の空間を闇雲に探索(\textcolor{red}{ディリクレ分布}
\index{ディリクレぶんぷ@ディリクレ分布}で4変数の空間を探索)し、
二つ目の方法では、規則正しく(１変数にして、細かく定まった間隔で探索)
空間を調べ上げました。
変数の数が多いとき、空間は広く、
空間が広くなると、空間全体を調べることが大変になります。
そのようなときに、うまく探索する方法が必要になります。
その一つが\textcolor{green}{EMアルゴリズム}
\index{EMアルゴリズム@EMアルゴリズム}という方法です。
\footnote{
ちなみに、「アルゴリズム」というのは、何かしらの目的を達成するための、
「方法に関する工夫」とでも言うものです。
たいていは、完璧な方法がなかったり、あったとしても、非常に時間や労力がかかる場合に、
少ない労力で、満足できる結果を得るような工夫であって、
その工夫が確かに、そうなっていることが、示されている(論文等で発表されている)もののこと、
と言ってもよいでしょう。
}

ＥＭアルゴリズムを用いて、観測データ(ディプロタイプ頻度データ)を元に
ハプロタイプ頻度を推定するときには、
ハプロタイプ頻度に、暫定的に値を割り当てるところからスタートします。
そして、与えた暫定頻度の下での、ハプロタイプ頻度の期待値を計算しなおして、
暫定的な値を更新します。
このような手順を踏むと、段々に尤度が高くなることが知られています。
その点を信用することとして、実際の処理の動きを見ていくことにします。
アルゴリズムが完璧であれば、最初に与える暫定的はハプロタイプ頻度(初期値)によらずに、
得られるべき結果(最尤推定値)に行き着くべきですが、
このアルゴリズムの特徴として、より大きな
尤度を与える値に移動していくので、\textcolor{red}{極大値}\index{きょくだいち@極大値}
が複数あるときには、最大ではない極大値に
行き着くこともあります。
しかしながら、多くの場合には、最尤推定値に行き着くものと思っておくことにしましょう。

ここでは、アルゴリズムの動きを理解するにとどめることにします。
ここでは、４ハプロタイプに均一の頻度を与えて、それを初期値とします。
頻度の更新回数は1000回とします。
４ハプロタイプの頻度の初期値が与えられると、
観察ジェノタイプデータから、４ハプロタイプの本数を期待値を算出することになります。
今、９ジェノタイプのうち、２多型ともヘテロ型(Aa,Bb)である場合以外は、
2本のハプロタイプの内訳が確定します。
２座位がホモならば、同一のハプロタイプを２本、持ちますし、
１座位がホモでもう１座位がヘテロのときには、異なる２つのハプロタイプの対
であることが確定します。
ですから、その９ジェノタイプのうち８つについては、ハプロタイプ頻度の推定値とは
無関係にハプロタイプ本数が数えられます。
最後の１つのディプロタイプについては、(AB,ab)というハプロタイプの組み合わせか、(Ab,aB)という
組み合わせかの、いずれかです。
それぞれの組み合わせの起きる確率は、HWEを仮定すれば、\\
$f_1f_4, f_2f_3$ですので、$n_{22}$人のうち、$\frac{f_1f_4}{f_1f_4+f_2f_3}$は(AB,ab)であって、
残りは$(Ab,aB)$であるとします。
そうすると、今観察しているディプロタイプのデータに占める、４ハプロタイプの標本頻度が
得られます。
この頻度を、新たなハプロタイプ頻度として、暫定的に認めます。
後は、同じ処理を繰り返します。
この更新の手続きと、その経過を保管する処理をするＲのコマンドと、
尤度の増え方を、繰り返し処理の
はじめの20回について描いたものです。
確かに、尤度がだんだんに大きくなり、収束していく様子が見て取れます。
3,4回の繰り返し計算で、ほぼ頂上にたどり着いています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-019.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

\begin{lstlisting}
 EM2loci <- function(n, p = NULL, Niter = 1000) {
     if (is.null(p))         p <- rep(0.25, 4) #初期頻度を与えなければ、均等に割り当て
     f <- p
     rs <- rep(0, Niter) # 連鎖不平衡係数の収束ログ
     logLikes <- rep(0, Niter) # 対数尤度の収束ログ
     fs <- matrix(0, Niter, 4) # ハプロタイプ頻度の収束ログ
    # ハプロタイプ頻度によらず、確定するハプロタイプ本数
     fixed<- c(n[1, 1] * 2 + n[1, 2] + n[2, 1], n[1, 3] *  2 + n[1, 2] + n[2, 3], n[3, 1] * 2 + n[2, 1] + 
            n[3, 2], n[3, 3] * 2 + n[2, 3] + n[3, 2])
     for (i in 1:Niter) {
# 暫定ハプロタイプ頻度における２重ヘテロジェノタイプをf[1]-f[4]とf[2]-f[3]との２組に分配する比率
         tmpratio<-f[1]*f[4]/(f[1]*f[4]+f[2]*f[3]) 
         tmp <- rep(0,4)
         tmp[1] <- fixed[1] + n[2, 2] * tmpratio
         tmp[2] <- fixed[2] + n[2, 2] * (1 - tmpratio)
         tmp[3] <- fixed[3] + n[2, 2] * (1 - tmpratio)
         tmp[4] <- fixed[4] + n[2, 2] * tmpratio
# この回の更新結果を格納
         fs[i, ] <- tmp/sum(tmp); logLikes[i] <- CalcLike3x3(n, tmp); rs[i] <- CalcR(tmp)
         f <- tmp # 暫定ハプロタイプ頻度の更新
     }
     list(f = f, r = rs[Niter], logLike = logLikes[Niter], 
         fHsitory = fs, rHistory = rs, logLikeHistory = logLikes)
 }
 emout <- EM2loci(ns)
 maxp_A <- emout$f[1] + emout$f[3]; maxp_B <- emout$f[1] + emout$f[2]; maxr <- emout$r
plot(emout$logLikeHistory[1:20], type = "b") # 20回目までの対数尤度の更新履歴をプロット
\end{lstlisting}


\chapter{棄却と検定}
\section{信じるのが難しい仮説を棄却する 3カテゴリの観察}
データから、変数の期待値や最尤推定量、信頼区間などを推定しました。
こんどは、ある変数がある値であるという仮説を信じるべきか信じないべきか、
その程度を数値で表す、という話しです。

仮説を信じる程度が低いときに、その仮説をある基準で\textcolor{green}{棄却}
\index{ききゃく@棄却}する、と言うので、
ここの話しは、「仮説の棄却」の話しです。
３カテゴリについて、(10,7,3)と観測した場合を考えます。
３カテゴリが同一頻度だとしたときに、
N人が３カテゴリに$n_1,n_2,n_3$人に分かれて観察する確率は
\begin{equation*}
\frac{N!}{n_1!n_2!n_3!} (\frac{1}{3})^N
\end{equation*}
です。
20人を3カテゴリに分ける分け方のすべてについて、
その確率を計算して、(10,7,3)と観測した場合の確率とを比較して
色分けしたのが、図13.1の左図です。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-022.eps}
\caption{３要素の三角プロット。２０人の内訳が(20,0,0),(0,20,0),(0,0,20)のときが三角形の頂点。
左は、３要素の確率が等しいとしたときに(10,7,3)と観測するより高確率の領域が黒で、
低確率が淡い色。右は、３要素の確率が(0.9,0.05,0.05)とした仮説の下で
(10,7,3)と観測するより高確率が低確率かで色分けした}

 \end{center}
 \label{fig:one}
\end{figure}

すべての点の確率を足し合わせると１になっています。
赤は(10,7,3)と観測する確率以下の確率の場合です。
この部分の確率を足し合わせると、0.177ですから、
(10,7,3)と同程度か、それよりも珍しいことが起きる確率は、0.177です。
これを、３カテゴリが同頻度だという仮説のもとで、(10,7,3)を観察する珍しさであり、
この値が小さければ小さいほど、珍しく、
大きくなるとその最大値は１で、もっともありふれていることになります。
この数値が正確確率を基にしたｐ値です。
3カテゴリが同一頻度であるという仮説検定のp値です。
今、観察される珍しさが、低い方から
0.05に含まれれば、仮説を信じることをやめることにする、とするとき、
0.05を\textcolor{green}{棄却水準}\index{ききゃくすいじゅん@棄却水準}と言います。
そしてこのP値が0.05未満であれば、仮説が棄却された、と言います。
今回は、p=0.177ですから、棄却水準を0.05とすれば、棄却されないことがわかります。

今、３カテゴリの頻度が同じ場合について、計算しましたが、
異なる仮説についても同様に絵が描けますし、赤い部分を足し合わせる
ことができます。
もしも、３カテゴリの比率が、(0.9,0.05,0.05)だという仮説に立っていたら、
赤・黒のプロットは(図13.1右)のようになります。
通常は、このような、特別な比率の仮説が棄却されるかどうかは、問題にしませんが、
考え方としては同じことです。
\begin{lstlisting}
dpoly<-function(n=c(1,2,3),p=NULL){ # apply()関数で使うための関数を作る(確率計算)
  N<-sum(n)
  if(is.null(p)){
   p<-rep(1/length(n),length(n))
  }
  exp(lgamma(N+1)-sum(lgamma(n+1))+sum(n*log(p)))
}
DrawHigherLower<-function(g=c(10,7,3),p=NULL){
 N<-sum(g)
 x<-0:N;y<-0:N
 xy<-expand.grid(x,y) # x,yの全組み合わせを作る
 z<--(xy[,1]+xy[,2])+N
 xyz<-matrix(c(xy[,1],xy[,2],z),nrow=length(z))
 xyz<-xyz[apply(xyz,1,min)>=0,] # xyzのすべての要素は非負

 probs<-apply(xyz,1,dpoly,p=p) # xyzの各行(３数値)について、上で定義したdpoly()関数を適用する
 probObs<-dpoly(c(10,7,3)) # 観測データの確率
 higher<-which(probs>probObs) # 観測データの生起確率より大きい確率の場合
 lower<-which(probs<=probObs) # 観察データの生起確率以下の確率の場合
 xyz2<-apply(xyz,1,CoordTriangle)
 x2<-xyz2[1,];y2<-xyz2[2,]
xlim<-c(0,1);ylim<-c(0,2/sqrt(3))
# 高確率は黒で、それ以外は赤でプロット
 plot(xyz2[1,which(probs>probObs)]/N,xyz2[2,which(probs>probObs)]/N,xlim=xlim,ylim=ylim,col = "black",pch=15)
 par(new=T)
 plot(xyz2[1,which(probs<=probObs)]/N,xyz2[2,which(probs<=probObs)]/N,xlim=xlim,ylim=ylim,col = "red",pch=15)
 DrawTriangleFrame()

}
par(mfcol=c(1,2))
DrawHigherLower(c(10,7,3))
DrawHigherLower(c(10,7,3),c(0.9,0.05,0.05))
par(mfcol=c(1,1))
\end{lstlisting}

\section{分割表検定}

次に、２カテゴリについて、集団からケース(G1)とコントロール(G2)とを
サンプリングした場合を考えます。
観察データは$2\times 2$\textcolor{green}{分割表}\index{ぶんかつひょう@分割表}
として得られます。

\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a&計 \\ \hline
G1& $n_{11}=15$&$n_{12}=25$ &$n_{1.}=40$\\ \hline
G2&$n_{21}=15$&$n_{22}=45$&$n_{2.}=60$\\ \hline
計&$n_{.1}=30$&$n_{.2}=70$ &$n_{..}=100$\\ \hline
\end{tabular}

G1とG2が属する集団には、A,aの比率がありますが、それが、今わからないとします。
G1とG2のサンプルはどちらも、この集団からランダムに抜き取られたものであるという仮説
を立てます。
そして、その仮説に立ったときに、観察データがどれくらいありふれたものなのか、
珍しいものなのかを評価して、仮説を棄却するかどうかを考えてみます。
この集団の２カテゴリの比率は特定しませんが、変数を使って、$p,1-p$とします。
G1,G2ともに、この比率からのサンプリングですから、
表のようなサンプリングがなされる確率は
\begin{equation*}
\frac{n_{1.}!}{n_{11}! n_{12}!} \frac{n_{2.}!}{n_{21}!n_{22}!} p^{n_{.1}} (1-p)^{n_{.2}}
\end{equation*}
です。
今、$p$の値がなんであれ、ある値に確定しているとすれば、
与えられた周辺度数の条件のもとで$n_{11},n_{12},n_{21},n_{22}$と観察される確率は、
$\frac{n_{1.}!}{n_{11}! n_{12}!} \frac{n_{2.}!}{n_{21}!n_{22}!}$
に比例したものになります。
実際、
\begin{equation*}
\frac{n_{1.}!}{n_{11}! n_{12}!} \frac{n_{2.}!}{n_{21}!n_{22}!} \times \frac{n_{.1}! n_{.2}!}{n_{..}!}
\end{equation*}
としてやると、与えられた周辺度数のもとで、観察されうるすべての
$n_{ij}$の場合について足し合わせたときに１になるので、
$p$の値が何であれ、G1,G2が同じ集団からのサンプルであるという仮定のもとでの、
表の観察確率は、これになります。
これに基づいて、他の観察されうる表と観察の珍しさを比較してP値化すれば、
それが、この２ｘ２表において因子の独立に関するフィッシャーの正確確率検定となります。

さて、今、この表の周辺度数を満足する表を変数$\delta$を用いて、
期待値表から、$\delta$だけずれた表として表します。\\
\begin{tabular}[htb]{|c|c|c|c|} \hline
　&A&a&計 \\ \hline
G1& $n_{11}=n_{1.}n_{.1}/n_{..} +\delta $& $n_{12}=n_{1.}n_{.2}/n_{..} -\delta$ &$n_{1.}=40$\\ \hline
G2& $n_{21}=n_{2.}n_{.1}/n_{..} -\delta$ & $n_{22}=n_{2.}n_{.2}/n_{..} +\delta$ &$n_{2.}=60$\\ \hline
計&$n_{.1}=30$&$n_{.2}=70$ &$n_{..}=100$\\ \hline
\end{tabular}\\
この$\delta$を0から少しずつ増やしていきます。
すると
G1,G2が同じ集団からのサンプルだとした場合の生起確率は次の図の黒丸のように
だんだん小さくなります。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-024.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

この黒丸の作るカーブが何かしらの規則に乗っているかも知れないと予想してみます。
$\delta$の自乗に応じて小さくなっていくのではないかと当たりをつけてみます。
\begin{equation*}
a\times e^{-b \delta^2}
\end{equation*}
に近似できるかもしれないと思って Rの optim()関数
\footnote{
optim()関数は係数とともに指定した関数への最適化関数です。help(optim)で確認できる通り、
係数の値の求め方がいくつか実装されています。13章では、線形近似に使用しています
}
を用いて$a,b$の近似値を
求めてみます。そのようにして推定した関数の曲線が図13.2の赤い線です。
非常によく一致します。
つまり、２ｘ２表の表の生起確率は、期待値からのずれ$\delta$の２乗に応じて減じていくことがわかりました。
二乗で減じるのは、\textcolor{red}{正規分布}
\index{せいきぶんぷ@正規分布}・\textcolor{red}{カイ分布}
\index{カイぶんぷ@カイ分布}・
\textcolor{red}{カイ自乗分布}\index{カイじじょうぶんぷ@カイ自乗分布}の性質でした(8章)。
この性質から、２ｘ２分割表では期待値からのずれをカイ自乗分布で評価できるのです。
また、２ｘ２表は自由度が１の場合ですが、同様に、任意の自由度の表でも、
こうした対応関係があるので、
カイ自乗値を用いた検定(\textcolor{red}{ピアソンの独立性検定}\index{ピアソンのどくりつせいけんてい@
ピアソンの独立性検定})が有用となります。 
\begin{lstlisting}
# 期待値表を作る
makeExptable<-function(m = matrix(c(10, 20, 30, 40), nrow = 2)) {
 m1 <- apply(m, 1, sum);m2 <- apply(m, 2, sum); N <- sum(m);etable <- m1 %*% t(m2)/N
}
# 2x2表の第1セルを期待値から１ずつ動かして生起確率を返す
 ProbDistance <- function(m = matrix(c(10, 20, 30, 40), nrow = 2)) { 
# 周辺度数・期待値表を作る
   m1 <- apply(m, 1, sum)
   m2 <- apply(m, 2, sum)
   N<-sum(etable)
   etable <- m1 %*% t(m2)/N
   d <- seq(from = 0, to = 20, by = 1) #　期待値からのずれ
# 4セルとも正であるような２ｘ２表の４値を作る
    x <- d + etable[1, 1]; y <- -d + etable[1, 2]; z <- -d + etable[2, 1]; w <- d + etable[2, 2]
    mat <- matrix(c(x, y, z, w), ncol = 4);mins <- apply(mat, 1, min);matOK <- mat[mins > 0, ]
# 生起確率の対数の周辺度数部分
    tmp <- sum(lgamma(m1 + 1)) + sum(lgamma(m2 + 1)) - lgamma(N)
# 表ごとに正確生起確率を算出
    prob <- exp(-apply(lgamma(matOK+1),1,sum)+tmp)
    list(x = d[mins > 0], prob = prob) # xに期待値からのずれ、probに生起確率
}
d <- ProbDistance(m = matrix(c(10, 20, 30, 40), nrow = 2)) #c(10,20,30,40)の２ｘ２表について実施
ylim <- c(0, max(d$prob))
plot(d$x, d$prob, ylim = ylim)
# optim()関数を使って推定する関数を作る
fForOptim <- function(x) {
     a <- x[1]; b <- x[2] ;c<-x[3]# ２変数関数
# optim()関数が目指すのは、a * exp(-b*x^2) と観察値との距離の自乗を全観察点について合算したもの
     sum((d$prob - (a * exp(-b * d$x^2)))^2) 
}
optout <- optim(c(1, 1), fForOptim) # 推定する関数の係数の初期値は1,1から開始させる
xfine <- seq(from = min(d$x), to = max(d$x), by = 0.01) # 横軸の値
optprob <- optout$par[1] * exp(-optout$par[2] * xfine^2)# 推定関数の値
par(new=T) # 表の正確生起確率に推定関数を重ねて描く
plot(xfine, optprob, ylim = ylim, col = "red", type = "l")
\end{lstlisting}


\subsection{ピアソンの独立性検定 カイ自乗検定}
$N\times M$サイズの任意の表について
ピアソンの独立性検定(カイ自乗検定)が行えます。

観察表とその周辺度数から独立を仮定して作る期待値表との違いを
次の様に数値化します。

\begin{tabular}[htb]{|c|c|c|c|c|c|c|} \hline
　& $B_1$  & ... & $B_j$ & ... & $B_N$ &計 \\ \hline
$A_1$ & $n_{11}$  & ... & $n_{1j}$ & ... &$n_{1N}$ & $n_{1.}$ \\ \hline
... & ...  & ... & ... & ... & ... & ... \\ \hline
$A_i$ & $n_{i1}$  & ... & $n_{ij}$ & ... &$n_{iN}$ & $n_{i.}$ \\ \hline
... & ...  & ... & ... & ... & ... & ... \\ \hline
$A_M$ & $n_{M1}$  & ... & $n_{Mj}$ & ... &$n_{MN}$ & $n_{M.}$ \\ \hline
計& $n_{.1}$  & ... & $n_{.j}$ & ... &$n_{.N}$ & $n_{..}$ \\ \hline
\end{tabular}
\begin{tabular}[htb]{|c|c|c|c|c|c|c|} \hline
　& $B_1$ & ... & $B_j$ & ... & $B_N$ &計 \\ \hline
$A_1$ & $e_{11}$  & ... & $e_{1j}$ & ... &$e_{1N}$ & $n_{1.}$ \\ \hline
... & ... &...  & ... & ... & ... & ... \\ \hline
$A_i$ & $e_{i1}$  & ... & $e_{ij}$ & ... &$e_{iN}$ & $n_{i.}$ \\ \hline
... & ...  & ... & ... & ... & ... & ... \\ \hline
$A_M$ & $e_{M1}$  & ... & $e_{Mj}$ & ... &$e_{MN}$ & $n_{M.}$ \\ \hline
計& $n_{.1}$  & ... & $n_{.j}$ & ... &$n_{.N}$ & $n_{..}$ \\ \hline
\end{tabular}\\
$e_{ij}=\frac{n_{i.}n_{.j}}{n_{..}}$\\

独立性の検定のピアソンのカイ自乗値は次の式で与えられます。
\begin{equation*}
\chi^2=\sum_{i,j} \frac{(n_{ij}-e_{ij})^2}{e_{ij}}
\end{equation*}
縦軸と横軸が独立であるときに、
この値を観察する確率が自由度$(M-1)(N-1)$のカイ自乗分布の確率密度分布と
似ていることを利用してP値化します。
ソースから、検定の計算法を読み取ってください。\\
\begin{lstlisting}
chisqCalc<-function(m=matrix(c(10,20,30,40),nrow=2)){ # 行列を引数とする
 etable<-makeExptable(m) 
 chi2<-sum((m-etable)^2/etable) # 全セルについて、(観測値-期待値)^2/期待値
 df<-(length(m1)-1)*(length(m2)-1) # 自由度は (N-1)x(M-1)
 p<-pchisq(chi2,df,lower.tail=FALSE) # p値は、カイ自乗分布から得る
 list(chi2=chi2,p=p,df=df)
}
\end{lstlisting}
なお、この関数は、Rの関数"chisq.test(correct=FALSE)"と同じことです。
\footnote{
"correct=FALSE"とは\textcolor{green}{イェーツの連続性補正}
\index{イェーツのれんぞくせいほせい@イェーツの連続性補正}をしないというオプションです。
イェーツの連続性補正とは、セルの値が小さめのときに行うことが
推奨される補正で、カイ自乗値を小さめ(P値を大きめ)にします。
本書では、この補正については取り扱いません。
特定の条件の場合のみに補正を入れると、本書で行っている、方法間の
比較に際して、数値の整合性を悪くするなどの影響があるためです。
なお、補正が必要な場合には、正確確率検定によって、その代用をすることを薦めます。
}\\
\subsection{帰無仮説と最尤仮説を比較して統計量にする　尤度比検定}
前々項の正確確率検定では、G1,G2が同一集団からのサンプルであるという仮説の下での
表の観察確率を使って珍しさを算出しました。
今度は、その裏側、尤度を使います。
ある表が観察されたときに、G1,G2が、因子保有率が異なる
２集団からのサンプルであるという仮説に立ちます。
2集団の因子保有率には観察データから得られる\textcolor{red}{最尤推定値}
\index{さいゆうすいていち@最尤推定値}
$\frac{n_{11}}{n_{1.}},\frac{n_{21}}{n_{2.}}$を仮定して、
その仮定の下で、観察データが観察される確率をまず考えます。
\begin{align*}
L(G1\not = G2) &= \frac{n_{.1}!}{n_{11}!n_{21}!} \frac{n_{.2}!}{n_{12}!n_{22}!}
(\frac{n_{11}}{n_{1.}})^{n_{11}} (\frac{n_{12}}{n_{1.}})^{n_{12}}  (\frac{n_{21}}{n_{2.}})^{n_{21}} (\frac{n_{22}}{n_{2.}})^{n_{22}} \\
&=\frac{n_{.1}!}{n_{11}!n_{21}!} \frac{n_{.2}!}{n_{12}!n_{22}!}
\frac{ n_{11}^{n_{11}} n_{12}^{n_{12}} n_{21}^{n_{21}} n_{22}^{n_{22}} }
{n_{1.}^{n_{1.}} n_{2.}^{n_{2.}} }
\end{align*}

次に、G1,G2とが同一集団からのサンプルであるとしたときの、
その保有率の最尤推定値を仮定して、
その仮定の下で、観察データが観察される確率を考えます。
\begin{align*}
L(G1= G2) &= \frac{n_{.1}!}{n_{11}!n_{21}!} \frac{n_{.2}!}{n_{12}!n_{22}!}
(\frac{n_{.1}}{n_{..}})^{n_{11}+n_{21}} (\frac{n_{.2}}{n_{..}})^{n_{12}+n_{22}} \\
&= 
\frac{n_{.1}!}{n_{11}!n_{21}!} \frac{n_{.2}!}{n_{12}!n_{22}!} 
\frac{ n_{.1}^{n_{.1}} n_{.2}^{n_{.2}} }{n_{..}^{n_{..}} }
\end{align*}
G1,G2が同一集団からのサンプルであるとしたときの尤度の方が小さくなりますが、
その尤度の違いを数値にします。
小ささを測るのに、比をとることにします。
尤度の比なので、\textcolor{green}{尤度比}
\index{ゆうどひ@尤度比}と言います。
これは、G1,G2が異なる集団からのサンプルだという仮説を立てて、
もっとも尤度が高くなるようにした場合と、
G1,G2が同じ集団からのサンプルだという仮説とを比較したものです。

比を取ると階乗の部分はきれいになくなって、
\begin{equation*}
\frac{L(G1\not = G2) }{L(G1= G2) }=
\frac{n_{..}^{n_{..}}  n_{11}^{n_{11}} n_{12}^{n_{12}} n_{21}^{n_{21}} n_{22}^{n_{22}} }
{n_{1.}^{n_{1.}} n_{2.}^{n_{2.}}n_{.1}^{n_{.1}} n_{.2}^{n_{.2}}  }
\end{equation*}
実際、この式は、$n_{i.}=\sum_{j} n_{ij},n_{.j}=\sum_{i} n_{ij}$に注意して$N\times M$に一般化すると
\begin{equation*}
\frac{L(G1\not = G2) }{L(G1= G2) } = 
\prod_{i,j} (\frac{n_{ij}}{e_{ij}})^{n_{ij}}
\end{equation*}
となることが示せます。
そして、この値の対数の２倍が、
この表のピアソンのカイ自乗値とほぼ一致します。
したがって、この値(尤度比の対数の２倍)を用いても、
帰無仮説の棄却検定を行うことが出来ます。
\textcolor{green}{尤度比検定}
\index{ゆうどひけんてい@尤度比検定}と呼ばれる手法です。

\begin{lstlisting}
likelihoodRatioTest<-function(m=matrix(c(10,20,30,40),nrow=2)){
 etable<-makeExptable(m)
 chi2<-2*sum(log(m/etable)*m)
 df<-(length(m[,1])-1)*(length(m[1,])-1)
 p<-pchisq(chi2,df,lower.tail=FALSE)
 return(list(statistic=chi2,p.value=p,df=df))
}
\end{lstlisting}

\section{３つの検定方法の比較 正確確率検定　ピアソンの独立性検定　尤度比検定}
さて、同じ分割表に３種類の相互に似た結果を返す検定手法が３つ出てきました。
それぞれの特徴を知るために、比較してみることにします。
p値で較べます。
\subsection{サンプル数が小さいときと大きいとき}
\subsubsection{サンプル数が小さいとき}
サンプル数が小さいとき(3,6,9,12)と、大きいとき(100,200,300,400)の2通りで、
\textcolor{red}{ピアソンの独立性検定}\index{ピアソンのどくりつせいけんてい@ピアソンの独立性検定}、
\textcolor{red}{尤度比検定}\index{ゆうどひけんてい@尤度比検定}、
\textcolor{red}{フィッシャーの正確確率検定}\index{フィッシャーのせいかくかくりつけんてい@
フィッシャーの正確確率検定}の結果を比べて見ます。

まず、サンプル数が小さいときです。
図13.3を見てください。横軸に第１セルの値を、縦軸にp値をとりました。
縦軸を通常スケールにした左図では、
全般的に大き目の値が出ているのが、正確検定で、小さめな値が出ていて、
ほぼ一致しているのが、ピアソンの方法と尤度比検定です。
一方、縦軸を対数スケールにすると(右図)、横軸の両端(p値が小さい範囲)で、
尤度比検定が他の２法とずれている傾向があります。
この範囲ではピアソン法と正確法が近くなっています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-034.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-035.eps}
\caption{サンプル数が小さいとき。左は縦軸(Ｐ)が通常スケール、右は対数スケール}

 \end{center}
 \label{fig:one}
\end{figure}

このように、サンプル数が小さいときは、正確確率検定に較べて、
カイ自乗分布になぞらえた検定(ピアソンの方法と尤度比検定)は、Ｐ値が
小さめに出ます。ｐ値が小さく出るということは、
「統計的に有意」な判定を下しやすいということから、
好ましくありません(検定では、ｐ値を大きめに算出することを、
「\textcolor{green}{保守的}\index{ほしゅてき@保守的}」と呼んで尊重することが多いです)。
カイ自乗分布は整数にも実数にも対応した連続な分布であるのに、
観察は、整数であるために、離散的なデータとずれが生じます。
サンプル数が大きくなると、離散的とは言っても、かなりなめらかな動きになりますが、
サンプル数が小さいと、滑らかでなくなるために、連続なカイ自乗分布による近似がうまくいかなくなっている
わけです。
このことから、サンプル数が小さいときの、カイ自乗分布になぞらえた検定については、
解釈を慎重にすることが必要で、
正確検定を使うほうが適当である場合が生じます。
また、正確検定の値に近づくように補正する方法などが提唱されたりしています。
これは、信頼区間の推定方法として提案されていたいくつかの方法が、やはり、分布の連続性がもたらす不都合を
補正するものなのと同じことです。\footnote{
"correct=FALSE"でコメントしたイェーツの連続性補正は、サンプル数が小さいときにおきるこの影響の
補正法です}\\

\subsubsection{サンプル数が大きいとき}
次に、サンプル数を大きくした場合を見ます。
図13.4を見てください。
サンプル数が小さいときとの違いは、グラフが滑らかになっている点です。
それ以外の違いとしては、
縦軸が通常スケールのときには、３法でほとんど差がないことです。
このように、3種類のｐ値は、サンプルの数を大きくするとほとんど一致するほどに、
似てきます。
この、「完全には一致しないけれども、ほぼ、同じとみなせるくらいに似ていく」ということを
\textcolor{green}{漸近近似}
\index{ぜんきんきんじ@漸近近似}と言うので、カイ自乗検定は、漸近近似検定のひとつです。
一方、縦軸を対数スケールにすると、ｐ値が非常に小さい範囲でずれている様子がわかります。
ずれているのがピアソンの方法です。正確検定と尤度比検定は、ほぼ一致しています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-039.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-040.eps}
\caption{サンプル数が大きいとき。左が縦軸(Ｐ)の通常スケール、右が対数スケール}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{検定の対称性}
サンプル数が大きい場合の縦軸を対数スケールにした図13.4をもう一度見ることにしましょう。
ｐ値の対数をとった右図で、ｐ値が最大となるのは、期待値表(x=120)のときです。
そこから、xが100減ったときと100増えたときとを、縦線で示してあります。
x=120+100=220のときの、３つのｐ値はほぼ同じですが、その点を通る水平線を引くと、
ピアソンの方法のｐ値は、x=120-100=20のときに同じ値をとっていることがわかります。
このことから、ピアソンの方法のｐ値は左右対称であると知れます。
他方、尤度比検定と正確検定のｐ値は、それよりも小さい値であることがわかります。
この２つの手法は左右が非対称になっています。
サンプル数が大きく、期待値表からさして遠くない範囲では、この非対称性は問題になりませんが、
手法の性質として、対称性のありなしという違いがあることがわかります。

サンプル数が大きい場合の３方法のｐ値の一致の様子を対数スケールで
\textcolor{red}{散布図}\index{さんぷず@散布図}にしてみます。
尤度比検定と正確検定はほぼ完全一致している様子が見えますが、
ピアソンの方法は、その他２法との関係で、２本の線が現れます。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-042.eps}
\caption{pPearson's:ピアソンの方法のp、pLRT:尤度比検定のp、pFisher:フィッシャーの正確確率検定のp}

 \end{center}
 \label{fig:one}
\end{figure}

この２つの線のうちの片方は、期待表から、(１，１)セルが増える方向の表に由来し、
もう片方の線は、逆に減る方向の表に由来しています。

\subsection{有限な範囲と無限の広がりの違い}
対称性を確認した図13.4(右)を見ると、別のことにも気づきます。
ｘ軸の両端寄りの部分では、ピアソンの方法のｐ値に較べて、
正確確率検定と尤度比検定のｐ値が小さい傾向があります。
この傾向は、サンプル数が小さいときには、現われませんが、
サンプル数が大きくなると見えてきます。
これは、ピアソンの方法が、ｘの範囲を$-\infty \le x \le \infty$としているのに対して、
正確確率検定と尤度比検定では、図に表示したｘの範囲の外側は、
ありえない範囲(観察数が負になる、比率が負になる)としている、という違いに由来します。
ピアソンの方法では、ありえない領域にも小さいながらもある程度の確率を割り振り、
その分、期待値に近い領域の確率をほんのわずかずつですが小さめにしています。
そして、ありえない領域の確率は、観察データよりも珍しいとして、ｐ値の計算に足し合わせています。
ありえるけれども、期待値から遠いデータの場合には、このありえない確率の分が無視できない
大きさとなるために、ｐ値が大きめに出てくるわけです。
正確確率検定と尤度比検定ではありえない領域に確率をわりふらないので、この影響を受けていません。
この影響は、ｐ値が非常に小さい場合にのみ現われますが、検定方法の特性のひとつです。

\subsubsection{計算量の違い}
この他の大きな違いに、計算量の多寡があります。
正確確率検定は、すべての場合を数え上げるので、サンプル数の自由度乗のオーダーで計算量が増えます。
したがって、自由度が大きい場合には、非常に計算負荷が大きくなり実際的ではなくなります。
\subsubsection{計算量の違いのまとめ}
ここまでで見てきた３方法の特徴を表にします。\\

\begin{tabular}[htb]{|c|c|c|c|c|c|} \hline
方法&計算量&少サンプルのとき&対称性&漸近近似&連続・離散 \\ \hline
正確確率検定& 多&保守的・正確& 非対称&正確&離散 \\ \hline
ピアソンの方法 & 少 &正確検定との乖離が大きい& 対称 &漸近近似 &連続\\ \hline
尤度比検定& 少 &正確検定との乖離が大きい& 非対称 &漸近近似 &連続\\ \hline
\end{tabular}\\
きいです。


\section{仮説に制約を定めて検定する}

分割表について、行と列とが独立であるという仮説の棄却をする方法が
複数あることを述べてきました。

本項では、同一の分割表に対して、対立仮説の立て方にいろいろな制約を定めた上で、
検定することを考えます。
対立仮説に定める制約をモデルとも言いますから、
モデルと変数の関係、自由度について少し話しを進めます。

ある2アレル型座位で、次のような２ｘ３表が得られたとします。

\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&$AA$&$Aa$&$aa$&計 \\ \hline
ケース&$n_{12}=30$&$n_{11}=55$&$n_{10}=15$&$n_{1.}=100$ \\ \hline
コントロール&$n_{22}=16$&$n_{21}=48$&$n_{20}=36$&$n_{2.}=100$ \\ \hline
計&$n_{.2}=46$&$n_{.1}=103$&$n_{.0}=51$&$n_{..}=200$ \\ \hline
\end{tabular}\\


尤度比検定では、独立仮説と、データから最尤推定した変数を仮説として比較の相手としました。
尤度は、どんな仮説にも計算できますから、尤度比もさまざまな２つの仮説について計算することが可能です。
しばしば立てる仮説にどんなものがあるかを見てみることにします。
\begin{enumerate}
\item ジェノタイプとフェノタイプに関係がないという仮説(帰無仮説)
\item Aは\textcolor{red}{優性遺伝形式}\index{ゆうせいいでんけいしき@優性遺伝形式}でフェノタイプに影響しているというモデルに基づく仮説
\item Aは\textcolor{red}{劣性遺伝形式}\index{れっせいいでんけいしき@劣性遺伝形式}でフェノタイプに影響しているというモデルに基づく仮説
\item Aは\textcolor{red}{相加的遺伝形式}\index{そうかてきいでんけいしき@相加的遺伝形式}でフェノタイプに影響しているというモデルに基づく仮説
\item Aは優性遺伝形式・Additive model・劣性遺伝形式のいずれか１つで影響しているというモデルに基づく仮説\footnote{
３遺伝形式の検定統計量の最大値を採用する方法 MAX3 テストで検定する。RならRassocパッケージの"MAX3()"関数}
\item ジェノタイプとフェノタイプには、(どういう形式にしろ、何でも良いので)独立ではない関係があるという仮説
\end{enumerate}

３ジェノタイプのリスクが同一であるという仮説(仮説１）は、
ジェノタイプ頻度を決めるのに2(=3-1)変数、有病率を定めるのに1変数の合計
３変数で定めるモデル(自由度３)です。
仮説６は、ケースとコントロールの相対危険度を自由に決めるためのに
２変数増やして全部で変数５個を必要とするモデルです(自由度５)。
自由度が２つ増えました。
優性遺伝形式・劣性遺伝形式では、
ヘテロ接合体のＲＲがリスクホモ接合体と同じか、非リスクホモ接合体と
同じなので、自由度は１増えるだけで、４になります。
ヘテロ接合体のリスクが２つのホモ接合体のリスクの算術平均の場合(仮説５(相加的モデル)）は、
ヘテロ接合体のリスクを、２つのホモ接合体の「中間」に定めている仮説ですが、
この場合も、ヘテロ接合体のリスクが中間に固定されているので、自由度は４です。
仮説５では、３つの遺伝形式(優性・劣性・相加的)がありえるとするモデルですが、
以下の表のように、kという変数を用いて、ヘテロ接合体のリスクを表すことができて、
これによりある程度の自由があります。
しかしながら、どんな値でも取れる、というのに比べ、大幅に限定的ですから、
この場合は、自由な変数４個と、制約された変数１個となり、
全体の自由度は４より大きいですが、
５よりは小さいです。

\begin{tabular}[htb]{|c|c|c|c|c|c|c|} \hline
仮説&AAリスク&Aaリスク&自由な変数の数&限定的な変数の数 \\ \hline
１&0&0&3&0 \\ \hline
２& $1 < RR_{AA} \le \infty $ & $RR_{Aa}=RR_{AA}$ &4 &0\\ \hline
３& $1 < RR_{AA} \le \infty $ &1&4 &0\\ \hline
４& $1 < RR_{AA} \le \infty $ &$RR_{Aa}=\frac{1}{2}(RR_{AA}+1)$ &4 &0\\ \hline
５& $1 < RR_{AA} \le \infty $ &$RR_{Aa}= k (RR_{AA}-1)+1; k=\{0,0.5,1\}$ &4 &1\\ \hline
６& $0 \le RR_{AA} \le \infty $ & $0 \le RR_{Aa} \le \infty $ &5 &0\\ \hline
\end{tabular}\\
$f_{AA}+f{Aa}+f{aa}=1$\\

今、この６つの仮説から、2個を取り出して、それぞれの尤度を計算し、
それを比較することが可能です。
6仮説同士の比較を表にすると次のようになります。
表の左上から右下の対角線の上側には、仮説間の自由度の差が書き込まれ、
対角線の下側には、比較・検定手法の例が記されています。
仮説５との比較の場合には、不自由な変数の分の自由度が整数で表せないので、
その分を$\alpha$としてあります。

\begin{tabular}[htb]{|c|c|c|c|c|c|c|c|} \hline
仮説&１&２&３&４&５&６ \\ \hline
１& na&1&1&1&$1+\alpha$&2\\ \hline
２& 2x2表ピアソン(df=1) &na&0&0&$\alpha$&1\\ \hline
３& 2x2表ピアソン(df=1)  & LR比較 &na&0&$\alpha$&1\\ \hline
４& 2x3表傾向性検定(df=1) & LR比較 &LR比較  &na&$\alpha$&1\\ \hline
５& MAX3テスト& --& --& -- &na&$1-\alpha$\\ \hline
６& 2x3表ピアソン(df=2) & $LRT(df=1)$ & $LRT(df=1)$ & $LRT(df=1)$  & --&na \\ \hline
\end{tabular}\\

\subsection{一つの分割表にいろいろな検定を適用してみる}
仮説１との比較にあっては
ピアソンの独立性検定が適用できます。
$2 \times 2$表を自由度１のカイ自乗検定する場合と、
$2 \times 3$ 表を自由度２のカイ自乗検定する場合とがあります。
尤度比を用いて、自由度kのカイ自乗分布に従う統計量を算出して検定することもできます。
自由な変数の少ない方の仮説の棄却検定になります。
また、仮説５(相加的遺伝形式）の場合には、$2\times 3$表について、
３ジェノタイプに線形のリスクを想定して、
\textcolor{green}{傾向性の検定}\index{けいこうせいのけんてい@傾向性の検定}
を実施することと同じです。
特に、\textcolor{green}{Cockran-Armitage の傾向性検定}
\index{Cockran-Armitage のけいこうせいけんてい@Cockran-Armitageの傾向性検定}
と呼ぶこともあります。
優性・劣性・相加的形式は、傾向性の検定において、k=1,0,0.5という係数を
与えた傾向性検定と考えることもできて、以下のＲの処理では、Rassocパッケージの
"CATT()"にその係数を引数として
渡して算出させています。

いずれにしろ、モデルのおき方に合わせて、
手法を選んでいるということになります。
表で示した分割表の周辺度数が与えられているとします。
このとき、G1の３ジェノタイプのカウントを決めると、G2のカウントも決まります。
また、G1の３ジェノタイプカウントは、３カテゴリで自由度が２なので、
正三角形上の点として表せることになります(４章　正単体)。
そのような座標をとり、それぞれの表について、自由度２のピアソンの独立性検定を実施すると
そのp値が等しい表は楕円を描きます(図13.6左上)。

\begin{figure}[htbp]
 \begin{center}

   \includegraphics[width=50mm]{./Fig/PartIV-047.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-045.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-048.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-049.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-046.eps}
\caption{G1のジェノタイプカウントを正三角形座標上に表したもの。
左上、右上、左中、右中、下の順に
ピアソンの独立性検定(自由度２)、相加モデルコクランアーミテージ検定(自由度１)、優性検定、劣性検定、MAX3検定
のｐ値の常用対数x(-1)の等高線を描いた}

 \end{center}
 \label{fig:one}
\end{figure}
同様に、相加的モデル、優性モデル、劣性モデルのp値の等高線は直線を作ります。
MAX3テストは、３つのモデルを合わせたものなので、等高線も、この３つのモデルを合わせたものに
なっています(図13.6)。

このことからわかるのは、以下のことです。
\begin{itemize}
\item 周辺度数を共有する2x3表は、自由度２の平面に分布させることができて、
自由度２の検定は、そこに楕円を描くこと
\item その表について自由度１の検定をすることは、
直線状の等高線を引いてデータを評価すること
\item それ以外にも等高線の引き方が作れること
\item 楕円や直線の等高線を引く検定には、検定手法が用意されていること
\end{itemize}


\begin{lstlisting}
library(Rassoc)
# 2次元平面の座標から、観察テーブルを作成する
casecontRTheta<-function(R,t,af,f,N){
 popW<-c(af^2+af*(1-af)*f,2*af*(1-af)*(1-f),(1-af)^2+af*(1-af)*f)
 table<-matrix(c(popW*N[1],popW*N[2]),nrow=2,byrow=TRUE)
 table[1,1]<-table[1,1]-R/2*cos(t)+sqrt(3)/2*R*sin(t)
 table[2,1]<-table[2,1]+R/2*cos(t)-sqrt(3)/2*R*sin(t)
 table[1,2]<-table[1,2]+R*cos(t)
 table[2,2]<-table[2,2]-R*cos(t)
 table[1,3]<-table[1,3]-R/2*cos(t)-sqrt(3)/2*R*sin(t)
 table[2,3]<-table[2,3]+R/2*cos(t)+sqrt(3)/2*R*sin(t)
 return(table)
}
af<-0.4;f<-0;N-c(100,100) # allele frequency,HWE-f,sample size
x<-y<-seq(from=-10,to=10,by=1)
#2次元格子点のテーブルを作成し各種検定
cattp<-max3p<-df2p<-domp<-recp<-matrix(rep(0,length(x)*length(y)),nrow=length(x))
for(i in 1:length(x)){
 for(j in 1:length(y)){
  R<-sqrt(x[i]^2+y[j]^2)
  if(x[i]==0){
   t<-pi/2
  }else{
   t<-atan(y[j]/x[i])
  }

  popdata<-casecontRTheta(R=R,t=t,af=af,f=f,N=N)
  if(min(popdata>0)){
   catout<-CATT(popdata,x=0.5) # Cockran-Armitageの傾向性検定を係数0.5で実施
   domout<-CATT(popdata,x=1) # Cockran-Armitageの傾向性検定を係数1で実施
   recout<-CATT(popdata,x=0) # Cockran-Armitageの傾向性検定を係数0で実施
   max3out<-MAX3(popdata)
   df2chi2out<-chisq.test(popdata,2)
   cattp[i,j]<-catout$p
   max3p[i,j]<-max3out$p
   df2p[i,j]<-df2chi2out$p.value
   domp[i,j]<-domout$p
   recp[i,j]<-recout$p
  }
 }
}
filled.contour(x,y,-log(cattp,10),color=terrain.colors)
filled.contour(x,y,-log(max3p,10),color=terrain.colors)
filled.contour(x,y,-log(df2p,10),color=terrain.colors)
filled.contour(x,y,-log(domp,10),color=terrain.colors)
filled.contour(x,y,-log(recp,10),color=terrain.colors)
\end{lstlisting}

\subsection{離散的な仮説空間での尤度比の比較}

仮説の比較方法の表には、２つの仮説しかありえなくて、そのどちらかの判断を
するような場合もあります。
親子の判定(親子なのかそうでないのか）はこの部類に入ります。
これらの場合には、次のような検定になるでしょう。
２つの仮説だけが、「仮説の空間」です。
したがって、２つの仮説から算出される尤度(L(h1)+L(h2))を足し合わせたものが、仮説空間の全体
にわたる尤度の和です。
したがって、２つの尤度の和に占める、それぞれの仮説の尤度の割合($\frac{L(hi)}{L(h1)+L(h2)}; i=1,2$)が
仮説$hi$を信じるべき程度になります。
特に、確率分布などを用いる必要のない判断になります。

\subsection{検定同士の非独立な関係}
さて、前節では、１つの$2\times 3$表にいろいろな仮説を立てて、
いろいろな較べ方をしてみました。
自由度１の検定が３種類(優性・劣性・相加的)、自由度２の検定(ピアソンの独立性検定)が１種類ありました。
これらのｐ値は、相互に独立ではありません。
その様子を見てみることにします。適当に表を作り、その検定結果を比較してみます。

まず、独立な２つの検定があったとき、その２つの検定のp値には、全く相関がありません。
そのようなときにペアとなるp値の\textcolor{red}{散布図}
\index{さんぷず@散布図}は図13.7のように、ばらばらになります。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-051.eps}
   \includegraphics[width=50mm]{./Fig/PartIV-051-2.eps}
\caption{左は独立な２検定のｐ値の散布図。右は、$2\times 3$表に自由度２の独立性検定と３種類の
遺伝形式モデルに関する自由度１の検定を実施したときの、４検定(chiP:ピアソンの方法,domP:優性モデル,
cattp:相加的モデル,recP:劣性モデル)のｐ値の散布図}

 \end{center}
 \label{fig:one}
\end{figure}


\begin{lstlisting}
library(Rassoc)
Niter<-1000
Nca<-1000
Nco<-1000
chiP<-cattP<-domP<-recP<-rep(0,Niter)
for(i in 1:Niter){
 af<-runif(1)*0.6+0.2
 df<-c(af^2,2*af*(1-af),(1-af)^2)
 case<-sample(c(0,1,2),Nca,df,replace=TRUE)
 cont<-sample(c(0,1,2),Nco,df,replace=TRUE)
 m<-matrix(c(length(which(case==0)),length(which(case==1)),length(which(case==2)),length(which(cont==0)),length(which(cont==1)),length(which(cont==2))),nrow=2,byrow=TRUE)
 chiP[i]<-chisq.test(m,correct=FALSE)$p.value
 cattP[i]<-CATT(m,0.5)$p
 domP[i]<-CATT(m,1)$p
 recP[i]<-CATT(m,0)$p
}
plot(as.data.frame(cbind(chiP, domP, cattP, recP)), cex = 0.1)
cormat <- cor(cbind(chiP, domP, cattP, recP))
\end{lstlisting}


p値間の\textcolor{red}{相関}\index{そうかん@相関}を調べて、数値で表せば次のようになり、関連があることがわかります。
\begin{screen}
\begin{Schunk}
\begin{Sinput}
> cormat
\end{Sinput}
\begin{Soutput}
           chiP       domP     cattP       recP
chiP  1.0000000 0.63872128 0.6711991 0.63932199
domP  0.6387213 1.00000000 0.5568540 0.04097478
cattP 0.6711991 0.55685396 1.0000000 0.53840145
recP  0.6393220 0.04097478 0.5384015 1.00000000
\end{Soutput}
\end{Schunk}
\end{screen}

優性・相加的・劣性の３検定の間では、優性と劣性とが比較的独立性が強いことがわかります。
これは、優性と劣性のモデル同士が、優性と相加的のそれよりも「遠い」関係だからです。
ピアソンの方法と３つの自由度１の検定との相関は同程度であることもわかります。

このように独立でない検定を複数、行ったときには、
個々の検定の結果の解釈には注意が必要です。
このことは、17章の\textcolor{red}{多重検定}\index{たじゅうけんてい@多重検定}
のところで扱います。

\section{表のサイズを変える}
\subsection{表形式のデータ}
表というのは、行と列に２つの変数をあてはめたデータの表現方法のひとつです。
あるサンプルセットがあり、そのサンプルが、２つの変数に関するデータを持っているときに、
作れるものです。
そして、サンプルを通じた２変数の関係について検討することができます。
本項では、この２つの尺度に関するデータについて、\textcolor{red}{データ型}\index{データがた@データ型}
ごとにどのような
対応をするのかについてみていきます。

量的データに特定の分布があることを仮定しないタイプの手法が対応します。
\footnote{量的変数に関しては、その分布の様子などにより、適切な解析手法が分かれますが、
本書では、カテゴリカルなデータ型からの延長として量的尺度を眺めることに重点を置き、
その点についてはあえて触れません}
分布を仮定する手法を\textcolor{green}{パラメトリック}
\index{パラメトリック@パラメトリック}mendex -r -c -g -s line.ist -p any PartxI-VI-noFigs20100507.idx

platex PartxI-VI-noFigs20100507.tex
、仮定しない手法を\textcolor{green}{ノンパラメトリック}\index{ノンパラメトリック@
ノンパラメトリック}と呼びますので、
基本的にはノンパラメトリックな手法を中心とした話しになっています。

\subsection{順序のありなしと検定手法}
\subsubsection{片方の軸が２カテゴリの場合}
\textcolor{red}{ピアソンの独立性検定}
\index{ピアソンのどくりつせいけんてい@ピアソンの独立性検定}は、順序なしのカテゴリ$\times $順序なしのカテゴリの手法です。
\textcolor{red}{傾向性の検定}
\index{けいこうせいのけんてい@傾向性の検定}は、片方の軸が２カテゴリであるときに、もう片方の軸に順序のあるカテゴリであるときの検定です。
順序のあるカテゴリに０，１，２、。。。という重みをつけるのが、もっとも単純なやり方ですが、
重み付けにカーブをかけたりすることもできます。
遺伝形式の場合で言えば、0,1,1が優性モデル、0,0.5,1が相加的モデルで「もっとも単純なやり方」、0,0,1が劣性モデルです。
このように重みをつけるのは、「線形な関係」に持ち込むことですが、
重みを忘れて、ただ単に、値の大小関係のみを使うこともできます。値に応じて、順位をつけるのです。
片方の軸が２カテゴリでもう1つの軸に、大小を考慮して順位をつけるのは、
\textcolor{red}{順位和検定}\index{じゅんいわけんてい@順位和検定}です。
順序の方のカテゴリ数が、どんどん増えて、同じカテゴリに含まれるサンプルが１つもないような
状態にまでなると、サンプルの数だけカテゴリ数があるような状態になります。
このときに、サンプルの値の大小順に順序を与えて検定するのが、順位和検定です。
量的データ型のときも、すべてのサンプルが異なる値を持っていて、その値に順序がつけられますから、
同様に順位和検定ができます。
片方の軸が順序のない３以上カテゴリで、もう片方の軸に順序がある場合には、
\textcolor{green}{クラスカル-ウォリス検定}\index{クラスカル-ウォリスけんてい@
クラスカル-ウォリス検定}が使えます。
２カテゴリは順位ありとみなせますから、２カテゴリ対順序のない３以上カテゴリの場合にも、
これを適用することが可能です。
\textcolor{green}{線形回帰}\index{せんけいかいき@線形回帰}を適用してしまうことも可能です。
この場合は、「線形」の回帰ですから、同じく「線形」を仮定している傾向性検定と似た挙動をとります。
\subsubsection{片方の軸が順序なしの３以上カテゴリの場合}
片方の軸が３軸以上のカテゴリで順序なしになった場合には、もう片方の軸が順序なしであれば、
ピアソンの独立性検定になります。
もう片方の軸が順序ありの場合には、クラスカル-ウォリスの検定があります。
片方が３以上カテゴリで順序がないときには、もう片方の軸が量的形質になっても、
同じくクラスカル-ウォリスの検定が
適用できます。
\subsubsection{両軸に順序がある場合}
片方の軸が順序ありのカテゴリになり、もう片方の軸も順序がある場合には、
\textcolor{green}{Jonckheere-Terpstra検定}\index{Jonckheere-Terpstraけんてい@
Jonckheere-Terpstra検定}が使えます。
この検定も、値の大小関係を用いた処理をしているので、
3以上の順序ありカテゴリも量的データの場合も同様に使えます。
両方の軸が連続な場合も順序ありという意味では同じです。
したがって、この場合もJonckheere-Terpstraが使えます。
また、線形回帰も使えます。

\begin{tabular}[htb]{|c|c|c|c|c|} \hline
　&２カテゴリ&３以上順序なしカテゴリ&３以上順序ありカテゴリ & 順序あり連続\\ \hline
２カテゴリ& ピアソン$\chi^2$ & ピアソン$\chi^2$ & 順位和検定・傾向性の検定 & Kruskal-Wallis\\ \hline
３以上順序なしカテゴリ& -&ピアソン$\chi^2$  & Kruskal-Wallis & Kruskal-Wallis\\ \hline
３以上順序ありカテゴリ& -& -& Jonckheere-Terpstra & Jonckheere-Terpstra \\ \hline
順序あり連続& -& -& -& Jonckheere-Terpstra /(線形)回帰\\ \hline
\end{tabular}

\subsection{複数の手法の挙動の比較}
あるデータ型のときにある手法が「使えます」と述べてきましたが、
それは、データの処理がそのデータ型を受け付けるという意味です。
複数の方法が使えるデータ型のときには、それぞれの結果が異なりますから、その
違いの由来が何なのかを意識する必要があります。

さて、２カテゴリ$\times $ カテゴリの場合は、順序ありとも順序なしともみなせる軸の組み合わせなので、
上述したすべての検定を実行することが可能で、それらは、ほぼ同じ結果をもたらします。
他との一致が多少なりとも悪いのは
Jonckheere-Terpstraですが、これは、軸が高次になったときなどにうまくいくように工夫された計算式を
使っている分、単純な場合にしわ寄せが来ていると思えばよいでしょう。(図13.8)

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIV-057.eps}
\caption{$2\times 2$表の場合。上から、ピアソンの検定、クラスカル-ウォリス、Jonckheere-Terpstra、
線形回帰、傾向性検定}

 \end{center}
 \label{fig:one}
\end{figure}

次に$2\times 3$ 表にします。その結果が図13.9です。
ピアソンの独立性検定は、自由度２ですから、その他の検定(自由度１）とは
異質です。
それ以外の検定は、クラスカル-ウォリスもYonckheere-Terpstraも傾向性検定も線形回帰
も大体同じです。
特に、傾向性検定と線形回帰はどちらも、線形を仮定した検定ですから、計算誤差を除いて
完全に一致します。(図13.9)
\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIV-058.eps}
\caption{$2\times 3$ 表の場合}

 \end{center}
 \label{fig:one}
\end{figure}

次に、片方の軸を２カテゴリにしたまま、もう片方のカテゴリ数を10まで増やしますと、
ピアソンとそれ以外の検定は、ほとんど独立になります。
しかしながら、それ以外は、おおまかに同じです。
傾向性の検定と線形回帰の一致は相変わらずよいです。(図13.10)

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIV-059.eps}
\caption{$2\times 10$ 表の場合}

 \end{center}
 \label{fig:one}
\end{figure}

今度は、
$3 \times 3$ 表にします。
すると、傾向性の検定を除く４つの検定手法は３群に分かれます。
両方の軸に順序のないピアソンの方法、片方にのみ順序のないKruskal-Wallis、
両方に順序のあるYonckheere-Tersptra,線形回帰の３群です(図13.11)。

両軸に順序のある２方法はよく似ています。
Kruskal-Wallisはピアソンと線形２手法の中間に位置しますが、プロットの似方も
ピアソン　＞　Kruskal-Wallis ＞　線形２法
という傾向があることが見て取れます。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartIV-060.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

\begin{lstlisting}
CompareTests<-function(N=2,M=2,Niter=1000,Ns=100,k1=10,k2=3){
library(MCMCpack)
library(clinfun)
pearsonp<-trendp<-kwp<-jtp<-lmp<-rep(0,Niter)
for(i in 1:Niter){
# $N\times M$ テーブルをランダムに作る
 fn<-rdirichlet(1,rep(k1,N))
 fm<-rdirichlet(1,rep(k2,M))
 first<-sample(1:N,Ns,prob=fn,replace=TRUE)
 second<-sample(1:M,Ns,prob=fm,replace=TRUE)
 t<-table(first,second)
 pearsonp[i]<-chisq.test(t,correct=FALSE)$p.value
 if(N==2){
  trendp[i]<-prop.trend.test(t[1,],t[1,]+t[2,],score=1:M)$p.value
 }
 kwp[i]<-kruskal.test(second~first)$p.value
 jtp[i]<-jonckheere.test(second,first,alternative="two.sided")$p.value
 lmp[i]<-summary(lm(second~first))$coefficients[2,4]
}
databind<-cbind(pearsonp,kwp,jtp,lmp,trendp)
plot(as.data.frame(databind))
return(databind)
}
N<-2;M<-2;Niter<-100;Ns<-1000 # 実施条件
ctout22<-CompareTests(N,M,Niter,Ns)
N<-2;M<-3;Niter<-100;Ns<-1000 # 実施条件
ctout22<-CompareTests(N,M,Niter,Ns)
N<-2;M<-10;Niter<-100;Ns<-1000 # 実施条件
ctout22<-CompareTests(N,M,Niter,Ns)
N<-3;M<-3;Niter<-100;Ns<-1000 # 実施条件
ctout22<-CompareTests(N,M,Niter,Ns)
\end{lstlisting}

\chapter{関係と因果}
\section{原因と結果と時間}
何かを調べるとき、何かの原因が知りたい、ということが多いです。
「AならばBである」です。
しかしながら、データから得られる情報は、「AとBとは関係がある」であることが多いです。
データは「関連」について教えてくれるが、その「\textcolor{green}{因果関係}
\index{いんがかんけい@因果関係}」について教えてくれるわけではないです。
「因果関係」を知ることは、「関係」を知ることとは異なることなので、
そのための工夫が必要です。

\textcolor{green}{疫学}\index{えきがく@疫学}という学問分野は、まさに、因果関係のありなしについて研究しているので、
因果関係の妥当性に関する体系的理解は、そちらを参照していただくこととしますが、その中で、
遺伝子の解析に関係の深い点である、「時間」の要素を取り上げます。

「AとBとに関係がある」ときに\\
\begin{itemize}
\item AがBの原因である場合
\item BがAの原因である場合
\item AとBとが、ともに、Cを原因とする結果である場合
\item AとBとが、ともに、Cの原因である場合
\end{itemize}
の４パターンが考えられるでしょう。
図で表わすと図14.1のようになります。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/4-1.eps}

  %\includegraphics{./Figures/Ch8/CauseEffect/CauseEffect.eps}
\caption{AとBとに関連があるときの因果関係の色々左上：AがBの原因、右上：BがAの原因、
左下：CがAとBの原因、右下：AとBとがCの原因}
 \end{center}
\end{figure}

今、「A」が「B」より「時間的」に先に存在しているときを考えます。
その\textcolor{red}{時間}\index{じかん@時間}的な前後関係がわかれば、
4つの場合のうち、
「BがAの原因である場合」の可能性がなくなります。
残りの３つの場合は相変わらず残ります。

研究のスタイルを、\textcolor{green}{後ろ向き研究}
\index{うしろむきけんきゅう@後ろ向き研究}と
\textcolor{green}{前向き研究}\index{まえむきけんきゅう@前向き研究}とに２分するのも、「因果関係」の観点から言うと、
「時間」の要素をどのように取り扱うという点で大きく異なるからです。
「結果と目をつけている」因子が、研究開始時点では確かに存在しなかったのに、研究を進めていくうちに、確かに
存在するようになったことを、前向き研究は「確認」することができます。

多くの実験研究は前向き研究です。
「原因と目をつけている」因子のありなしを操作して、「結果と目する」因子がどのよう影響を受けるかを
観察するように実験を組み立てるのが普通だからです。

\section{原因としてのジェノタイプ}
遺伝子の場合はどうでしょうか？
ジェノタイプとフェノタイプの区別をするときに、
「染色体のうち、変化しない部分(塩基配列)」をジェノタイプとし、
それ以外の化学修飾等は、RNA、タンパク質、形質一般とともに、フェノタイプとすることを
４章で述べました(図4.1)。
このようにして定義したジェノタイプは、細胞の誕生時点・個体の誕生時点ですでに存在しており、
細胞・個体に起きる各種の現象は、ジェノタイプの確定より後に起きますから、
ジェノタイプとその他すべてのフェノタイプについての解析に限ると、
時間的関係を限定した解析として扱えることになります。
この点が、統計遺伝学に特化した点の１つといえるでしょう。

\section{有向グラフ・ベイジアンネットワーク}
因果関係には、時間的順序が大事な要素であることを説明したときに、
図14.1にある通り、向きのある辺を持つ\textcolor{red}{グラフ}
\index{グラフ@グラフ}（\textcolor{red}{有向グラフ}
\index{ゆうこうグラフ@有向グラフ})で説明しました。
時間という軸が持つ、１方向性の情報をグラフでは矢印の向きとして取り扱えます。
また、10章で\textcolor{red}{マルコフ連鎖}\index{マルコフれんさ@マルコフ連鎖}
のことに触れました。
マルコフ連鎖では、ある時点の様子が、その直前(もしくは数段階前まで)の状態によって確率的に決まっていました。
マルコフ連鎖の状態と状態の間には、データが支持する「関係」があります。
その「関係」がどちらを向いているかは、両者の「関係」のみに着目していては、決まりません。
なんらかの理由で、向きを与えると、その関係は、
ある状態から次の状態への\textcolor{red}{推移確率}\index{すいいかくりつ@推移確率}になります。
前の状態が確率的に表され、それに基づいて、次の状態が確率的に決められていました。

今、有向グラフに着目して、ある点の様子が、
その点に向かう辺の出所の点の状態によって確率的に
決まると考えれば、直線上ではないけれども、似たような取り扱いが可能です。
有向グラフであって、どの点から出発しても自身の点に戻って来られないようなグラフを
\textcolor{green}{非循環有向グラフ}
\index{ひじゅんかんゆうこうぐらふ@非循環有向グラフ}
\footnote{
非循環有効グラフのパターン数は15章15.3で扱います}
と言いますが、
要因とその因果関係をこの非循環有向グラフを用いて表し、
観察データを最もよく説明するグラフを推定する手法に
\textcolor{green}{ベイジアンネットワーク}
\index{ベイジアンネットワーク@ベイジアンネットワーク}があります。
以下では、その概略をRのbnlearnパッケージを用いて試してみます。
まず、非循環有向グラフを描いてみます。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-066.eps}
\caption{超点数5のグラフ。相互に結ばれて三角形を作る3点がありますが、辺の
向きに注意すると、非循環であることがわかります}

 \end{center}
 \label{fig:one}
\end{figure}

図14.2のグラフを見てください。
これは、点が5個のグラフで、辺に向きがありますから、有向グラフです。
ACEに三角形ができていますが、矢印の向きに注意すると、ぐるぐると回ることができないので、
これは、「非循環」です。

次に、適当に依存関係のある５要素のデータを作ってみます。
5個の頂点のある有向グラフは、$5\times 4/2=10$本の辺を引くことが可能です。
10本の辺のそれぞれに線を引くか引かないか、引くとして向きをどうするかの３つの場合がありますから、
$3^10=59049$通りの辺の引き方があります。
頂点の数が６になっただけで、この数は10000000を越えます。
非循環という制約をつけるので、これより減りますが、非常に多いことは明らかです。
したがって、たくさんのグラフパターンの中から、特定のグラフパターンを選び出す手順が
必要です。
bnlearnパッケージにも複数のアルゴリズムが実装されていますが、その1つで
グラフパターンを選択してみます。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-068.eps}
\caption{推定されたネットワークグラフ}

 \end{center}
 \label{fig:one}
\end{figure}

この選ばれたグラフパターンが図14.3です。
このパターンが他のパターンよりもデータに即して優れていることを確認する
ために、すべてのグラフパターンと比較することは出来ませんから、
いくつかを抜き出して比較してみましょう。
まず、ランダムにグラフパターンを作ってみて、それを図示してみます。(図14.4)

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=25mm]{./Fig/PartIV-069.eps}
   \includegraphics[width=25mm]{./Fig/PartIV-070.eps}
   \includegraphics[width=25mm]{./Fig/PartIV-071.eps}
   \includegraphics[width=25mm]{./Fig/PartIV-072.eps}
\caption{ランダムに作成した４個のネットワークグラフ}
 \end{center}
\end{figure}

いろいろなパターンが作れることがわかりましたので、100個ほど
作成して、それをあるアルゴリズム\footnote{Grow-Shrinkアルゴリズムを使いました}が選択したグラフと比較してみます。
比較するためには、なんらかの優劣の判定方法が必要ですが、そのための関数("score()")
もあります。
推定グラフのスコアを水平線で、ランダムに作ったグラフのスコアを点で示したのが図14.5です。
100個のランダムなグラフパターンの得点が、
推定パターンの得点(水平線)より小さいことが見て取れます。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartIV-073.eps}
\caption{推定ネットワークグラフの評価点(上部の水平線)は、
ランダムに作ったグラフの評価点(ドット)よりも高い}
 \end{center}
\end{figure}

このようにして因果関係を表す非循環有向グラフを選んだ後に、
有向辺に、要素間の関係の値を与えることで、選んだグラフにさらにモデルとしての意味づけをすることができます。
\begin{lstlisting}
library(bnlearn)
e <- empty.graph(LETTERS[1:5]) # 頂点数5の辺のないグラフ
arc.set <- matrix(c("A", "C", "B", "D", "C", "E","D","A","A","E"), ncol = 2, byrow = TRUE)
arcs(e)<-arc.set # 辺を与える
plot(e)
# 適当に関連のある５項目のデータを作る
 set.seed(123456) 
Ns<-10000
A<-rpois(Ns,3);B<-rpois(Ns,3);C<-rpois(Ns,3);D<-rpois(Ns,3);E<-rpois(Ns,3)

B[1:(Ns/4)]<-A[1:(Ns/4)];C[(1+Ns/4):(Ns/2)]<-A[(1+Ns/4):(Ns/2)]
D[(1+Ns/2):(3*Ns/4)]<-A[(1+Ns/2):(3*Ns/4)];E[(1+3*Ns/4):Ns]<-A[(1+3*Ns/4):Ns]
d<-as.data.frame(cbind(A,B,C,D,E))
res<-gs(d) # 手法の１つであるGrow-Shrink法を用いてネットワークを推定する
plot(res)
# 頂点数５のグラフをランダムに作ってプロットしてみる
plot(random.graph(LETTERS[1:5], num = 1))
# 推定されたグラフとランダムに作成したグラフとで、
# データの説明の良さを評価して比較してみる
GSscore<-score(res, d) # 推定ネットワークグラフの評価
Niter<-100 # ランダムに作るグラフの数
scores<-rep(0,Niter) # ランダムに作ったネットワークの評価
for(i in 1:Niter){
 randomGraph<-random.graph(LETTERS[1:5], num = 1)
 scores[i]<-score(randomGraph,d)
}
ylim<-c(min(scores,GSscore),max(scores,GSscore))
plot(scores,ylim=ylim,pch=20)
abline(h=GSscore)
bn.fit(res,d) # 推定結果の表示
\end{lstlisting}

\part{大規模なこと}
\chapter{数え上げる}
\section{順列・重複順列・分割表の正確生起確率}
ゲノムをはじめとする「オーム」研究は、そもそも、多くの要素を全体としてとらえることに
主眼を置いた研究なので、取り扱い要素が非常に多いです。
ヒトゲノムは30億塩基対、ヒトのコーディング遺伝子数は２−３万です。
さらに、要素間の関係を調べたり、組み合わせについて調べることも多いです。
また、木グラフやネットワークを使いますから、俄然、場合の数が大きくなります。
まずは、場合の数に関してまとめましょう。
\subsubsection{順列と組み合わせ}
N個の要素があるときに、k個を抜き出して並べる並べ方を
\textcolor{green}{順列}\index{じゅんれつ@順列}と言います。
順列の場合の数$P(N,k)$は次の式で表されます。
\textcolor{red}{ガンマ関数}\index{ガンマかんすう@ガンマ関数}も使います
(図15.1上)。
\begin{equation*}
P(N,k)=N(N-1)(N-2)...(N-(k-1))=\frac{N!}{(N-k)!}=\frac{\Gamma(N+1)}{\Gamma(N-k+1)}
\end{equation*}
k個のうち、1個目はN個の要素から選ぶことができて、2個目は残っているN-1個から、3個目はN-2個から、
というような選び方なので、この式となります。
\begin{lstlisting}
permN<-function(N=10,k=3){ exp(lgamma(N+1)-lgamma((N-k)+1)) }
\end{lstlisting}


\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/5-1.eps}
  %\includegraphics{./Figures/Ch13/PermRepPerm/PermRepPerm.eps}
\caption{}
 \end{center}
\end{figure}
N個異なった要素から、重複を許して$k$個を並べる並べ方を重複順列と呼びます。
こちらは単純で、$k$回とも、$N$個のどれかをあてはめられるので、
\begin{equation*}
\Pi(N,k)=N^k
\end{equation*}
です(図15.1下)。
\begin{lstlisting}
repPermN<-function(N=10,k=3){ N^k }
\end{lstlisting}
選び出す要素ごとに、個数を指定して、全部で$k$個にして並べる並べ方は、
\begin{equation*}
\frac{k!}{\prod_{i=1}^N n_i!};\;\; \sum_{i=1}^N n_i=k
\end{equation*}
です。
これは、$k$個の要素に、N種類のラベルをラベルごとの枚数を指定でラベル付けする
場合の数です。
NxMの分割表で考えます。

今、総数$n_{..}$に、２通りのラベルを貼ります。
N種類である1番目のラベルが$n_{i.};i=1,2,...,N$枚ずつあり、その貼り方の場合の数は
\begin{equation*}
\frac{n_{..}!}{\prod_{i=1}^N n_{i.}!}
\end{equation*}
M種類である２番目のラベルが$n_{.j};j=1,2,...,M$枚ずつあり、その貼り方の場合の数は
\begin{equation*}
\frac{n_{..}!}{\prod_{j=1}^M n_{.j}!}
\end{equation*}
です。
２つのラベルは、それぞれ、お互いに関係なく貼ることができるとき、
その貼り方は２つの貼り方の積なので、
\begin{equation*}
X= \frac{n_{..}!}{\prod_{i=1}^N n_{i.}!} \times \frac{n_{..}!}{\prod_{j=1}^M n_{.j}!}
\end{equation*}
です。
一方、総数$n_{..}$にNxM種類のラベルを１つだけ貼る場合を考えます。
各ラベルの枚数が$n_{ij};i=1,2,...,N,j=1,2,...,M$とすると
\begin{equation*}
Y=\frac{n_{..}!}{ \prod_{i=1}^N\prod_{j=1}^M n_{ij}!}
\end{equation*}
です。
\begin{equation*}
\frac{Y}{X}=\frac{n_{..}!}{ \prod_{i=1}^N\prod_{j=1}^M n_{ij}!}  \frac {\prod_{i=1}^N n_{i.}!} {n_{..}!} \times \frac{\prod_{j=1}^M n_{.j}!}{n_{..}!} =
\frac{\prod_{i=1}^N n_{i.}! \prod_{j=1}^M n_{.j}!}{n_{..}! \prod_{i=1}^N \prod_{j=1}^M n_{ij}!}
\end{equation*}
が、2軸が独立と仮定したときに表を観察する\textcolor{green}{正確生起確率}
\index{せいかくせいきかくりつ@正確生起確率}になります。
この計算は13章(13.2)での計算式と同じです。

正確生起確率と言うのは、すべてのラベル付けの場合の数(順列)のすべて数え上げているのですが、
本当に数え上げると面倒くさいので、同じラベルの枚数に関する考慮をしていることがわかります。

\subsection{組み合わせ・重複組み合わせ・２倍体ジェノタイプの種類数}
順列と似たものに\textcolor{green}{組み合わせ}
\index{くみあわせ@組み合わせ}、というのがあります。

N個からk個を選ぶ組み合わせは、
\begin{equation*}
C(N,k)=\frac{N!}{k!(N-k)!}=\frac{P(N,k)}{k!}
\end{equation*}
です。k個の順列を作った上で、k個の要素の並び方はどうでもよいので、
その場合の数$k!$で割ってあります。
これは、N個に2種類のラベルを貼る貼り方で、$k$枚と$N-k$枚に貼る貼り方のことです。
Rならば次のようになります。
\begin{lstlisting}
choose(N,k)
\end{lstlisting}
N種類から重複を許してk個を選んで作る組み合わせ(\textcolor{green}{重複組み合わせ}
\index{ちょうふくくみあわせ@重複組み合わせ})は次のようになります。
\begin{equation*}
H(N,k)=C(N+k-1,k)= \sum_{i=0}^k (C(N,k-i) \times H(k-i,i)) = \sum_{i=0}^k (C(N,k-i) \times C(k-1,i)) 
\end{equation*}
これは、N種類を１列に並べて、それを分けるN-1本の仕切り線を考えます。
k個を選ぶということを、N-1本の仕切り線で区分けされた場所にk個のものを置くという作業と考えます。
すると、N-1本の仕切り線とk個のものとを１列に並べる方法(順列)が、数え上げる場合の数になります。
ただし、N-1本の線とk個のものは、互いに区別がつかないことを考慮して
\begin{equation*}
H(N,k)=\frac{P(N-1+k)}{P(N-1)P(k)}=C(N+k-1,k)
\end{equation*}
たとえば、N種類のアレルを持つ\textcolor{red}{多型}\index{たけい@多型}が作る
\textcolor{red}{2倍体}\index{２ばいたい@２倍体}のジェノタイプは、$H(N,2)$です。
\begin{lstlisting}
repCombN<-function(N=10,k=3){
 choose(N+k-1,k)
}
repchoose<-function(n=4,k=3){
 ret<-0
 for(i in 0:k){
  ret<-ret+choose(n,k-i)*repCombN(k-i,i)
 }
 ret
}
\end{lstlisting}


\section{分割の数 スターリング数とベル数}

N個の要素を\textcolor{green}{分割}\index{ぶんかつ@分割}することを考えます。
まずk群に分割するとします。
どの要素も必ずどれかの群に属するものとして、
複数の群に属する要素はないものとします。
また、k群のすべてが、必ず１個以上の要素を持つように分割します。
これを\textcolor{green}{第2スターリング数}
\index{だい２すたーりんぐすう@第２スターリング数}
$St2(N,k)$といいます。
kは1,2,...,Nのいずれかです。
$k=1,N$のときには$St2(N,k)=1$です。
$St2(N,k)$がわかっているとします。
すると、$St2(N+1,k)$は、次のような式で表されます。
\begin{equation*}
St2(N+1,k)=St2(N,k)\times k +St2(N,k-1)
\end{equation*}
これは、要素数が$N$から$N+1$に増えたとき、$k$群に分けるわけ方は、
\begin{itemize}
\item $N$個の要素がすでに$k$群に分かれているときに、追加された１要素を$k$群のどれか１つに入れる場合が$St2(N,k)\times k$通り。
\item 追加する1要素が単独で１つの群に属するようにするとすると、
残りの$N$個の要素は$k-1$群に分けるので、
その場合は、$St2(N,k-1)$通り
\end{itemize}
の２パターンであることを利用しています。
2分割の場合の数は、$N$個から、k=1,2,...,N-1個を取り出す場合の数と関係しています。
$k$個取り出す場合をすべて数え上げると、$N-k$個を取り出す場合も数えてしまうので、
2倍になることを考慮して、
\begin{align*}
St2(N,2)&=\frac{1}{2} \sum_{i=1}^{N-1} C(N,i) \\
&= \frac{1}{2} (\sum_{i=0}^N C(N,i) -(C(N,0)+C(N,N)) )\\
&= \frac{1}{2} ((1+1)^N -2) = 2^{N-1}-1
\end{align*}

これは、N個の要素の\textcolor{red}{集合}
\index{しゅうごう@集合}の
\textcolor{red}{部分集合}\index{ぶぶんしゅうごう@部分集合}の数が、集合そのものと
\textcolor{red}{空集合}\index{くうしゅうごう@空集合}とを併せたときに$2^N$であることを
分割として書き直した式です。


重複順列と組み合わせ、分割の場合の数には図15.2のような相互関係があります。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/5-2.eps}
  %\includegraphics{./Figures/Ch13/RepPermVariation/RepPermVariation.eps}
\caption{左から重複順列、組み合わせ、組み合わせ、分割、重複組み合わせです。
同じ計算式になります}
 \end{center}
\end{figure}

\begin{lstlisting}
Stirling2N<-function(N=10,k=3){
 ret<-0
 if(k<=N && k>=1){
  if(k==1){
    ret<-1
   }else{
    ret<-Stirling2N(N-1,k)*k+Stirling2N(N-1,k-1)
   }
 }
 ret
}

\end{lstlisting}

N個の要素を、群の数は問わずに分けるわけ方は
\textcolor{green}{ベル数}\index{ベルすう@ベル数}($B(N)$)と呼ばれます。
これは、N個を$k=1,2,...,N$群に分ける場合の数($St2(N,k)$を足し合わせたものなので、
\begin{equation*}
B(N)=\sum_{i=1}^N St2(N,i)
\end{equation*}
です。
こうも書けます。
\begin{equation*}
B(N+1)=\sum_{i=0}^N C(N,i) B(i)
\end{equation*}
これは、次のように考えます。
$N+1$番目の要素が属する群が、$i+1$個の要素を持つ群だとします。
そうすると、その群に属する要素を除いた、$N-i$個の要素を分割するパターン($B(N-i)$)通りが
存在します。
$N$個の要素から、$N+1$番目の要素と一緒にする$i$個の要素の選び方は、
$C(N,i)$なので、このような関係が生じます。
ただし、0個の要素の分割という、定義しにくい値がありますから、$B(0)=1$と
定めて、\textcolor{red}{漸化式}\index{ぜんかしき@漸化式}がうまくいくようにしてあります。
\begin{lstlisting}
bellN<-function(N=10){
 bs<-rep(0,N+1)
 bs[1]<-1
 if(N>=1){
  for(i in 2:(N+1)){
   for(j in 0:(i-2)){
    bs[i]<-bs[i]+choose(i-2,j)*bs[j+1]
   }
  }
 }
 bs[2:length(bs)]
}
\end{lstlisting}

\section{分割とカテゴリの統合}
\subsection{順序のないカテゴリの場合}
第２スターリング数もベル数も集合を\textcolor{red}{分割}\index{ぶんかつ@分割}
する場合の数でした。

順序のないN種類のカテゴリを持つ尺度があったときに、Nカテゴリをそれぞれ独立に扱うこともありますが、
いくつかのカテゴリを分割する(共通項でまとめる)ことを考慮することがあるかもしれません。
そのようなときには、ここで示した場合の数のすべてを対象とするか、
一部の組み合わせだけをなんらかの根拠とともにプールすることになります。
ここでの集合の分割は、順序のないカテゴリの分け方に相当します。
\subsection{順序のあるカテゴリの場合}
では、順序のあるカテゴリ型尺度のときにはどうなるでしょうか(図15.3)。
たとえば、重症度が５段階で評価されている場合に、重症度と2カテゴリ型の要素(治療反応性など)との関係を
調べたいとします。
５段階に(1,2,3,4,5)と線形の重みをつければ、それに応じた
\textcolor{red}{傾向性の検定}\index{けいこうせいのけんてい@傾向性の検定}になります。
((1),(2,3,4,5))と分けたり、((1),(2,3),(4,5))と分けたりすることが適当なことがあるかもしれません。
では、どれくらいの分け方があるかを考えます。
N個の要素を1列に並べると、N-1箇所の仕切り線があります。
このN-1本の仕切り線のうち、1本だけ有効にすれば、2分割、2本を有効にすれば、3分割、
k=1,2,...N-1本を有効にすれば、k+1分割なので、結局、
順序のあるカテゴリの分割の場合の数は
\begin{equation*}
B_{order}(N)=\sum_{i=1}^{N-1} C(N-1,i)=2^{N-1}-1=St2(N,2)
\end{equation*}
\begin{lstlisting}
Border<-function(N){
 2^{N-1}-1
}
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/5-3.eps}


  %\includegraphics{./Figures/Ch13/BellStirling/BellStirling.eps}
\caption{順序のないカテゴリの分割と順序のあるカテゴリの分割}

 \end{center}
 \label{fig:one}
\end{figure}


\section{木の形の数・グラフの数　木・クラスタリング・ベイジアンネットワーク}
\subsection{木のパターンの数}
場合分けに\textcolor{green}{カタラン数}
\index{カタランすう@カタラン数}というものがあります。
根のある\textcolor{red}{木}\index{き@木}で表す現象は遺伝学では多いですが、
根のある木で、N本の辺を持つグラフの数がそれです。
また、N個の頂点を内部頂点とするグラフの形の数もカタラン数です。
根のある２分岐木の場合Ｎ個の内部頂点を持つものは、
N+1個の葉を持つ(N+1チームのトーナメント大会
はN試合)ので、N+1個の要素の纏め上げ2分岐木
の形の数がカタラン数であるとも言えます。
 

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/5-4.eps}

  %\includegraphics{./Figures/Ch13/Catalan/Catalan.eps}
\caption{上段の５つの木：内部頂点が３の木。
下段：根のある木で辺の数が３の木。
}

 \end{center}

\end{figure}
\begin{lstlisting}
CatalanN <- function(N = 10) {
 exp(lgamma(2 * N + 1) - lgamma(N + 2) - lgamma(N + 1))
}
\end{lstlisting}

\subsection{クラスタリングのパターンの数}
\textcolor{red}{分岐木}\index{ぶんきぎ@分岐木}といえば、
\textcolor{red}{クラスタリング}\index{クラスタリング@クラスタリング}
でも分岐木として纏め上げる作業をしました。
N個の要素を２分岐木に纏め上げるとき、ある分岐点の先を入れ替えても
クラスタリングのパターンとしては変わらないので、そのことを考慮することにします。
N個の要素を2分岐木パターンでクラスタリングする方法の数は
\textcolor{green}{ダブルファクトリアルナンバー}\index{ダブルファクトリアルナンバー@
ダブルファクトリアルナンバー}
と呼ばれて、
\begin{equation*}
(2N-1)!!=1\times 3\times 5 \times ...\times (2N-1)
\end{equation*}
と計算されます。

２分岐木を数え上げて描くツールはRにあって、
少数であれば、それを使っても数え上げることができ、
描くこともできます(図15.5)。
\begin{lstlisting}
library(phangorn)
allTrees(5) # 頂点数5でクラスタリングパターンを列挙する
trees<-allTrees(5)
plot(trees)
\end{lstlisting}


\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/5-5.eps}

  %\includegraphics{./Figures/Ch13/TreesU/TreesU.eps}
\caption{頂点数５の２分岐木パターンのすべて(15通り)}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{無向グラフの数 有向グラフの数 非循環有向グラフの数}
頂点数N個の\textcolor{red}{無向グラフ}
\index{むこうグラフ@無向グラフ}を作るときには、頂点数Nに対して、頂点のペア($\frac{N(N-1)}{2}$)について、
辺を引くか引かないかの選択が出来るので、
\begin{equation*}
2^{\frac{N(N-1)}{2}}
\end{equation*}
です。

\textcolor{red}{有向グラフ}
\index{ゆうこうグラフ@有向グラフ}の場合は、辺を引かないか引くとしたら2方向のどちらに引くかの３つの選択肢
があるので
\begin{equation*}
3^{\frac{N(N-1)}{2}}
\end{equation*}
です。

ベイジアンネットワークで出てきた
\textcolor{red}{非循環有向グラフ}\index{ひじゅんかんゆうこうぐらふ@
非循環有向グラフ}の種類は、

\begin{align*}
N_{acg}(0)=1\\
N_{acg}(N)=\sum_{i=1}^N (-1)^{k+1} C(N,k)\times 2^{k(N-k)} \times N_{acg}(N-k)\\
\end{align*}
で与えられます。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/5-6.eps}
  %\includegraphics{./Figures/Ch13/Graph/Graph.eps}
\caption{上段は頂点数３の無向グラフのパターン(８種類)。下段は非循環有向グラフのパターン(25種類)。
最下段の２パターンは循環しているもので数えていない}
 \end{center}
\end{figure}

\begin{screen}
ここで出た数列は"AT\&T Labs Research"が提供している整数列サイト(整数列大事典)で確認できます。
\url{http://www.research.att.com/~njas/sequences/}\\
\begin{itemize}
\item 順列(Factorial numbers) id A000142
\item 第２スターリング数(Stirling numbers of second kind) id A008277
\item ベル数(Bell or exponential numbers) id A000110
\item カタラン数(Catalan numbers) id A000108
\item ダブルファクトリアルナンバー(Double factorial numbers) id A001147
\item 非循環有向グラフ数(Number of acyclic digraphs (or DAGs) with n labeled nodes) id A003024
\end{itemize}

\end{screen}


\chapter{省略する}
\section{ランダムに抽出する・ランダムに巡回する}
見てきたように、場合の数が多いことが頻繁にあるため、工夫が必要になります。
何かしらの処理を手順として定め、その手順が効率的であるように工夫したものを
\textcolor{red}{アルゴリズム}\index{アルゴリズム@アルゴリズム}
と呼びます。
アルゴリズムのもうひとつの特徴は、いろいろな場面で使い回せることです。
分野を超えて利用されているものが多く、また、一つの目的に
複数のアルゴリズムがあり、長所・短所を含めた特徴があります。
また、多数の亜型が派生的に生じることも多いです。

まず、すべての場合を調べる代わりに、一部を調べることで、全体を調べたのと似た
結果を得ることを目指した工夫です。

\subsection{既知の分布からのランダムサンプリング}
ランダムに値を発生させるということは、値がとるべき分布に沿って発生させて、
有限個の値が、目指す分布と似ているようにすることです。
\textcolor{green}{乱数}\index{らんすう@乱数}を発生させるときに、計算機では
\textcolor{green}{疑似乱数列}\index{ぎじらんすうれつ@疑似乱数列}
というものを使います(18章18.4)。
疑似乱数列はアルゴリズムに基づいて発生します。
しばしば用いる既知の分布からの乱数発生は、そのための関数が用意されています。
３カテゴリの頻度分布は正三角形領域に表現できますが、そこに
\textcolor{red}{ディリクレ分布}\index{ディリクレぶんぷ@ディリクレ分布}
からの疑似乱数列発生により、均等に頻度分布が
発生している様子を示したのが図16.1です。
\begin{lstlisting}
library(MCMCpack)
d<-rdirichlet(1000,c(1,1,1)) # ディリクレ分布からの均一乱数の発生
plot(d[,1],sqrt(3)/2*(d[,2]-d[,3]),xlim=c(0,1),ylim=c(-sqrt(3)/2,sqrt(3)/2))
segments(c(0,1,0),c(-sqrt(3)/2,0,sqrt(3)/2),c(1,0,0),c(0,sqrt(3)/2,-sqrt(3)/2)) #三角形の周を描く
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-020.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{サンプルを使ってランダムサンプリング リサンプリングとパーミュテーションン}
\subsubsection{\textcolor{green}{リサンプリング}\index{リサンプリング@リサンプリング}}
得られたデータを用いて、ランダムサンプリングをすることもあります。
データがサンプルものなので、「再度」サンプリングするという意味で、「リ・サンプリング」
です。
母集団の分布はわからないながら、サンプルの分布が母集団の分布に準じたものになっていると
考えてこのようにします。
データの一部を用いて繰り返しサンプリングをすることで、
標本統計量の信頼区間等を推定することに利用します。
この方法の一つである\textcolor{green}{ブートストラップ}
\index{ブートストラップ@ブートストラップ}法では、サンプルセットから、
重複を許してリサンプリングします。
\begin{lstlisting}
x<-1:10
#bootstrap
sample(x,replace=TRUE)
#permutation
sample(x,replace=FALSE)
\end{lstlisting}
\begin{Schunk}
\begin{Soutput}
 [1] 7 4 8 6 7 8 8 3 5 4
\end{Soutput}

\begin{Soutput}
 [1]  6  2  5  8  7  3  1  9  4 10
\end{Soutput}
\end{Schunk}

サンプルの一部を使ってモデル推定をして、モデル推定に使わなかったサンプルを用いて、
そのモデルを検証することを、\textcolor{green}{クロス-バリデーション}
\index{クロス-バリデーション@クロス-バリデーション}と呼びますが、ここでも、
どのサンプルを検証用に選ぶかを決めるために、ランダムサンプリングを行います。
また、重複を許さないでリサンプリングすることがあります。
重複を許さないですべてのサンプルをリサンプルして、サンプルの順序を入れ替えると、
それは、\textcolor{green}{置換}\index{ちかん@置換}(
\textcolor{green}{パーミュテーション}\index{パーミュテーション@パーミュテーション})と呼ばれます。

\subsubsection{パーミュテーション(順列・置換)}

N個の要素のすべてを用いて順列を作ったものは特に\textcolor{green}{置換}
\index{ちかん@置換}(\textcolor{green}{パーミュテーション}\index{パーミュテーション@
パーミュテーション})と呼びますが、
この置換を用いて、独立性の検定をすることが出来ます。

８サンプルあって、2項目A,Bのデータが次に示す表のような場合を考えます。

\begin{tabular}[htb]{|c|c|c|} \hline
　&A&B\\ \hline
S1& 1 & 1 \\ \hline
S2& 2 & 3 \\ \hline
S3& 3 & 2 \\ \hline
S4& 4 & 6 \\ \hline
S5& 5 & 4 \\ \hline
S6& 6 & 7 \\ \hline
S7& 7 & 8 \\ \hline
S8& 8 & 5 \\ \hline
\end{tabular}

ここで、帰無仮説の棄却検定をするために、AとBとが無関係であると仮定します。
無関係ならば、８個のサンプルが取ったAの値はどのサンプルが
どの値をとるのも自由のはずなので、
Aの値をシャッフルしてやることにします。
今、このシャッフルする場合の数は\textcolor{red}{順列}\index{じゅんれつ@順列}$P(8,8)$
(15章15.1)ですから、
すべての置換(順列)を観察データと比較してやります。
比較するときには、何かしら基準が要ります。
この場合は、Aの値とBの値について、\textcolor{red}{Jonckheere-Terpstra}
\index{Jonckheere-Terpstra@Jonckheere-Terpstra}
テストのp値(jtp)の大小で比較することにしましょう。
jtpが小さいほどAとBとに関係がないという仮説の下では「珍しい」わけですから、
シャッフルして出たjtpの値(置換(順列)の数だけあります)のうち、
観察データのjtpの値以下の置換(順列)数を数え上げます。
その数が、置換(順列)の総数に占める割合が、パーミュテーション法のP値です。
置換(順列)の総数がさほど多くなければ、すべての
置換(順列)を調べ上げればよいですが、
たいていの場合は数え切れないので、
そのときには、適当な数の置換(順列)を作成してやり、それに対応するjtpの値と
観察jptの値とを比較してパーミュテーション法のP値とします。
一部の置換(順列)で済ます方法は、
計算機がランダムに作った置換(順列)に基づくので\textcolor{green}{モンテカルロ}
\index{モンテカルロ@モンテカルロ}
\footnote{
モンテカルロ法とは計算機を使って乱数を発生させて行う方法の総称
}なパーミュテーション法と言いますが、
現実的には、パーミュテーション法はモンテカルロにて行うことが普通なので、パーミュテーション法と言えば、
モンテカルロの方を指すことも多いです。
以下のソースにて、実行可能です。
パーミュテーション法のp値は\textcolor{red}{離散}
\index{りさん@離散}的な値をとっており、観測表のp値を水平線で示したので、
水平線以下の点の比率が、パーミュテーション法のp値となります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-024.eps}
\caption{モンテカルロ・パーミュテーション法によるp値の累積分布。
水平線は観測テーブルのp値}
 \end{center}

\end{figure}
\begin{lstlisting}
# データ作成
A<-1:8
B<-c(1,3,2,6,4,7,8,5)
# 全置換でのパーミュテーションかモンテカルロ・パーミュテーションかをMonteCarloで選択
PermutationTestJT<-function(A,B,MonteCarlo=TRUE,n=10000){# n モンテカルロのときの試行回数
 library(clinfun) # jonckheere.test()のパッケージ
 ret<-0
 Ps<-NULL
 jtp<-jonckheere.test(A,B)$p.value # 観測表p値
 if(MonteCarlo || length(A)>10){ # 要素数が10より大なら、必ずモンテカルロ
  Ps<-rep(0,n)
  for(i in 1:n){
   tmp<-sample(1:length(A),length(A))
   Ps[i]<-jonckheere.test(A[tmp],B)$p.value
  }
 }else{ # 全部の置換を実行
  library(gtools) # permutations()のパッケージ
  perms<-permutations(length(A),length(A)) # 全置換を返す
  Ps<-rep(0,length(perms[,1]))
  for(i in 1:length(perms[,1])){
   Ps[i]<-jonckheere.test(A[perms[i,]],B)$p.value
  }
 }
 ret<-length(Ps[Ps<=jtp])/length(Ps)
 list(originalp.value=jtp,permp.value=ret,Ps=Ps)
}
out1<-PermutationTestJT(A,B,MonteCarlo=FALSE) # 全置換に基づく：正確検定

out2<-PermutationTestJT(A,B,MonteCarlo=TRUE,n=100) # モンテカルロパーミュテーション法

out1$originalp.value # 観察表のjpt
out1$permp.value # 全置換に基づくp
out2$permp.value # モンテカルロパーミュテーションに基づくp。シミュレーション回数が値の最小値を決める
plot(sort(out1$Ps))
abline(h=out1$permp)
\end{lstlisting}

\subsection{ランダムウォーク}
いろいろな状態(変数の値)を確率的に取るようなときに、
その状態を網羅的にランダムに作成することは、難しいことも多いです。
そのようなとき、ある状態からスタートして、
そこから変化しうる状態を選んで、次の状態とすることで、色々な状態を作成する
方法が\textcolor{green}{ランダムウォーク}\index{ランダムウォーク@ランダムウォーク}です。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-025.eps}
\caption{2次元平面上のランダムウォーク}
 \end{center}
\end{figure}

ここでは、2次元平面のランダムウォークを描きました。

今、変数の\textcolor{red}{推定}
\index{すいてい@推定}をしている(捜し物をしている)ときに、変数が取り得る空間の全体を
歩き回れるなら、適切なところを見つけることができるだろう、という考えに
基づく方法です。
推定途中の変数の値(位置)を用いて、次のステップでの変数の値(位置)を決めるので、
これは、\textcolor{red}{マルコフ連鎖}
\index{マルコフれんさ@マルコフ連鎖}です。
捜し物ですから、最終的には、求めている点(変数の推定値)へ向かって移動して行って欲しいものです。
ですから、ランダムウォークでは空間をやみくもに歩くのではなく、適切な方向へ進む
確率が高いように工夫(アルゴリズム)がなされます。
\textcolor{green}{メトロポリス-ヘイスティング}
\index{メトロポリス-ヘイスティング@メトロポリス-ヘイスティング}法や、
\textcolor{red}{共役事前分布}\index{きょうやくじぜんぶんぷ@共役事前分布}
を用いるサンプリングなどは、
このランダムウォークを効率よくする工夫です。
このタイプの処理では、どこから歩き始めるかが結果に影響を与えますから、
その影響がなくなるまで(なくなったと判断できるまで)ランダムウォークを続けることや、
開始点を複数採用して複数の結果を求め、その一致性を確認することなどもなされます。
これらも「工夫」の一部です。


\section{主要な部分のみを使う}
\subsection{近似する}
\subsubsection{モデルにあてはめる}
分割表の\textcolor{red}{正確確率}\index{せいかくかくりつ@正確確率}が
\textcolor{red}{カイ分布}\index{カイぶんぷ@カイ分布}の
\textcolor{red}{確率密度関数}\index{かくりつみつどかんすう@確率密度関数}と似ていることを
13章(図13.2)で
確認しましたが、
そこでは、ある特定の関数の式を想定し、その係数を推定しました。
Rのoptim()関数を用いました。
観測データをある関数にあてはめるにあたり、
そこからのずれが最小になるような係数を推定しました。
繰り返し処理をして、係数を改善し、ずれが十分に小さくなるまで続けました。
そしてこの係数の改善方法についても複数のアルゴリズムが実装されていて、
選択することができました。
図13.2の例は、あてはめる関数が、データがほとんど完全に
その関数にて表される場合でした。

配列比較の評価(４章4.2.4)で\textcolor{green}{極値分布}
\index{きょくちぶんぷ@極値分布}を使うとかきました。
配列の異同を調べるとき、
よく似た配列を較べるため
2つの配列に関連がないという仮説の棄却という観点で
評価すると、非常に極端な値が得られます。
このように非常に極端な値は、その由来に関わらず、
ある関数で表される分布に従うことが知られており、それを
極値分布と言います。
\footnote{
大水害や、飛行機事故など、
稀な事象が起きる確率は、実際におきることが稀なので、同レベルの事象の発生確率を観察すること
からは、わかりません。
比較的稀な事象の観察を重ね、その分布をとったうえで、極値の分布の形を推定することを介して、
極端な事象の確率を推定します。
このとき用いる分布が極値分布です
}

この極値分布の係数を推定してみることにします。
沢山の仮説をカイ自乗検定して、多くのカイ自乗値を得たときに、
そのカイ自乗値の中で、一番大きな値を考えます。
このような沢山の仮説の検定を何セットも行ってやると、
セットの数だけ、「一番大きなカイ自乗値」が得られます。
この「一番大きなカイ自乗値」が極値分布で近似できます。
沢山の検定を行ったときに、どのくらいの統計量を有意であると
考えるかを\textcolor{red}{多重検定}\index{たじゅうけんてい@多重検定}問題と言いますが、
この「一番大きなカイ自乗値」の分布に照らして、検定結果を評価することが、
多重検定問題を解く鍵です。

さて、極値分布は、
\begin{equation*}
G(x)=e^{-(1+\zeta(\frac{x-\mu}{\sigma})^{-\frac{1}{\zeta}}}
\end{equation*}
という式で表されます。

複数の「一番大きなカイ自乗値」を得たら、
その分布に適合するような係数$\zeta,\mu,\sigma$を推定してやります。

実際にRで行ってみます。
ランダムに発生した1000個のカイ自乗値の中の最大値を
100セット分つくり、それをもとに、極値分布を推定しています。
100個の観察された、最大カイ自乗値と、
推定された極値分布との累積分布をプロットしています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-028.eps}
\caption{カイ自乗分布からの乱数1000個のうちの最大値を100回観察したときの
累積分布が黒。それに合致する極値分布が赤}
 \end{center}
\end{figure}

\begin{lstlisting}
#自由度1のカイ自乗分布乱数を100000個発生
N<-100000;M<-100;t<-rchisq(N,1)
#100セットに分割
matt<-matrix(t,nrow=M)
#それぞれのセットの最大値を取り出し
maxt<-apply(matt,1,max)
#evdパッケージを読み込んでから、その関数を使用して、
#Generalized Extreme Value Disvributionの３パラメタを推定した上で
#その累積確率を算出してプロットし、観察値の累積分布と比較
library(evd)
gevest<-fgev(maxt)
qfromgev<-qgev(ppoints(length(maxt),a=0),loc=gevest$estimate[1],scale=gevest$estimate[2],shape=gevest$estimate[3])
plot(ppoints(length(maxt),a=0),sort(maxt),ylim=c(0,max(maxt)),type="l")
par(new=T)
plot(ppoints(length(maxt),a=0),qfromgev,ylim=c(0,max(maxt)),type="l",col="red")
\end{lstlisting}

\subsubsection{形式にあてはめる}
前項の例は、関数とそれを定める係数とがあって、
データを用いて、係数を推定するという手続でした。

限られた数の変数で作ったモデルを観察データにあてはめることと同じです。

データをかいつまむ章では、このように、意味のある変数によってデータを捉える方法と、
データそのものから変数を引き出す方法とがあることを述べました(７章7.3)。
ここでも、その視点で眺めてみます。

データをうまく説明するために、大きく説明する変数から、だんだんに、細かく説明する変数
を加えて行くことを考えます。\textcolor{red}{固有値分解}
\index{こゆうちぶんかい@固有値分解}の手続がこれに相当しました。

\textcolor{green}{多項式近似}\index{たこうしききんじ@多項式近似}というのがあります。
\begin{equation*}
f(x)=a_0+a_1 x + a_2 x^2 +... + a_k ^k
\end{equation*}
という式での近似です。
k次の多項式近似と言います。
線形関数への回帰式の推定と同じことです。
平均を取るというのは、0次多項式近似とも言えます。
ランダムなカイ自乗値1000個の最大値の分布について、optim()関数を用いて、
次数を上げるに従って、線形近似がよくなる様子を見てみましょう。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-030.eps}
   \includegraphics[width=50mm]{./Fig/PartV-032.eps}
   \includegraphics[width=50mm]{./Fig/PartV-033.eps}
   \includegraphics[width=50mm]{./Fig/PartV-034.eps}
   \includegraphics[width=50mm]{./Fig/PartV-035.eps}
\caption{カイ自乗分布からの乱数1000個のうちの最大値の線形近似}
 \end{center}
\end{figure}

\begin{lstlisting}
sortedMaxt<-sort(maxt) # 最大カイ自乗値の昇順ソートリスト
s<-ppoints(length(sortedMaxt),a=0)
#例えば2次での近似
f2 <- function(x) {
  sum((sortedMaxt-(x[1]*s^0+x[2]*s^1+x[3]*s^2))^2)
}
optout2<-optim(rep(1,3),f2,method="BFGS")
ylim<-c(0,max(sortedMaxt))
plot(s,sortedMaxt,ylim=ylim,type="l")
optt<-rep(0,length(s))
for(i in 1:length(optt)){
 optt[i]<-sum(optout2$par * s[i]^ (0:0) )
}
par(new=T)
plot(s,optt,ylim=ylim,col="red",type="l")
\end{lstlisting}


データの説明のときには、自由度の数だけの変数を使って、データのすべてを
説明することができる、という話をしました。
今回の多項式近似では、$k$はどこまでも大きくできます。
本来のデータが多項式で表すことができなくても、自由度の数を超えて
用いることで、近似をどんどんよくすることができます。

同様に、複雑な関数を取り扱いが容易な成分に
で近似する方法に、\textcolor{green}{テイラー展開}
\index{テイラーてんかい@テイラー展開}などの
\textcolor{green}{無限級数}\index{むげんきゅうすう@無限級数}展開があります。
これも微分を繰り返して、無限に細かくする近似です。


\section{意義の大きいほうから選ぶ　小さいほうから捨てる}
分散の分解
(固有値分解)と多項式近似・無限級数展開
とは
\textcolor{red}{有限}\index{ゆうげん@有限}に分けるか
\textcolor{red}{無限}\index{むげん@無限}に分けるかと言う点に違いがありましたが、
次の２点が共通です。

\begin{itemize}
\item 大きな特徴を捉えたのち、段々に細部を説明していく
\item 大づかみから詳細説明へと進める手順が決まっている
\end{itemize}

似たような方法として、複数の要素から、
意味のある要素の\textcolor{red}{組み合わせ}\index{くみあわせ@組み合わせ}
を絞り込むという作業があります。
要素の組み合わせは、全部を調べ上げようとすると膨大な場合の数となるために、
すべての組み合わせを試す代わりに、
次のような手順で一部の組み合わせのみを調べることがあります。
まず、1つめを選ぶときに、一番強い要素を選択し、
次に、初めに選択した要素は、必ず、取り入れることとして、
残った要素について、最大のものを取り出します。
常に、その時点での対象から一番のものを選び出すことの繰り返しです。
この逆もあります。
すべての要素をリストに入れた状態からスタートします。
ここで、もし、ただ１つを外すとしたら、どれが一番外されるべきかを
検討します。
次に外すときは、すでに外したものは外すことを前提に、
外すべき要素を、一番不要のものとして外します。
多数の要素が、順番に並びました。

有限個の変数への分解にあたっても、説明力の強さの順番がありました。
無限個への展開の場合も順番がありました。

これらはみな、要素が順番に並んでいて、
段々に説明する内容が小さくなっています。
基本的には、大きく捉えている部分に意味が大きく、小さく捉えている部分は、意義が小さめです。
この順序のあるリストの始めの方からいくつかを取り上げることで、
全体の説明とすることが近似なわけですが、
では、何個目までを取り上げることにして、
何個目以降は取り上げないようにするのがよいかの判断が必要なことがあります。

段階を１段進めたときに、それが重要な一段なのか、意味のない１段なのかの判定をするのも、
統計的判断です。
その判断に当たっては、採用される変数と観察されたデータとの間の
\textcolor{red}{尤度}\index{ゆうど@尤度}、変数の数と
\textcolor{red}{自由度}\index{じゆうど@自由度}の
関係に基づいて、
\textcolor{green}{情報量基準}\index{じょうほうりょうきじゅん@
情報亮基準}と呼ばれる基準とそれを用いた判断方法が複数あり、
詳細には触れませんが、この処理を支えています。

\chapter{たくさんの検定}
\section{多重検定}
\subsection{独立な検定の繰り返し}
下手な鉄砲も数撃てば中たる、のことわざにもあるように、
珍しい出来事も、繰り返して何度も行えば一度くらいは起こるものです。
これと同じことで、小さいｐ値に相当する珍しい統計量も、
繰り返して測れば、珍しいことではなくなります。
これが、たくさんの検定を行ったときにｐ値の解釈を変える理由です。
\textcolor{green}{多重検定}
\index{たじゅうけんてい@多重検定}と呼ばれる問題です。
以下に、ｐ値の解釈の変更方法(補正方法)について述べていきます。

\subsection{多重検定時のp値の期待値}
今、独立なk個の検定を行ったときに一番小さいp値はどのくらい小さいかを考えます。
k=1のときは、0-1の均一分布で、その期待値は0.5です。
この期待値がkとどういう関係にあるかというと、\\
$E(min(p)|k)=\frac{1}{k+1}$\\
です。
さらに、k個の検定を行ったときに$i$番目に小さいp値の期待値は
\begin{equation*}
E(i_{th} p | k)=\frac{i}{k+1}
\end{equation*}
です。
また、一般に、第$i$番目に小さいP値の期待値は
\begin{equation*}
\frac{i}{k+1}
\end{equation*}
です。
\footnote{
すべてのp値がa未満である確率は、
$Pr(all p<a|k)=1-(1-a)^k$
でした。
１番小さいp値(minP)がaから$a+\delta a$である確率は、
すべてのp値がa未満である確率より大きく、
すべてのp値が$a+\delta a$未満である確率より小さいので、
そのような確率は
$Pr(all p < a+\delta a|k)-Pr(all p< a|k)$
で表されます。
$mimP=a$の確率は、$\delta a$を極限まで小さくしたときの値ですから、
結局、
$Pr(min p=a|k)=\frac{d Pr(all p<a|k)}{da} (a)=k(1-a)^{k-1}$
となります。
さて、期待値は、取り得る値について、起きる確率の重みをつけて積分すればよいので
$ \int_{0}^{1} a Pr(min p=a|k) da = \int_{0}^{1} ka (1-a)^{k-1} da =
k ( \int_{0}^{1} (1-a)^{k-1} da -\int_{0}^{1} (1-a)^{k} da ) =
\Biggl[ k (\frac{1}{k} (1-a)^k-\frac{1}{k+1} (1-a)^{k+1}) \Biggl]_0^1=
k(\frac{1}{k}-\frac{1}{k+1})=\frac{1}{k+1}
 $
さらに、一般に、k個のうち$i$番目に小さいp値の期待値は、
$i$個がa未満で$k-i$個がa以上である確率が$\frac{k!}{i!(k-i)!} a^i(1-a)^{k-i}$であることから、
最小p値の場合より多少面倒ですが、$\frac{i}{k+1}$であることを示すことができます。
}

\subsection{一番小さいp値の補正}
\subsubsection{Sidakの方法}
沢山の要素について、多くの検定を繰り返すと最も小さいｐ値の期待値が
ｐ値の名目上の値より小さくなることがわかりました。
では、名目上のｐ値を実質上の珍しさを表わすように補正する方法について考えてみます。

最も基本となるのは、相互に\textcolor{red}{独立}\index{どくりつ@独立}
な検定を繰り返す場合です。
個々の検定のP値が0-1の\textcolor{red}{均一分布}
\index{きんいつぶんぷ@均一分布}となるとします。
k個の検定のそれぞれで、$p<a$となる確率が$a$なので、
k個の検定のすべてで$p<a$となる確率は$a^k$です。
逆に、すべてで$p \ge a$となる確率は$(1-a)^k$です。
今、k個の検定の中で１番小さいp値がaであったとします。
これよりも珍しいのは、\\
「k個のうちどれか一つでもp値がa未満である」＝「すべての検定で$p \ge a$であるわけではない」\\
ような珍しさと同じなので、
「すべての検定で$p \ge a$でない」確率となります。
それは、$1-(1-a)^k$です。
逆に言えば、$1-(1-a)^k=0.01$のようなaを観測したら、
それが、k個の独立な検定を行ったときに、100回に1回しか起きないほど
珍しいこと、という意味で、
「多重検定をしたときの補正後p値$p_c=0.01$」です。
\begin{equation*}
1-(1-a)^k=p_c
\end{equation*}
今、ある検定でp=0.00001を観察したとします。
ただし、一緒にk個の検定をしていて、そのk個の検定の中で最も小さいp値だったとします。
kを1個から10万個まで増やして、$p_c$の値がどのように変化するかをプロットしてみます。

k=1のときには、$p_c=0.00001$と、ｐ値をそのままの値で評価すればよいですが、
k=100000のときは、$p_c=0.6321224$と、まったく帰無仮説を
棄却しないことがわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=30mm]{./Fig/PartV-036.eps}
   \includegraphics[width=30mm]{./Fig/PartV-037.eps}
   \includegraphics[width=30mm]{./Fig/PartV-038.eps}
\caption{左・中央。k個の独立な検定を繰り返したときに、p=0.00001をいくつに補正するか(曲線がSidakの方法、直線がBonferroniの方法)。
左は通常スケール、中央は常用対数スケール。
右は、k個の検定を繰り返したときに、
Sidakの補正後ｐ値が0.01となるような名目上のｐ値をプロットしたもの
}
 \end{center}
\end{figure}

逆に、k個の検定をしたときに、どれくらい小さい名目上のp値を
とると、それは、沢山の検定をしたにも関わらず、十分珍しく、
0.01の棄却水準でも仮説を棄却できるでしょうか？
\begin{equation*}
a=1-(1-p_c)^{\frac{1}{k}}
\end{equation*}
ですから、

k=1の場合には、p=0.01ならば、補正後も$p_c=0.01$です。
k=100000のときは、名目上のｐ値が$1.005034*10^{-4}$くらい小さいときに、
補正後が$p_c=0.01$となります。

この方法で個々の検定のp値を多数の同時検定の数に応じて補正する方法を
Sidakの方法と呼びます。
\subsubsection{Bonferroniの方法}
これよりももっと単純に
\begin{equation*}
p_c=a\times k
\end{equation*}
とする方法もあります。
1000個の検定を行ったら、名目上のp値を1000倍して補正後p値とする、という単純なものです。
Sidakの方法とBonferroniの方法を比較したのが図17.1(左・中央)です。
Bonferroniの方法では、kが大きくなるに連れて、$p_c$がSidakの方法よりずいぶんと
大きめになることがわかります。
Bonferroniの方法では、p値が大きめに出ますから、\textcolor{red}{保守的}\index{ほしゅてき@保守的}な補正法
であると言われます。
逆に言えば、Bonferroniの方法を適用しても$p_c$が棄却水準を
越えている場合には、十二分の自信を持って、仮説を棄却できるとも言えます。


\subsection{非独立な検定の繰り返し}
11章で$2\times 3$表に複数の検定を実施したときに、その検定のｐ値は
相互に独立でないことをみました。
形質同士に共通点があったりジェノタイプ同士に共通点があったりする
(\textcolor{red}{連鎖不平衡}\index{れんさふへいこう@連鎖不平衡}・
\textcolor{red}{集団構造化}\index{しゅうだんこうぞうか@集団構造化})ときには、
個々の検定は独立ではありません。
そのようなときに、Bonferroniの補正法やSidakの補正法を用いると、
保守的に過ぎて、\textcolor{red}{偽陰性}\index{ぎいんせい@偽陰性}が増えてしまいます。
しかしながら、数多くの検定のp値の中の最小のp値がどのような分布をとるのかが
不明なので、補正の仕方がわかりません。
逆に言えば、数多くの検定のp値の中の最小のp値の分布がわかれば、
その分布に照らして補正すればよいのです。
沢山の検定をしているときには、手元に沢山の
データがありますから、それを用いて、
\textcolor{red}{リサンプリング}\index{リサンプリング@リサンプリング}の手法を利用して
分布を推定してやり、その推定した分布における\textcolor{red}{クオンタイル}
\index{クオンタイル@クオンタイル}から
多重検定補正を行うことができます。
リサンプリングとして、\textcolor{green}{パーミュテーション}
\index{パーミュテーション@パーミュテーション}(置換・順列)を用いる方法を利用する
ことができて、それは、パーミュテーション法と呼ばれました。
多重検定のパーミュテーションによる補正をしてみます。
\subsection{モンテカルロ・パーミュテーションによる多重検定補正}
今、ある因子Pと、複数の因子Xi i=1,2,...とを検定するとして、
Xi,Xjの独立が言えないような場合です。
まず、PとXiとの検定を実行します。
次に、Pの値はXの値と独立なのか、そうでないかを検定したいので、
Pの値をリサンプリングによって変更します。
\textcolor{green}{パーミュテーション}\index{パーミュテーション@パーミュテーション}法
は、繰り返し抽出を許さないタイプの\textcolor{red}{リサンプリング}
\index{リサンプリング@リサンプリング}方法ですから、
置換(順列)(Pp)を作ることになります。
置換(順列)ごとに、PpとXiとの検定を実施します。
興味があるのは、PとXiとの検定の最小P値がPpとXiとの検定の最小P値の分布におけるクオンタイル
です。
このクオンタイルがパーミュテーション法による補正後p値です。

以下では、相互に依存関係のある複数項目のデータXを作り、Pとのt検定を行っています。

リサンプリングの繰り返しとともに、補正後ｐ値がどのように変化するかをプロットした
のが図17.2です。
徐々ににある値に収束していく様子がわかります。
十分に収束すれば、それがパーミュテーション法での補正p値と言えます。
なお、パーミュテーション法による補正p値は、繰り返しリサンプリングの回数が
N回のときには、$\frac{1}{N}$より小さくなりません。
逆に、N回の繰り返しによって、観察最小P値以下の最小P値が出なくても、せいぜい
$\frac{1}{N}$未満であるとしか言えません。

\begin{figure}[htbp]
 \begin{center}
\includegraphics[width=50mm]{./Fig/PartV-043.eps}
   \includegraphics[width=50mm]{./Fig/PartV-052.eps}


\caption{左：リサンプリング回数が増えるとパーミュテーション補正後ｐ値は収束していく。
右：1,2,...,10番目に小さいｐ値の期待値}

 \end{center}
 \label{fig:one}
\end{figure}

\subsection{非独立な検定を繰り返したときの一番小さいｐ値}
さて、Xの各因子が相互に独立なときには、$i$番目に小さいp値の期待値は
$\frac{i}{N}$でした。
p値のXのデータの依存の程度を強くすると、
$i$番目に小さいp値の期待値はどのように変わるのでしょうか？

10個変数について検定をします。
この10変数のデータは５段階の関連を持たせます。
全く関連がない場合には、17.1.2で述べたように、$i$番目に
小さいP値の期待値は$\frac{i}{10+1}$となって、図17.2(右)では一番傾きの
大きなグラフとして描かれています。
だんだんに変数間の関連を強くすると、
グラフの傾きが小さくなり、
一番関連が強い場合(10変数のすべてが常に同じ値をとるようにしてあります)
では、水平になっています。
10個の変数($X_1,...,X_{10}$)がすべて同じときには、すべての変数が
同じｐ値をとるので、大小の差が出ません。
その結果、１番小さいｐ値の期待値も１番大きいｐ値の期待値も同じ(水平グラフ)になります。

\begin{lstlisting}
#パーミュテーション法による最小ｐ値のクオンタイル算出
Ns<-100;Nm<-10;Niter<-500 # サンプル数、マーカー数、パーミュテーション試行回数
# Pの置換(順列)とdとのt検定をする関数
Fx<-function(d){
 t.test(d~P[shuffle])$p.value
}
P<-rbinom(Ns,1,0.5) # ケース・コントロールの２値型フェノタイプ
X<-matrix(rnorm(Ns*Nm),nrow=Ns)
meanPs<-matrix(0,length(rs),Nm);Pminhistories<-matrix(0,length(rs),Niter)

X<-matrix(rnorm(Ns*Nm),nrow=Ns)
r<-0.3
for(i in 2:Nm){
 R<-sample(c(0,1),Ns,replace=TRUE,prob=c(r,1-r))
 X[,i]<-X[,i-1]*(1-R)+X[,i]*R
}
shuffle<-1:Ns
obsPs<-apply(X,2,Fx)
obsMinP<-min(obsPs)

Plist<-matrix(0,Niter,Nm)
Phistory<-rep(0,Niter)
counter<-0
for(i in 1:Niter){
 shuffle<-sample(1:Ns)
 Plist[i,]<-sort(apply(X,2,Fx))
 if(min(Plist[i,])<=obsMinP){
  counter<-counter+1
 }
 Phistory[i]<-counter/i
}
plot(Phistory, type = "b")
\end{lstlisting}


\section{p値が均一に分布しないとき}
前節の話しは、帰無仮説が成り立つ場合、p値の期待値が0.5となるような
場合に関してでした。
本節では、そうでない場合について、いくつかのパターンに分けて考えてみます。
\subsection{p値が小さめに出るとき  ジェノミックコントロール法}
p値の期待値が0.5ではなく、小さめに出る場合というのを考えてみます。

今、ケースとコントロールが、少々異なる集団からサンプリングされたとします。
\textcolor{green}{集団構造化}\index{しゅうだんこうぞうか@集団構造化}
のある集団からケースとコントロールが不均一にサンプリングされたような場合です。
\footnote{集団構造化。サンプルは遺伝的に多様です。
その多様性は、完全には混ざりきっていないために、偏りがあります。
\textcolor{red}{HWE}\index{HWE@HWE}からずれているのもそのような例です。
また、多民族が構成している集団などでは、社会的要因などがあり、ランダム
メイティングが実現しないことなどもその要因です。
このような集団から、着目形質の解析のためにサンプリングをすると、
ケースサンプルとコントロールサンプルの遺伝的背景が完全に一致しないことがあります。
このように、サンプリングする母集団が不均一な集団であることを集団が、遺伝的に
構造化していると言います。}

ゲノム上の多数のマーカーについて、ケース・コントロール間で関連検定をすることにします。
たくさんのマーカーの大多数は、ケースとコントロールの形質の違いとは無関係だとします。
その代わり、ケースとコントロールは遺伝的に少々違うので、マーカーごとの
関連検定のp値は0-1の均一分布より小さめに出ると思われます。
どの程度、どのように小さめに出るかを調べてみます。
ケースとコントロールは、HWEにある集団からのランダムなサンプルであるとします。
そして、それぞれの母集団は、マーカーのアレル頻度が同じこともあるし、異なることもあるものの、
小さく異なることの方が多く、大きく異なることは少ないとします。
マーカーはたくさんありますし、２集団間で混ざりのよいマーカーとそうでもないマーカーがあると
考えることは悪くないでしょうから、アレル頻度の違いが平均0の
\textcolor{red}{正規分布}\index{せいきぶんぷ@正規分布}に従うとしてシミュレーションしてみます。
\begin{lstlisting}
set.seed(995599);Niter<-1000 # 試行回数
library(Rassoc)
st<-rep(0,Niter);p<-rep(0,Niter) # 結果格納ベクトル
for(i in 1:Niter){
 af<-runif(1)*0.6+0.2 # およそのアレル頻度
 delta<-rnorm(1) # 2群のアレル頻度の違いは正規分布で決める
 af1<-af+af*0.05*delta; af2<-af-af*0.05*delta # 群別のアレル頻度
# ケース・コントロールをそれぞれ第1群・第2群からランダムサンプリング(HWEを仮定)
 case<-sample(c(0,1,2),1000,c(af1^2,2*af1*(1-af1),(1-af1)^2),replace=TRUE)
 cont<-sample(c(0,1,2),1000,c(af2^2,2*af2*(1-af2),(1-af2)^2),replace=TRUE)
# 分割表作成
 t<-matrix(c(length(case[case==0]),length(case[case==1]),length(case[case==2]),
             length(cont[cont==0]),length(cont[cont==1]),length(cont[cont==2])),nrow=2,byrow=TRUE)
 cattout<-CATT(t) # 相加的モデルで検定
 st[i]<-(cattout$statistic)^2; p[i]<-cattout$p
}
plot(ppoints(length(p),a=0),sort(p)) # p値の昇順プロット
# 色々なλで補正してみる
ylim=c(0,1)
plot(ppoints(length(p),a=0),sort(p),ylim=ylim,type="l") # 観測ｐのプロット
for(i in 2:20){ # 補正ｐのプロット
par(new=T)
plot(ppoints(length(p),a=0),sort(pchisq(st/i,1,lower.tail=FALSE)),ylim=ylim,col="red",type="l")
}
# 観測ｐ値の中央値が0.5となるようにλの値を算出
lambda<-quantile(st,0.5)/qchisq(0.5,1)
stc<-st/lambda
pc<-pchisq(stc,1,lower.tail=FALSE)
par(new=T)
plot(ppoints(length(pc),a=0),sort(pc),type="l") # 中央値を利用した補正法による補正後ｐのプロット
\end{lstlisting}

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-068.eps}
\caption{構造化集団でのケース・コントロール関連検定のｐ値}
 \end{center}
\end{figure}

帰無仮説が成り立っているときには、昇順に並べたp値はグラフで対角線上に並ぶはずですが、図17.3
では、期待するより、小さいp値が多いことがわかります。

このｐ値を次のように補正してみることにします。

今回の検定は相加的遺伝形式に関する\textcolor{red}{Cockran-Armitageの傾向性検定}
\index{Cockran-Armitageのけいこうせいけんてい@Cockran-Armigateの傾向性検定}
(自由度1)を用いました。
その検定統計量であるカイ自乗値を正のある値$\lambda$で割り、
それを補正後カイ自乗統計量とし、この
補正後値を自由度1のカイ自乗分布で評価してp値にするのです。

その様子を見てみます。
図17.4では$\lambda=2,3,...,20$としてあります。
下に凸だったプロットがだんだんに、対角線(均一分布)に近づいて、さらに行き過ぎて、
上に凸になっていく様子が見て取れます(図16.9左)。

この性質を利用して、観察された統計量から補正項$\lambda$を計算して、均一分布に
なるように補正することを考えます。
観察カイ自乗値の\textcolor{red}{中央値}
\index{ちゅうおうち@中央値}が、理論的カイ自乗値の中央値の何倍であるかを補正項$\lambda$
にするのが一つのやり方です。
\footnote{中央値は、値のセットの中にはずれ値があってもその影響が及びにくい値ですから、
たくさんのマーカーの中に、少しくらいの本当に形質と関連しているマーカーが混入していても、
影響はほとんどないという利点があります。}
\begin{figure}[htbp]
 \begin{center}
   %\includegraphics[width=50mm]{./Fig/PartV-069.eps}
   \includegraphics[width=50mm]{./Fig/PartV-070.eps}
\caption{下に大きく凸の黒線が構造化集団でのケース・コントロール関連検定のｐ値。
ほぼ対角線の黒線が中央値を利用した補正後のｐ値。
その他の赤線は、補正係数を２から２０まで変えたときの補正後ｐ値のシリーズ}

 \end{center}
 \label{fig:one}
\end{figure}


\begin{lstlisting}
\end{lstlisting}

この方法を\textcolor{green}{ゲノムワイド形質マッピング}
\index{ゲノムワイドけいしつマッピング@ゲノムワイド形質マッピング}では
\textcolor{green}{ジェノミックコントロール法}
\index{ジェノミックコントロールほう@ジェノミックコントロール法}
と呼んで、ゲノム上のマーカーを用いて大規模に関連検定
を行うときの集団構造化補正に用います。

\subsection{対立仮説が成り立つとき 非心カイ自乗分布}
前項ではケースとコントロールとが異なる集団からのサンプルである場合でした。
そして、検定する多数の因子の頻度の集団間の違いが0を中心とした正規分布になるような
例でした。

今度は、
帰無仮説が成り立たず\textcolor{red}{対立仮説}
\index{たいりつかせつ@対立仮説}が成り立つとき(関連が真であるとき)に
帰無仮説の棄却検定を実施する場合を考えてみます。
当然のことながら、\textcolor{red}{帰無仮説}\index{きむかせつ@帰無仮説}を棄却する検定の
p値は均一ではなくなります。
ジェノミックコントロールの例では、ケースとコントロールとでアレル頻度の差を
ゼロを中心とする正規分布で取りましたが、今回は、あるマーカーで
２群間にアレル頻度の差があるという
対立仮説が成り立つときにランダムサンプリングをし、
その検定ｐ値の累積分布を示します(図17.5(左))。

大きく下に凸の線が、関連が真であるときのｐ値をソートしてプロットしたものです。
図17.3の図と同様に下に凸ですが、
前項で紹介したジェノミックコントロール法で補正してみると対角線に
なりません(図17.5左の立ち上がりの早い線)。
補正後P値が対角線にならないことから、この補正が無効であることがわかります。

対立仮説が真の場合のカイ自乗値は、
行と列との独立を仮定した期待値ではない値(対立仮説での期待値)が
もっとも観察されやすく、そこを中心として距離の２乗に応じて生起確率が減少するからです。
このような分布を
\textcolor{green}{非心カイ自乗分布}\index{ひしんカイじじょうぶんぷ@非心カイ自乗分布}言います。

非心カイ自乗分布は自由度(k)と、ピークの原点からのずれを表す変数($r$)とで形が決まります。
非心カイ自乗分布の平均が$k+r$であることを利用して、発生させた
検定統計量の平均から$r$を算出し、
それに基づいて非心カイ自乗分布をプロットしてみますと、
確かに、シミュレーションして得られた統計量の
分布とよく重なります。

すべての検定で帰無仮説が成り立つ場合にカイ自乗分布、
すべての検定で「ある対立仮説」が成り立つ場合に非心カイ自乗分布
すべての検定で「対立仮説」が成り立つけれども、その「対立仮説」は大概は「帰無仮説」と
ほぼ同じで、両者の差はゼロを中心に正規分布となるような場合には


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-072-2.eps}
   \includegraphics[width=50mm]{./Fig/PartV-073.eps}
\caption{左：2群のアレル頻度差が0を中心とした正規分布の場合と、
一定割合の場合の比較。また、差が一定値の場合にジェノミックコントロール法補正後ｐ値が
均一分布から大きくずれることを併せて示した。
右 対立仮説下での相加的モデル検定ｐ値と非心カイ自乗分布との一致}
 \end{center}
\end{figure}
\begin{lstlisting}
stB<-rep(0,Niter);pB<-rep(0,Niter) # 統計量とｐ値の格納ベクトル
# ２群に異なるアレル頻度を与える
af<-runif(1)*0.6+0.2 ;delta<-0.05;af1<-af+af*delta;af2<-af-af*delta
# 対立仮説下での期待値表
exptable<-matrix(c(af1^2,2*af1*(1-af1),(1-af1)^2,af2^2,2*af2*(1-af2),(1-af2)^2),nrow=2,byrow=TRUE)*1000
# 相加的モデル検定
cattout<-CATT(exptable)
expSt<-(cattout$statistic)^2 # 対立仮説下でのカイ自乗値期待値
for(i in 1:Niter){ # 対立仮説下でのランダムサンプリング
 case<-sample(c(0,1,2),1000,c(af1^2,2*af1*(1-af1),(1-af1)^2),replace=TRUE)
 cont<-sample(c(0,1,2),1000,c(af2^2,2*af2*(1-af2),(1-af2)^2),replace=TRUE)
 t<-matrix(c(length(case[case==0]),length(case[case==1]),length(case[case==2]),
             length(cont[cont==0]),length(cont[cont==1]),length(cont[cont==2])),nrow=2,byrow=TRUE)
 cattout<-CATT(t); stB[i]<-(cattout$statistic)^2; pB[i]<-cattout$p
}
# ジェノミックコントロール法を適用してみる
lambdaB<-quantile(stB,0.5)/qchisq(0.5,1) ;stcB<-stB/lambdaB;pcB<-pchisq(stcB,1,lower.tail=FALSE)
plot(ppoints(length(pB),a=0),sort(pB),type="l",ylim=ylim)
par(new=T)
plot(ppoints(length(pcB),a=0),sort(pcB),type="l",col="red",ylim=ylim)
# 対立仮説下でのカイ自乗統計量の平均と分散
meanstB<-mean(stB);varstB<-var(stB)
# 非心カイ自乗分布の算出
# 非心パラメタは平均-自由度であることを利用して
ncp<-meanstB-1
# 非心カイ自乗分布と観察カイ自乗とをプロット
ncpvalue<-qchisq(ppoints(length(pcB),a=0),1,ncp)
ylim<-c(0,max(stB,ncpvalue))
plot(ppoints(length(stB),a=0),sort(stB),ylim=ylim)
par(new=T)
plot(ppoints(length(stB),a=0),ncpvalue,type="l",col="red",ylim=ylim)
\end{lstlisting}

\subsection{検定のパワー}

対立仮説が成り立っているという条件では、
統計量の分布が、帰無仮説の場合と異なりました。
今、２つの仮説があるとき、
一つ目の仮説の下で得られる確率密度分布と
二つ目の仮説の下で得られる確率密度分布とが決まります。
前節の場合は、自由度１のカイ自乗分布と自由度１の非心カイ自乗分布でした。
図17.6にこの２つの分布を描きました。
右下がりの曲線が自由度１のカイ自乗分布、
低い山型の分布が非心カイ自乗分布です。
ここで、統計量がある値(図17.6の垂直線)よりも大きければ、
１つ目の仮説を棄却することにします。
１つ目の仮説が真のときに統計量がこの値より大きい確率(P1)は、
この垂直線よりも右側の部分について、この分布を積分した値になります。

他方、二つ目の仮説が真のときにこの垂直線よりも右側のカイ自乗値を
観察する確率(P2)は、低い山型曲線に関して積分した値になります。
この値P2が相当程度大きいとき、
仮説１は棄却され、仮説２は棄却されません。
P1が検定の\textcolor{green}{棄却水準}\index{ききゃくすいじゅん@棄却水準}
で、P2が検定の\textcolor{green}{パワー}\index{パワー@パワー}です。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-074-2.eps}
\caption{}

 \end{center}
 \label{fig:one}
\end{figure}

\begin{lstlisting}
x<-seq(from=0,to=30,by=0.01)
df<-1;lambda<-ncp
chi<-dchisq(x,df,0) # カイ自乗分布
ncChi<-dchisq(x,df,lambda) # 非心カイ自乗分布
ylim=c(0,0.2)
plot(x,chi,ylim=ylim,type="l")
par(new=T)
plot(x,ncChi,ylim=ylim,type="l",col="red")
abline(v=qchisq(0.05,1,lower.tail=FALSE))
pchisq(qchisq(0.05,1,lower.tail=FALSE),1,ncp,lower.tail=FALSE) # パワーの計算
\end{lstlisting}


帰無仮説(一つ目の分布)と対立仮説(二つ目の分布)の下での統計量の分布がわかっていて、
帰無仮説を棄却する基準(垂直線)が決まればパワーが計算できるわけです。

逆に言えば、帰無仮説と対立仮説の下での統計量の分布を定めるだけの情報(変数)と
棄却水準とのすべてとが決まらなければパワーは決まりません。
帰無仮説と対立仮説の下での統計量は、
それぞれの仮説の構成要素(因子ごとの頻度など)とサンプルサイズとで決まりますから、
結局、以下の要素が、パワーに関わる諸因子です。
\begin{itemize}
\item 仮説の構成要素
\item サンプルサイズ
\item 棄却水準
\item パワー
\end{itemize}

この諸条件のうち１つ以外を既知としてやることで、残りの１つの条件の値が決まります。
その様子をRで見てみます。
Rのpower.prop.testでは、サンプル数、２群の因子陽性比率、パワーのいずれかを未知として、
他を指定することで、未知変数の値が返ります。
\begin{lstlisting}
help(power.prop.test)
help(power.t.test)
help(power.anova.test)
power.prop.test(p1 = 0.5, p2 = 0.4, sig.level = 0.01, power = 0.9)
\end{lstlisting}
Rの出力は以下のようになります。
指定しなかった、サンプル数nが算出されて、指定した変数の値とともに示されています。
\begin{screen}
\begin{Schunk}
\begin{Soutput}
     Two-sample comparison of proportions power calculation 

              n = 734.0538
             p1 = 0.5
             p2 = 0.4
      sig.level = 0.01
          power = 0.9
    alternative = two.sided

 NOTE: n is number in *each* group 
\end{Soutput}
\end{Schunk}
\end{screen}

\section{たくさんの結果の分布を活用する}
\subsection{主成分分析を使って補正する}
サンプル間に非独立性があるために、p値が小さめに出たときに、
そのp値の出方の歪みを\textcolor{red}{ジェノミックコントロール法}
\index{ジェノミックコントロールほう@ジェノミックコントロール法}
で補正をしました。
この方法では、帰無仮説が成り立つとみなせる多くの検定結果の分布を、
あるべき分布(p値として均一分布)になるように１個の変数で補正をしていました。
この補正法では、算出した補正係数(１より大)によりカイ自乗値を割って、それを
補正統計量として用いましたから、
すべての検定結果が、補正後には帰無仮説を棄却しにくい方向へと補正します。

今、あるマーカーがフェノタイプに関係しており、ケース群で多いはずとします。
しかしながら、ケースとコントロールとが異なる２集団から
サンプリングされてしまったとしましょう。
そしてこのマーカーはケース群をサンプリングした母集団での頻度が低めで、
コントロール群をサンプリングした母集団での頻度が高めであったとすると、
マーカーとフェノタイプとの関係が希釈されて、検出しにくくなります。
逆の場合には、バイアスのかかったサンプリングの影響は、マーカーと
フェノタイプの関係を実際より大きく見せる働きをするでしょう。
このように、サンプリングバイアスの影響はマーカーごとに異なる向き・
異なる強さにあるので、すべてのマーカーについてひとしなみに補正をしてしまうのは
乱暴です。
もしも、マーカーごとに「適切に」補正することができれば、
より的確な評価ができるでしょう。

多数のサンプルについて、多数のマーカーの値があるとします。
サンプルには集団の構造化があるので、
マーカーに関する個々のサンプルの値を利用して、
サンプル同士の遠近関係を求めることが出来ます。
情報は、マーカーの数の次元だけありますが、
その中から、軸の情報量に大小がつくように、直交座標を選びなおすことができます。
このようにして、新たに選んだ座標系では、主要な軸にサンプルのばらつき具合が集約されていますから、
主要な軸のみを使ってサンプルの位置をさだめることにします。
少数の軸で近似していると考えればよいです。
このようにすると、サンプルが空間に配置され、
マーカーはこの空間に濃淡のパターンを描きます。
空間上である特定の領域に配置されたサンプルが、
あるマーカーの特定のアレルばかりを持てば、そのマーカーは空間に濃淡を作ります。
逆に、空間に均等に分布するようなマーカーもあるでしょう。
また、マーカーの濃淡は、特定の軸に沿っていることもあるでしょうし、
そうでないこともあるでしょう。
いずれにしろ、サンプルの位置に応じて、個々のマーカーの値として、とりやすい値と
とりにくい値が出来てきます。
フェノタイプの方も、空間内に値の分布ができますから、個人の位置によって、取りやすい値と
取りにくい値が出来てきます。
ジェノタイプもフェノタイプも位置による値のとりやすさ・とりにくさで補正をした後に、
ジェノタイプとフェノタイプの検定をします。
このプロセスをＲで見てみることにします。

まず構造化のある集団を作ります。
\begin{lstlisting}
#構造化集団をシミュレート
Nm<-1000 ;Npop<-4; #マーカー数 亜集団数
Ns<-c(100,200,200,200) #集団別人数
M<-NULL #全ジェノタイプデータを納める行列
#亜集団別にアレル頻度・HWE-fを振ってジェノタイプのシミュレーション
for(j in 1:Npop){
 tmpM<-matrix(rep(0,Nm*Ns[j]),nrow=Nm)
 for(i in 1:Nm){
  af<-runif(1)*0.8+0.1;  f<-rnorm(1,sd=0.01);  if(abs(f)>1) f=0
  df<-c(af^2,2*af*(1-af),(1-af)^2)
  df[1]<-df[1]+f/2*df[2];  df[3]<-df[3]+f/2*df[2];  df[2]<-1-df[1]-df[3]
  tmpM[i,]<-sample(c(0,1,2),Ns[j],replace=TRUE,prob=df)
 }
 #全データ行列に格納
 M<-cbind(M,tmpM)
}
\end{lstlisting}

次にこのデータから、\textcolor{green}{主成分分析}\index{しゅせいぶんぶんせき@主成分分析}
によって、主要な軸を取り出します。
\begin{lstlisting}
##PCA##
mu<-apply(M,1,mean) # マーカー別平均
M<-M-mu # マーカー平均で標準化
M<-M/sqrt(mu/2*(1-mu/2)) # 分散で標準化
X<-1/Nm*t(M)%*%M # 個人間の分散・共分散行列
eiout<-eigen(X) # 固有値分解
plot(eiout$values) # 固有値をプロット
\end{lstlisting}
固有値をプロットしてみます。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-078.eps}
\caption{上位の3軸が重要であることが示されている}
 \end{center}
\end{figure}
大きな固有値を持つのは３個、残りは小さく、また、大差がありません。
したがって、取り出された意味のある固有値の数は３個とわかります。
この数は、シミュレーションで作った亜集団の数-1です。

第１から第５までの軸を縦横の軸にとってサンプルの分離の様子をプロットしてみます。
確かに、１−３軸までは分離していますが、４・５軸は意味を持っていない様子が見て取れます。

したがって、この３つの主要な軸を用いて補正をするのが適切であることもわかります。
\begin{lstlisting}
#意味のある固有値の数は、亜集団の数-1
#plotしてみる
eivect<-as.data.frame(eiout$vectors)
eilist<-1:(Npop+1)
plot(eivect[,eilist])
\end{lstlisting}
\begin{figure}[htbp]
 \begin{center}
   \includegraphics{./Fig/PartV-079.eps}
\caption{トップ５軸での散布図。トップ３軸はサンプルを
クラスタリングするが、第４・５軸はクラスタリングしない}
 \end{center}
\end{figure}

次に、ケースとコントロールを構造化亜集団から異なる比率でサンプリングします。
\begin{lstlisting}
#偏らせて形質を与える
phenotype<-c(sample(c(0,1),sum(Ns)/2,replace=TRUE,prob=c(0.45,0.55)),sample(c(0,1),sum(Ns)/2,replace=TRUE,prob=c(0.55,0.45)))
\end{lstlisting}

構造化した集団のジェノタイプと、
そこからアンバランスにサンプリングしたケースとコントロールのサンプルについて、
フェノタイプとジェノタイプの関連を検定します。
また、主成分分析を実行して、それによって
フェノタイプとジェノタイプの補正をした上で、
フェノタイプとジェノタイプの関連を検定します。
\begin{lstlisting}
#検定統計量とｐ値の格納用
Chisq<-rep(0,Nm);CorrChisq<-rep(0,Nm);Ps<-rep(0,Nm);CorrPs<-rep(0,Nm)
L<-3 # 考慮する軸数
Emat<-eiout$vectors[,1:L] # 考慮する軸数分をeigenvector行列から抜き出し
Esqs<-apply(Emat*Emat,2,sum) # 抜けデータがなければ、これはすべて値が1
phenotype<-phenotype-mean(phenotype) # phenotypeの平均値補正
Gamma<-apply(Emat*phenotype,2,sum)/Esqs # 補正項
corrphenotype<-phenotype-Emat%*%Gamma # 補正形質
for(i in 1:Nm){ # マーカーごとにループ
 genotype<-M[i,]
 Gamma<-apply(Emat*genotype,2,sum)/Esqs # マーカーことの補正項
 corrgenotype<-genotype-Emat%*%Gamma # マーカー別の補正ジェノタイプ
#補正なしのトレンド検定統計量は、ジェノタイプ-フェノタイプのcor^2のサンプル数倍
 Chisq[i]<-(sum(Ns))*cor(genotype,phenotype)^2
#補正ありのそれは、補正後ジェノタイプ-補正後フェノタイプのcor^2の(サンプル−亜集団数)倍
 CorrChisq[i]<-(sum(Ns)-(L+1))*cor(corrgenotype,corrphenotype)^2
}
#自由度１でｐ値算出
Ps<-pchisq(Chisq,1,lower.tail=FALSE)
CorrPs<-pchisq(CorrChisq,1,lower.tail=FALSE)
#補正前・後p値昇順plot
ylim<-c(0,1)
plot(ppoints(length(Ps),a=0),sort(Ps),ylim=ylim)
par(new=T)
plot(ppoints(length(Ps),a=0),sort(CorrPs),ylim=ylim,col="red")
# 補正前・後p値散布図
plot(Ps, CorrPs)
# ジェノミックコントロール法との比較
chivalue<- qchisq(Ps,1,lower.tail=FALSE)
lambda<-quantile(chivalue,0.5)/qchisq(0.5,1)
chivalueGC<-chivalue/lambda
pGC<-pchisq(chivalueGC,1,lower.tail=FALSE)
plot(Ps,pGC)
\end{lstlisting}
補正前のp値と補正後のp値の昇順プロット(図17.9)を見ると、
補正によって、p値が均一分布(対角線)に近づいていることがわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-082.eps}
\caption{補正前ｐ値と主成分分析による補正後ｐ値の昇順プロット}
 \end{center}
\end{figure}

また、補正前のp値と補正後のp値との関係を見ると、
補正によってp値が大きくなるもの(y=xの対角線の上側)と、逆に小さくなるもの(対角線の下側)
とがあることがわかります。

ジェノミックコントロールによって補正した場合は、すべて対角線の上側に来ているのと対照的です。

主成分分析による方法では、マーカーごとに補正をするのに対し、
ジェノミックコントロール法ではすべてのマーカーを一律に補正することがこの散布図の違いを作っています。


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-083.eps}
   \includegraphics[width=50mm]{./Fig/PartV-084.eps}
\caption{左。PCAに基づく補正前後のｐ値の散布図。
右。ジェノミックコントロール法による補正前後のｐ値の散布図}

 \end{center}
 \label{fig:one}
\end{figure}


このように、多くのデータがあるときには、サンプル間の依存性を抽出して、補正に
利用することができることがわかりました。

\subsection{帰無仮説が必ずしも棄却されるべきではないとき}
\subsubsection{False Discovery Rate(FDR)}
多くの帰無仮説棄却検定を行うにあたり、p値が一様分布とならないときの補正について述べましたが、
検定仮説の少なからぬ部分が帰無仮説を棄却するような場合はどのように考えればよいでしょうか？
大規模な遺伝子発現データの場合などは、関連があって当然な因子を数多く取り扱います。
そのようなときの対処法として、\\
「そもそも、検定のうちのある割合は帰無仮説を棄却するものとする」\\
という発想をします。
雑な考え方をすれば、全部でN個の仮説を検定するときに、
陽性となるべき仮設の割合が$a$としたら、
p値が小さい方から、$N \times p$個の仮説が帰無仮説を棄却したとみなす、というのはどうでしょうか。
これでは、p値が小さい方の仮説のp値の大小に関わらず、採択されてしまうので、
さすがに乱暴に過ぎるでしょう。
では、N個のp値が、$\frac{i}{N+1};i=1,2,...,N$というように、多重検定における、
p値の期待値どおりに出ていたらどうでしょう(図17.11)。
この場合も、すべてが帰無仮説を満足しているときに、「普通に出るべきp値」なわけですから、
このようなときに上位$a$の仮説を棄却するのは、問題があるでしょう。
\begin{lstlisting}
# 人工的なp値分布を作ります
N<-1000
peven<-p1<-ppoints(N,a=0) # N個のp値の期待値のベクトル
p1<-peven^5 # 期待値から人工的に逸脱させる
plot(peven, peven,xlim=c(0,1),ylim=c(0,1),type="l")
par(new=T)
plot(peven,p1,xlim=c(0,1),ylim=c(0,1),type="l")
abline(v=0.05) # 棄却水準
\end{lstlisting}
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-085-2.eps}
\caption{1000個のｐ値の昇順プロット。対角線を描いているのが、すべて帰無仮説が真である場合。
下に凸の場合は、名目上のｐ値が小さめに出ている(対立仮説が真である検定が多い)。
垂直線が棄却水準0.05。垂直線より左側を有意とすれば、全体の0.05の
検定を有意とすることになる}

 \end{center}
\end{figure}

すべての仮説が帰無仮説を満足するときに期待されるp値の線よりも、小さめの方に
p値のプロットがあるときには、
そのうちのp値の小さいほうから、採択することは妥当のように思えます。
問題は、どこまでを採択するかの線引きにルールをどうやって作るかです。
今の考えたいのは「$a$の割合の仮説が採択される」ことが、
標準的であるように工夫したルールです。

\textcolor{green}{False Discovery Rate}
\index{False Discovery Rate@False Discovery Rate}(
\textcolor{green}{FDR}\index{FDR@FDR})と呼ばれる手法がこれに相当します。
工夫には色々やり方がありますが、その中で、最も単純な
\textcolor{green}{BH法}\index{BHほう@BH法}
と呼ばれるもののやり方を見てみることにします。

まずはじめに、次の様に考えてみます。
「可能性は極めて低いですが、もしかしたら、すべての仮説が帰無仮説を棄却するかもしれない」です。
そのようなときには、最も大きいp値を返した仮説も帰無仮説を棄却するはずです。
今、棄却水準を$b$としますと、
最も大きいp値が$b$よりも小さかったら、その仮説も含めて、すべての仮説が
帰無仮説を棄却したと考えることにしましょう。
最も大きいp値だけではなく、それ以外のp値にも採択するべきかどうかの基準を定めることにします。
第i番目に大きいp値に関しては、
\begin{equation*}
b\times \frac{N-i+1}{N}
\end{equation*}
の値よりも小さかったら、採択することにすると、
最も大きいp値の場合には、$b \times \frac{N-1+1}{N}=b$となって、この方法でうまく行きます。

図にしてみます。
図17.11の下に凸なp値を大きいほうから並べます。
プロットすると下に凸です。(図17.12)
図では、下に凸な曲線が2本、引かれています。
下側の線がp値のプロットです。
そこに、$y=b \frac{N-i+1}{N}$($b=0.05$)の線を引きます。
この線より下回った検定が採択されます。

Rでは、
"p.adjust()"という関数が、あります。
この関数は、検定結果のp値の列を補正して、それぞれのp値が、
直線$y=b \frac{N-i+1}{N}$と交わるとしたら、bの値がいくつのときかを算出して、
それを補正後のp値として返します。
したがって、b=0.05のときには、補正後p値が0.05より小さければ、
FDR法で採択されたことになります。
図17.12では、"p.adjust()"関数の補正後p値もプロットしてあります。
そのラインが、下に凸な２本の線のうちの上側の線です。
観察ｐ値のラインが直線$y=0.05 \frac{N-i+1}{N}$と交わった点を通る垂直線が引いてあり、
その垂直線と補正後p値のプロットの交点のy軸値が$0.05$と成っていることを示しています。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-087-2.eps}
\caption{下に凸な曲線は、下側のそれが観測されたp値を降順にプロットしたもの、
上側のそれは、FDR補正後のp値。$y=0.05 \frac{N-i+1}{N}$の
直線(y切片が0.05)と観測ｐ値曲線との交点を通る垂直線が引かれている。
その垂直線とy=0.05との交点を補正後ｐ値曲線は通る
}
 \end{center}

\end{figure}


\begin{lstlisting}
N<-1000
peven<-p1<-ppoints(N,a=0) # N個のp値の期待値のベクトル
peven<-ppoints(length(p),a=0)
p1<-peven^5 # 期待値から人工的に逸脱させる
pfdr<-p.adjust(p1,"fdr")# FDR
b<-0.05 # ５％で帰無仮説が成立すると仮定する
# 名目ｐ値
plot(peven,sort(p1,decreasing=TRUE),xlim=c(0,1),ylim=c(0,1),type="l")
par(new=T)
# BH法の補助直線
plot(peven,b*((length(p1)-(1:length(p1))+1)/length(p1)),xlim=c(0,1),ylim=c(0,1),type="l",col="red")
# 補正後ｐ値
par(new=T)
plot(peven,sort(pfdr,decreasing=TRUE),xlim=c(0,1),ylim=c(0,1),type="l",col="blue")
abline(h=b) # y=bの水平線
abline(v=peven[length(which(pfdr>b))]) # 名目p値の線と補助直線の交点を通る垂線
\end{lstlisting}


\subsubsection{ベイズ因子}

FDRでは、仮説のうちのある割合が採択されるべきであるというところから出発して、
採択基準を定めました。
別の方法もあります。
今、ゲノム上の多数のマーカーとある形質との関係を調べているものとします。
ジェノミックコントロールのときに仮定したとおり、ほとんどすべてのマーカーは
形質と無関係かもしれません。
しかしながら、ゲノム上には非常に多くの要素があり、
それらは、大きな力・小さな力、色々な力を持っていて、
ごく微細な力も含めれば、ゲノム上のほぼすべての場所が形質となんらかの関連がある
と考えることも、また、悪くないかもしれません。
実際、ジェノミックコントロールでは、
数多くのマーカーが、その程度はまちまちだけれども、亜集団ごとにアレル頻度が違っている
ことを仮定しました。
そのときの仮定として、アレル頻度の違いは平均を0とする正規分布に従うものとしました。
配列多様性も大部分は微弱な関連を持ち、
ごくわずかに目立った関連があるとすることもあります。
このように、帰無ではない関連の分布を仮定することは、
\textcolor{red}{事前分布}\index{じぜんぶんぷ@事前分布}をある意味で勝手に仮定しているわけで、ベイズの考え方です。
このように大多数のマーカーに帰無ではない仮説を仮定してデータを
解釈する手法として\textcolor{green}{ベイズ因子}\index{ベイズいんし@ベイズ因子}があります。



\section{複数の結果を合わせる メタアナリシス}
\subsection{相互に独立な検定を併せる}
$2\times 2$分割表による独立性の検定が２つあるときのことを考えます。
この２つの検定は、お互いに何の関係もないとします。
この２つの検定をサンプルを取り直して何度も繰り返すことを考えます。
それぞれの帰無仮説のもとでのピアソンの独立性検定のカイ自乗値は、
自由度１のカイ自乗分布に従いますので、
２つの検定のピアソン検定のカイ自乗検定のｐ値の散布図を描くと、次のようになります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-089.eps}
\caption{互いに独立な２つの検定を行ったときのｐ値の散布図}
 \end{center}
\end{figure}

\begin{lstlisting}
Nt<-100
df1<-1
df2<-2
chi1<-rchisq(Nt,df1)
chi2<-rchisq(Nt,df2)
plot(pchisq(chi1,df1,lower.tail=FALSE),pchisq(chi2,df2,lower.tail=FALSE))
chisum<-chi1+chi2 # カイ自乗値の和
plot(ppoints(Nt,a=0),sort(pchisq(chisum,df=df1+df2,lower.tail=FALSE)),type="l") # 自由度の和を自由度としてｐ値化
\end{lstlisting}

２つの検定からのカイ自乗値の和は、２つの検定の自由度の和である２を自由度としたカイ自乗分布に従うこと
は、シミュレーションデータのｐ値が均一分布となっていることで示せます(図17.14)。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-090.eps}
\caption{２つの相互に無関係なピアソンの方法のカイ自乗値を足した値を、
もとの２つのピアソンの方法の自由度の和を自由度とするカイ自乗分布に照らしてｐ値としたときの
ｐ値の昇順プロット。均一分布になっている}
 \end{center}
\end{figure}

実際、検定の数は２に限らず、個々の検定の自由度が任意であってもこの関係は成り立ちます。
相互に独立な検定を複数行うことは、個々の検定の自由度の和の検定を１個行うことであることも
も同様に示せます。
\footnote{

Nt<-10000
Nk<-rpois(1,10)
dfs<-rpois(Nk,10)
chis<-matrix(0,Nt,Nk)
for(i in 1:Nk){
 chis[,i]<-rchisq(Nt,dfs[i])
}
dfs
chisum<-apply(chis,1,sum)
dfsum<-sum(dfs)
plot(ppoints(Nt,a=0),sort(pchisq(chisum,df=dfsum,lower.tail=FALSE)),type="l")

}


２つの$2\times 2$表検定に話しを戻します。
２つの表があったときに、
この表のそれぞれの周辺分布のもとで取りうる表を
全部作成して、２次元にプロットします(図17.15)。
これまで、多くの２次元プロットが正三角形でしたが、
今度は、２つの表の軸を\textcolor{red}{直交}\index{ちょっこう@直交}させます。
２つの表が\textcolor{red}{独立}\index{どくりつ@独立}だからです。
横軸は１つ目の表の第１セルの値の期待値からのずれを、
縦軸は２つ目の表のそれをとって描いてあります。
楕円の等高線が描けていることがわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-093.eps}
\caption{2つの$2\times 2$ 表があるときに、第一の表の期待値表からのずれを横軸に、
第二の表のそれを縦軸にとる。２表のピアソンの独立性検定カイ自乗値の和が等しいような
表のペアに相当する点をつなぐと楕円になる。
楕円を描いている}
 \end{center}
\end{figure}

２次元に楕円の等高線が描けるところは、自由度２の表($2\times 3$表)の自由度にのピアソンの
カイ自乗検定と同じです。
$2\times 3$表で自由度２の検定をしたときには、優性・劣性・additiveなどの遺伝形式には拘束されず、
３つの列が完全に自由に値を取れることを前提とした検定でした。
今、２つの$2\times 2$表について、それぞれが自由に値を取れることを
対立仮説として、２つの表の両方が帰無であるという仮説を帰無仮説とすると、
自由度２の検定となるわけです。
\begin{lstlisting}
# ２表とその周辺度数・期待値テーブルを作る
t1<-t2<-matrix(c(10,10,10,10),nrow=2,byrow=TRUE)
m11<-m21<-apply(t1,1,sum);m12<-m22<=apply(t1,2,sum);M1<-M2<-sum(t1)
e1<-m11%*%t(m12)/M1;e2<-m21%*%t(m22)/M2
# 取り得るテーブルをx11,y11を変数として網羅する
x11<-seq(from=-M1,to=M1,by=1);x12<--x11+m11[1];x21<--x11+m12[1];x22<--x12+m12[2]
xbind<-cbind(x11,x12,x21,x22);okx<-which(apply(xbind,1,min)>0)
x11<-x11[okx];x12<-x12[okx];x21<-x21[okx];x22<-x22[okx]
y11<-seq(from=-M2,to=M2,by=1);y12<--y11+m21[1];y21<--y11+m22[1];y22<--y12+m22[2];ybind<-cbind(y11,y12,y21,y22);oky<-which(apply(ybind,1,min)>0)
y11<-y11[oky];y12<-y12[oky];y21<-y21[oky];y22<-y22[oky]
# カイ自乗を計算する
chi1<-(x11-e1[1,1])^2/e1[1,1]+(x12-e1[1,2])^2/e1[1,2]+(x21-e1[2,1])^2/e1[2,1]+(x22-e1[2,2])^2/e1[2,2]
chi2<-(y11-e2[1,1])^2/e2[1,1]+(y12-e2[1,2])^2/e2[1,2]+(y21-e2[2,1])^2/e2[2,1]+(y22-e2[2,2])^2/e2[2,2]
# ２つのカイ自乗値を足し併せる
z<-outer(chi1,chi2,FUN="+")
xlim<-ylim<-c(min(x11-e1[1,1],y11-e2[1,1]),max(x11-e1[1,1],y11-e2[1,1]))
contour(x11-e1[1,1],y11-e2[1,1],z,xlim=xlim,ylim=ylim)
abline(h=0);abline(v=0) # 補助線

\end{lstlisting}

\subsection{２表を単純に足し併せる}
では、本節の本題であるメタアナリシス
に話しを移します。
２つの$2\times 2$表分割表検定を統合しようとしているとします。
どちらの表も
因子A/aと因子B/bとの間の関連について調べているものとします。

このメタアナリシスでは、帰無仮説は、２つの検定のそれぞれで帰無仮説が成り立つ、というものです。
そして、対立仮説は、２つの検定に共通した、何かしらの仮説です。
図17.15を見てみます。
この図は、２つの表のどちらもが期待値をとる場合が原点になっています。
そして、第一象限(右上の４半分)は、２つの検定の両方で、(1,1)のセルが期待値よりも大きくなっている領域で、
第３象限(左下の４半分)は、逆に、２つの検定の両方で、(1,1)のセルが期待値よりも小さくなっている領域です。
残りの２つの象限は、(1,1)のセルが、片方の検定では大きくなり、もう片方の検定では小さくなるような場合に
対応した領域です。
４つの象限において、２つの検定がどの因子の結びつき("A=B"は"AとBとが正の関連を持つ"を表わすものとします)
を指示しているかを表にすれば、次のようになります。

\begin{tabular}[htb]{|c|c|c|} \hline
　&第一検定&第二検定\\ \hline
第一象限& A=B,a=b & A=B,a=b \\ \hline
第二象限& A=b,a=B & A=B,a=b \\ \hline
第三象限& A=b,a=B & A=b,a=B \\ \hline
第四象限& A=B,a=b & A=b,a=B \\ \hline
\end{tabular}

したがって、２つの検定が、A/a,B/bの関係において、同様の傾向を示しているかどうかで言えば、
第１、第３象限はそういえますが、第２、第４象限は、反対の傾向を示していることになります。

さて、こんな風に考えて見ましょう。
実は、これは、１つのスタディであったとします。
たまたま、だれかが、データの一部を集計し、その残りを別に集計したものだったのです。
この場合は、もともと１つのスタディであったわけですから、単純に
合わせて１つのスタディと考えるのがよいです。

そうしてやると、２つのスタディの和に対して、カイ自乗値が１つ得られます。それを描いてみます
(図17.16)。
このようにしてやると、対立仮説は、２つの表を足し合わせた表から得られる、群別・因子別の
頻度がこの合算表の最尤推定値になりますから、
それを仮説とした検定になります。
合算する前には、２つの検定のそれぞれに自由な比率を仮説として検討できたのに、
自由な程度が減少してしまったわけです。
そうしますと、２ｘ３表に対して、何かしらの遺伝形式をモデルとして適用したときと同様に
２次元空間に直線状の等高線が現われました。
そして、その直線状の等高線は、自由度２の楕円の接線となりますが、
その接線は、傾きが-1です。
これは、第一の表の(1,1)のセルの期待値からのずれと第二の表の(1,1)のセルの
期待値からのずれを足した値が同じであるときには、メタアナリシスの結果も同じにすることを意味します。

そして、先ほど、２つの表が共通の対立仮説を持つならば、第１、第３象限がそれを支持し、
第２、第４象限は、それを否定することを確認しましたが、
確かに、この合算表による等高線は、共通仮説を支持する部分で大きなカイ自乗値を持ち、
共通仮説を否定する領域では、中心から遠くなっても(個々の検定でカイ自乗値が大きくなっても)、統合した
上での判定は、カイ自乗値が増えません。
\begin{lstlisting}
#　格子状に作った２表のセルの値を足し併せる
sum11<-outer(x11,y11,FUN="+");sum12<-outer(x12,y12,FUN="+")
sum21<-outer(x21,y21,FUN="+");sum22<-outer(x22,y22,FUN="+")
sume11<-e1[1,1]+e2[1,1];sume12<-e1[1,2]+e2[1,2];sume21<-e1[2,1]+e2[2,1];sume22<-e1[2,2]+e2[2,2]
# 足し併せた表のカイ自乗値
sumz<-(sum11-sume11)^2/sume11+(sum12-sume12)^2/sume12+(sum21-sume21)^2/sume21+(sum22-sume22)^2/sume22
xlim<-ylim<-c(min(x11-e1[1,1],y11-e2[1,1]),max(x11-e1[1,1],y11-e2[1,1]))
contour(x11-e1[1,1],y11-e2[1,1],sumz,xlim=xlim,ylim=ylim) # 表の足し併せによる等高線
par(new=T)
contour(x11-e1[1,1],y11-e2[1,1],z,col="red",xlim=xlim,ylim=ylim) # 楕円等高線
abline(a=0,b=1)
\end{lstlisting}


\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-095.eps}
\caption{２つの$2\times 2$表のカイ自乗値の和が等しいような表のペアに相当する点を結ぶと楕円になる。
１つ目の表の期待値表からのずれと２つ目の表の期待値表からのずれとの和が等しいような
表のペアに相当する点を結ぶと傾きが-1の直線になる}
 \end{center}
\end{figure}

\subsection{メタアナリシス}
複数の$2\times 2$表を統合する\textcolor{green}{メタアナリシス}
\index{メタアナリシス@メタアナリシス}の手法の等高線がどうなるかを見てみることにします。
メタアナリシスの方法には大きく２つあります。
\textcolor{green}{固定効果モデル}\index{こていこうかモデル@固定効果モデル}と呼ばれるものと、
\textcolor{green}{変量効果モデル}\index{へんりょうこうかモデル@変量効果モデル}と呼ばれるものです。
それらについてのイメージを知るには、等高線を眺めるほうが早いので、描いてみます。
固定効果モデルの代表的な手法である
\textcolor{green}{マンテル-ヘンツェル法}
\index{マンテル-ヘンツェルほう@マンテル-ヘンツェル法}では、
表を足し合わせたのと同じく、傾きが負の直線状の等高線になります。
一方、変量効果モデルの代表手法である\textcolor{green}{DerSimonian-Laird法}
\index{DerSimonian-Lairdほう@DerSimonian-Laird法}では、
統計量が大きくなる特定の峰があって、そこから離れると
急速に統計量が減少します。
逆に言うと、スタディ間で同じ傾向(ＯＲが等しいなど)とき
(図17.17のような描き方のときには、表のペアの座標が「傾きが正の直線に沿っているとき」)には、固定効果モデルも変量効果モデルも
違わないが、
スタディ間の違いが大きいときには、固定効果モデルよりも
変量効果モデルの方が保守的であることがわかります。

\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-096.eps}
   \includegraphics[width=50mm]{./Fig/PartV-097.eps}
\caption{左は固定効果モデルで同じ検定結果となる表のペアを結んだ線(赤)。
２表を単純に足し併せて検定したときの等高線を補助線(黒)として引いてありますが、それと同じになります。
右は変量効果モデルで同じ検定結果となる表のペアを結んだ線(赤)。
２表の統計量を足し併せたときの等高線(楕円)を補助線(黒)として引いてあります}
 \end{center}
\end{figure}

\begin{lstlisting}
library(rmeta)
n.case<-c(m11[1],m21[1]);n.ctrl<-c(m11[2],m21[2])
zMH<-z;zDSL<-z
for(i in 1:length(z[,1])){
 for(j in 1:length(z[1,])){
  mhout<-meta.MH(n.case,n.ctrl,c(x11[i],y11[j]),c(m12[1]-x11[i],m22[1]-y11[j]))
  zMH[i,j]<-mhout$MHtest[1] # マンテル-ヘンツェル法
  dslout<-meta.DSL(n.case,n.ctrl,c(x11[i],y11[j]),c(m12[1]-x11[i],m22[1]-y11[j]))
  zDSL[i,j]<-dslout$test[1]^2 # DerSimonian-Laird法
 }
}
zlim<-c(0,max(sumz))
contour(x11-e1[1,1],y11-e2[1,1],sumz,xlim=xlim,ylim=ylim,zlim=zlim,nlevels=10)
par(new=T)
contour(x11-e1[1,1],y11-e2[1,1],zMH,col="red",xlim=xlim,ylim=ylim,zlim=zlim,nlevels=10)
contour(x11-e1[1,1],y11-e2[1,1],z,xlim=xlim,ylim=ylim,zlim=zlim,,nlevels=10)
par(new=T)
contour(x11-e1[1,1],y11-e2[1,1],zDSL,col="red",xlim=xlim,ylim=ylim,zlim=zlim,nlevels=10)
\end{lstlisting}


ここまでは、対称的な表でしかも２個の表が同一という、極めて均整のとれた場合で図を描きました。
今度は個々の表の対称性を次のRコマンドが示すように崩してみます。
\begin{lstlisting}
t1<-matrix(c(10,20,30,40),nrow=2,byrow=TRUE)
t2<-matrix(c(200,300,50,40),nrow=2,byrow=TRUE)
\end{lstlisting}
このようにすると
楕円がゆがんだり、楕円と直線との相対的位置関係が
均整のとれていた場合と均整が崩れた場合とで異なる点などがありますが、
基本的には、2次元平面にどのような等高線を引くかによる違いであることがわかります。
\begin{figure}[htbp]
 \begin{center}
   \includegraphics[width=50mm]{./Fig/PartV-130.eps}
   \includegraphics[width=50mm]{./Fig/PartV-131.eps}
\caption{左が固定効果モデルでの等高線。表を足し併せて検定したときの補助線(黒)とずれが生じていますが、
等高線の傾きは同じです。
右が変量効果モデルでの等高線。等高線が作る峰の方向や幅が図18.17(右)と異なっています}
 \end{center}
\end{figure}



メタアナリシスは、帰無仮説を棄却するかどうかの判断と同時に、
共通する仮説における、「効果」の大きさの値を統合して、信頼区間を与えることも重要です。
また、
メタアナリシスは、そもそも統合するべきスタディをどうやって見つけるか、といった事柄も
重要ですが、それらを含めると話しが大きくなりますので、本書では、
メタアナリシスを、他の章の話題との関連で了解することを目的として、ここで終了とします。

\part{付録}
\chapter{Ｒ}

\section{\textcolor{green}{Ｒのインストール}
\index{Rのインストール@Rのインストール}と起動と終了}
Ｒは統計関係の処理がしやすいように作られた
フリーソフトです。
CRAN ("The comprehensive R Archive Network"　\url{http://cran.r-project.org/"})が
その管理サイトです。このサイトから利用環境(Linux, Mac, Windows)に合わせて
配布用バイナリをダウンロードしてインストールすることで利用が可能です。
普通のフリーソフトのインストールと同じです。
インストール後に起動用のアイコンができますから、起動すると画面が出ます。
カーソルが">"(プロンプト)の後にあるます。ここに、コマンドを打ちます。
\begin{screen}
 'demo()'と入力すればデモをみることができます。 \\
 'help()'とすればオンラインヘルプが出ます。 \\
 'help.start()'でHTMLブラウザによるヘルプがみられます。\\ 
 'q()'と入力すればRを終了します。\\
\\
>
\end{screen}
\section{\textcolor{green}{Rのパッケージ}
\index{Rのパッケージ@Rのパッケージ}を使う}
Rをインストールすると、基本的な統計解析とプログラミングが可能となります。
利用者が限定されているような処理については、
パッケージと呼ばれる関数群がCRANおよびそのミラーから
ダウンロードすることによって可能になります。

Ｒのインターフェースのメニューバーから、「パッケージ」を選択し、
CRANミラーサイトの選択(身近なところが良いです)し、
パッケージのインストールを選ぶと、パッケージ名のリストが表示されるので、
目的のパッケージを選びます。
そうすると、このパッケージがダウンロードされます。
ダウンロードしたパッケージは、"....\\R\\R-2.10.0\\library"に置かれます。
ここは、Ｒのインストール時についてきた基本的なパッケージの置き場です。
無事に済んだら
\begin{lstlisting}
library(ape)
\end{lstlisting}
とパッケージを読み込みます。
これで、"ape"パッケージの関数が使えるようになります。

本書で使用したパッケージは次の通りです。
"ape",
"binom",
"bnlearn",
"clinfun",
"evd",
"gregmisc",
"gtools",
"MCMCpack",
"phangorn",
"Rassoc",
"rmeta"

\section{\textcolor{green}{Rのヘルプ}\index{Rのヘルプ@Rのヘルプ}を出す。関数を使ってみる}
Rにはどんどん新しい関数が付け加わっています。
少しでも利用できるかもしれない関数を見つけたら、その内容を確認することが大事です。
そのために解説記事がついています。
例えば、dist()関数についての説明を読むなら、
\begin{lstlisting}
help(dist)
\end{lstlisting}
とします。
ヘルプ画面の終わりの方に使い方 "Examples"があります。
コピー・ペーストすれば動くようになっていますから、、
それをそのまま実行してみてください。
使い方について一通りわかります。
\subsection{\textcolor{green}{Ｒのソース}\index{Rのソース@Rのソース}を確認する}
\subsubsection{言語と構成}
Ｒの関数は、Ｒという言語で書かれたものと、Ｃ、Fortranで書かれたものでできています。
Ｒのアプリケーションとしての骨格、計算関数の基本部分はＣ、Fortranの関数を用いて作られていて、
統計的処理の関数にＲ言語の関数が用いられている場合が多いという構成です。
Ｒの関数は、Ｃの関数、Fortranの関数を呼び出して使っている場合も多いです。

\subsubsection{Ｒの構成 バイナリー配布とソース配布}
Ｒはアプリケーションとして動く状態で配布されます。
そのときに元のソースコードなしに、動く状態のファイルのみで入手することも出来ますし(バイナリ配布)、
ソースコードともども入手することも出来ます(ソースでの配布)。
いずれの方法にしろ、通常配布されたもは、統計ソフトとしての基本的なことが一通りできるアプリケーションを提供します。
場合によっては、この基本セットではできない解析などがあります。
Ｒはフリーであり、Ｒで作成した解析ツールを利用者相互で共有する仕組みを持っています。
パッケージという後付けできる関数セットがあり、これも、ネットからダウンロードすることができます。
このパッケージは、一定のルールで作られている場合には、Ｒ本体から簡単に入手することができます。
そのほかにも、ルールに合ってはいないけれども、入手して取り込むことができるパッケージが個人的に
公開されていることがあります。
また、自分や仲間内で関数作って共有することもあります。この場合には、パッケージとしての体裁をとる必要も
ありません。
\subsubsection{\textcolor{green}{Rのソース}\index{Rのソース@Rのソース}の確認の仕方}
ソースコードを読むと、統計処理の内容が良くわかりますから、ソースを表示してみることは有用です。
簡単に表示できる場合とそうでない場合があります。
簡単に表示できる場合は関数名を入力するだけです。
\begin{lstlisting}
 dist
t.test
\end{lstlisting}
このように、関数のソースが表示されることもあります。
ですが、中味がわからない場合もあります。
いくつかのパターンに分かれます。
\begin{lstlisting}
median
runif
sum
chol
\end{lstlisting}

"median()"関数の場合は、"UseMethod()"を使っていて、その詳細が隠されていることがわかります。
"UseMethod()"を使っている関数の場合には、"methods()"関数を使うと隠れている中味に関する情報が増えます。
"median"は"median.default"という中味をもっていることがわかるので、それをコマンドで与えると中味が出ます。
"t.test"の場合には"methods()"関数を使うと、"t.test.default"と"t.test.formula"という中味が隠れていることが
わかるのですが、さらに隠されていることが"*"で示されています。
この場合には、"t.test.default"というコマンドは無効で、"getS3method("t.test","default")"とか
"getS3method("t.test","foumula")"というコマンドによって初めて、中味が表示されます。
"runif()"の場合は"UseMethod"ではなく、".Internal()"となっています。
これは、Ｒのアプリケーション本体として登録された関数を呼び出していることを表しています。
また、Ｒ言語ではなくＣ言語で書かれていることも意味します。
Ｃ言語のソースはバイナリで入手したＲの場合は、読むことが出来ません。
ソースとして入手した場合には、"解凍先\\src\\main\\"以下にある、Ｃのソースファイルのどこかにあります。
Ｃの関数としての呼び名は、"解凍先\\src\\main\\names.c"というファイルに書かれているので、そこから
たどることになりますが、
関数名のファイルがあるわけではないので、見つけるのが簡単でないことも多いです。

"sum()"の場合には、".Internal()"の代わりに".Primitive()"となっていました。この場合も同様です。
"chol()"の場合には、再び、"UseMethod()"が使われていますので"chol.default"で呼び出します。
"dist()"はすぐに表示できます。
この２つの関数では、処理の主要な部分で、".Call()",".Fortran()",".C()"関数が使われています。
この３つと、もう1つ".External()"を併せた４つの関数は、すでに述べた".Internal()",".Primitive()"と
同様に、Ｒ言語以外で書かれた関数を呼び出すものです。
それらの関数は、ソースコードファイルの中にあります。
探し方の原則は、packageの名前等の付加情報を活用することでしょう。
必ずしも、容易にたどり着けるわけではないようです。

\section{確率分布関数、疑似乱数列の発生}
本書では図を作成するのにRの分布関数と疑似乱数列とを多用しました。
Rには様々な確率分布があります。
それらについて、Rでは確率密度、累積確率、クオンタイル、疑似乱数列を与える
関数が用意されています。
たとえば、二項分布であれば、分布名を表す文字列"binom"と、
確率密度(d: density)、累積確率(p: 検定p値を求めるときに使う)、クオンタイル(q: quantile)、
疑似乱数(r: random)とを組み合わせて、
"dbinom,pbinom,qbinom,rbinom"という関数があります。
ポアッソン分布なら"dpois,ppois,qpois,rpois"です。
\chapter{数式記号}
\begin{itemize}
\item $\sum_{i=1}^{n} x_i=x_1+x_2+...+x_n$
\item $\prod_{i=1}^{n} x_i = x_1\times x_2 \times ... \times x_n$
\item $N!=N \times (N-1) \times ... \times 2 \times 1$
\item $\binom{N}{k}=\frac{N!}{k! (N-k)!}$ 
\item \textcolor{green}{集合}\index{しゅうごう@集合}の交わり：
$A \cap B $、集合の和：$A\cup B$、\textcolor{green}{補集合}\index{ほしゅうごう@補集合}
：$A^c$, $A \cap A^c = \phi$ (\textcolor{green}{空集合}\index{くうしゅうごう@空集合})
\item \textcolor{green}{階乗}\index{かいじょう@階乗}と\textcolor{green}{ガンマ関数}($\Gamma(z)$)\index{ガンマかんすう@ガンマ関数}
と\textcolor{green}{ベータ関数}($B(x,y)$)\index{ベータかんすう@ベータ関数}、
\textcolor{green}{ベータ分布}\index{ベータぶんぷ@ベータ分布}
\begin{itemize}
\item $\Gamma(z)= \int_0^{\infty} t^{z-1} e^{-t} dt $
\item $\Gamma(z+1)=z!$
\item $\Gamma(\frac{1}{2})=\sqrt{\pi}$
\item $B(x,y)=\int_0^1 t^{x-1} (1-t)^{y-1} dt = \frac{\Gamma(x)+\Gamma(y)}{\Gamma(x+y)}$
\item $\beta(x;\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}
(1-x)^{\beta-1}$
\end{itemize}
\end{itemize}
\printindex
\end{document}